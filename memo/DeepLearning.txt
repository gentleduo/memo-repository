深层神经网络有两个非常重要的特征：多层、非线性
线性模型的局限性：
在线性模型中，模型的输出为输入的加权和，假设一个模型的输出y和输入x满足以下关系，这个模型就是一个线性模型。
有y = ∑wx + b
其中wi,b∈R为模型参数。被称之为线性模型是因为当模型的输入只有一个的时候，x和y形成了二维坐标系上的一条直线。类似的，当模型的输入有n个的时候，x和y就形成了n+1维空间的中的一个平面。而一个线性模型中通过输入得到输出的函数被称之为一个线性变换。上面的公式是一个线性变换。线性模型最大特点是任意线性模型的组合仍然还是线性模型。
激活函数实现去线性化
常用的激活函数：
ReLU函数：f(x)=max(x,0)
sigmoid函数：f(x)=1/(1+exp(-x))
tanh函数：f(x)=(1-exp(-2x))/(1+exp(-2x))
多层神经网络解决异或分类问题:
异或（XOR），如果两个输入符号相同时（同时为正或者同时为负）则输出为0，否则（一个正一个负）输出为1。
与（AND），如果两个输入其中一个符号为负时则输出为0，否则输出为1。
与非（NOT AND），和与（AND）相反：两个输入符号同时为正时则输出为0，否则输出为1。
或（OR），如果两个输入其中一个符号为正时则输出为1，否则输出为0。
与、与非、或都可以找到不止一条直线将各种情况分类开，但是异或，则找不出一条直线，将其进行分类。本质上，异或是一种线性不可分问题。

####################################################################信息量####################################################################
信息量：在日常生活中，极少发生的事件一旦发生是容易引起人们关注的，而司空见惯的事不会引起注意，也就是说，极少见的事件所带来的信息量多。如果用统计学的术语来描述，就是出现概率小的事件信息量多。因此，事件出现得概率越小，信息量愈大。即信息量的多少是与事件发生频繁（即概率大小）成反比。
1.如已知事件Xi已发生，则表示Xi所含有或所提供的信息量
H(Xi) = −log(a(pi))
例题：若估计在一次国际象棋比赛中谢军获得冠军的可能性为0.1（记为事件A），而在另一次国际象棋比赛中她得到冠军的可能性为0.9（记为事件B）。试分别计算当你得知她获得冠军时，从这两个事件中获得的信息量各为多少？
H(A)=-log2p(0.1) ≈3.32（比特）
H(B)=-log2p(0.9) ≈0.152（比特）
2.统计信息量的计算公式为：
Xi —— 表示第i个状态（总共有n种状态）；
P(Xi）——表示第i个状态出现的概率；
H(X）——表示用以消除这个事物的不确定性所需要的信息量。
例题：向空中投掷硬币，落地后有两种可能的状态，一个是正面朝上，另一个是反面朝上，每个状态出现的概率为1/2。如投掷均匀的正六面体的骰子，则可能会出现的状态有6个，每一个状态出现的概率均为1/6。试通过计算来比较状态的不肯定性与硬币状态的不肯定性的大小。
H（硬币）= -（2×1/2）×log2p(1/2) ≈1（比特）
H（骰子）= -（1/6×6）×log2p(1/6) ≈2.6（比特）
由以上计算可以得出两个推论：
[推论1]当且仅当某个P(Xi)=1，其余的都等于0时， H(X)= 0。
[推论2]当且仅当某个P(Xi)=1/n，i=1， 2，……， n时，H(X）有极大值log n。
####################################################################信息量####################################################################

######################################################################熵######################################################################
通常，一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。对所有可能结果带来的额外信息量求取均值（期望）就称为熵。即：
H(x) = E[I(xi)] = E[ log(2,1/P(xi)) ] = -∑P(xi)log(2,P(xi)) (i=1,2,..n)
式中对数一般取2为底，单位为比特。如果以e为底数，单位是nat；如果以10为底数，单位是det；
变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大；反之不确定性就小，熵也小。

相对熵（relative entropy），又被称为Kullback-Leibler散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布（probability distribution）间差异的非对称性度量 。在在信息理论中，相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值。
相对熵是一些优化算法，例如最大期望算法（Expectation-Maximization algorithm, EM）的损失函数 。此时参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗。
DKL(p||q)=Hp(q)−Hp(p)
显然，当p=q时,两者之间的相对熵DKL(p||q)=0
上式最后的Hp(q)表示在p分布下，使用q进行编码需要的bit数，而H(p)表示对真实分布p所需要的最小编码bit数。基于此，相对熵的意义就很明确了：DKL(p||q)表示在真实分布为p的前提下，使用q分布进行编码相对于使用真实分布p进行编码（即最优编码）所多出来的bit数。

交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。
CEH(p,q)=H(p)+DKL(p||q)
可以看出，交叉熵与相对熵仅相差了H(p),当p已知时，可以把H(p)看做一个常数，此时交叉熵与KL距离在行为上是等价的，都反映了分布p，q的相似程度。最小化交叉熵等于最小化KL距离。它们都将在p=q时取得最小值H(p)（p=q时KL距离为0）

∫是连续值求和
∑是离散值求和
∏是求积，连乘
######################################################################熵######################################################################

###################################################################优化算法###################################################################
通过反向传播（BackPropagation）和梯度下降（Gradient Descent）调整神经网络的参数，
反向传播算法是训练神经网络的核心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。
顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。梯度的求解其实就是求函数偏导的问题。
假设用θ表示神经网络中的参数，J(θ) 表示在给定的参数取值下，训练数据集上损失函数的大小，那么整个优化过程可以抽象为寻找一个参数 θ，使得 J(θ) 最小。因为目前没有一个通用的方法可以对任意损失函数直接求解最佳的参数取值，所以在实践中，梯度下降算法是最常用的神经网络优化方法。梯度下降算法会迭代式更新参数 θ，不断沿着梯度的反方向让参数朝着总损失更小的方向更新。参数的梯度可以通过求偏导的方式计算，对于参数θ，其梯度为 ：∂J(θ)/∂θ。定义学习率η（learning rate）来定义每次参数更新的幅度。
神经网络的优化过程可以分为两个阶段，第一个阶段先通过前向传播算法计算得到预测值，并将预测值和真实值作对比得出两者之间的差距。然后在第二个阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。梯度下降算法并不能保证被优化的函数达到全局最优解。只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。除了不一定能达到全局最优，梯度下降算法的另一个问题就是计算时间太长。因为要在全部训练数据上最小化损失，所以损失函数 J(θ) 是在所有训练数据上的损失和。这样在每一轮迭代中都需要计算在全部训练数据上的损失函数。
为了加速训练过程，可以使用随机梯度下降算法（stochastic gradient descent）。这个算法在每一轮迭代中，随机优化某一条训练数据的损失函数，大大加快每一轮参数更新的速度。缺点是：在某一条数据上损失函数更小并不代表在全部数据上损失函数更小，于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优。
综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中采用每次计算一小部分训练数据的损失函数的方法。这一小部分训练数据被称之为一个batch。通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多。另外，每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛的结果更加接近梯度下降的效果。

举一个非常简单的例子，如求函数f(x)=X²的最小值。
利用梯度下降的方法解题步骤如下：
1、求梯度(导数)， ▽ = 2X
2、向梯度相反的方向移动，如下：
x <- x - γ▽，
其中，γ为步长。如果步长足够小，则可以保证每一次迭代都在减小，但可能导致收敛太慢，如果步长太大，则不能保证每一次迭代都减少，也不能保证收敛。
3、循环迭代步骤2，直到x的值变化到使得f(x)在两次迭代之间的差值足够小，比如0.00000001，也就是说，直到两次迭代计算出来的f(x)基本没有变化，则说明f(x)此时已经达到局部最小值
4、此时，输出x，这个x就是使得函数f(x)最小时的的取值 。

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y),分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x,∂f/∂y)T,简称gradf(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0,∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。
那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0,∂f/∂y0)T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是-(∂f/∂x0,∂f/∂y0)T的方向，梯度减少最快，也就是更加容易找到函数的最小值。

导数、偏导数以及微分
导数描述的是函数在一点处的变化快慢的趋势，是一个变化的速率，微分描述的是函数从一点（移动一个无穷小量）到另一点的变化幅度，是一个变化的量。
导数（Derivative），也叫导函数值。又名微商，是微积分中的重要基础概念。当函数y=f(x)的自变量x在一点x0上产生一个增量Δx时，函数输出值的增量Δy与自变量增量Δx的比值在Δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f’（x0）或df(x0)/dx
f(x) = lim f(x+△x) - f(x) / △x
在一元函数中，导数就是函数的变化率。对于二元函数的“变化率”，由于自变量多了一个，情况就要复杂的多。
x方向的偏导
设有二元函数z=f(x,y)，点(x0,y0)是其定义域D内一点。把y固定在y0而让x在x0有增量△x，相应地函数z=f(x,y)有增量（称为对x的偏增量）△z=f(x0+△x,y0)-f(x0,y0)。如果△z与△x之比当△x→0时的极限存在，那么此极限值称为函数z=f(x,y)在(x0,y0)处对x的偏导数，记作f'x(x0,y0)或函数z=f(x,y)在(x0,y0)处对x的偏导数，实际上就是把y固定在y0看成常数后，一元函数z=f(x,y0)在x0处的导数。f'x(x0,y0) = lim f(x0+△x,y0) - f(x0,y0) / △x
y方向的偏导
同样，把x固定在x0，让y有增量△y，如果极限存在那么此极限称为函数z=(x,y)在(x0,y0)处对y的偏导数。记作f'y(x0,y0)。
f'y(x0,y0) = lim f(x0,y0+△y) - f(x0,y0) / △y

###################################################################优化算法###################################################################