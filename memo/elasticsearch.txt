=======================================================环境安装windows=======================================================
1、下载elasticsearch
https://www.elastic.co/cn/downloads/elasticsearch

2、elasticsearch-head (方便查看ES中的索引及数据)es5以上就需要安装node和grunt，所以安装head插件的前提，是需要把该两项配置好。
	1) node下载地址：https://nodejs.org/en/download/，下载对应环境的node版本，(绿色版路径：D:\installation_package\node-v14.15.1-win-x64，添加至环境变量并重启)
	2) 在node安装路径下，使用命令安装：npm install -g grunt-cli安装grunt。
	3) 安装结束后，使用命令grunt -version查看是否安装成功，
	4) 下载地址：https://github.com/mobz/elasticsearch-head，下载zip包，解压（路径：D:\elasticsearch\elasticsearch-head-master）
	5) 在dos窗口进入到elasticsearch-head路径下，使用命令npm install安装pathomjs
	npm install -g cnpm --registry=https://registry.npm.taobao.org
	6) 在dos窗口进入到elasticsearch-head路径下，使用命令npm run start启用服务
	npm run start
	7) 使用地址：localhost:9100访问，
	elasticsearch-head 无法连接elasticsearch的原因和解决
	首先确定的是，elasticsearch-head启动无误，elasticsearch启动无误。
	点击连接elasticsearch出现这个问题：集群健康值: 未连接
	需要在elasticsearch的elasticsearch.yml文件里面添加
	http.cors.enabled: true
    http.cors.allow-origin: "*"
    分析原因：
	elasticsearch-head发送请求的时候，跨域了，所以变成options，让options去发现有什么可以请求的方法，而options请求没有返回结果。
	
3、Kibana(方便开发通过rest api 调试ES，有代码提示)
https://www.elastic.co/cn/downloads/kibana
http://localhost:5601

4、中文分词elasticsearch-analysis-ik （ik）
	1)下载和es版本一致的ik
	https://github.com/medcl/elasticsearch-analysis-ik/releases
	2)进入es的plugins目录新建一个名为ik的子目录，并将elasticsearch-analysis-ik-7.10.0.zip包解压到该ik目录内
	D:\elasticsearch\elasticsearch-7.10.0\plugins
	3)将elascticsearch和kibana服务重启
	4)测试
	GET _analyze
	{
	  "analyzer": "ik_max_word",
	  "text": "上海自来水来自海上"
	}
	
Elasticsearch(Windows)指定单独JDK版本
修改bin/elasticsearch-env.bat
在下面这一块前面增加下面一句话：set JAVA_HOME=D:\elasticsearch\elasticsearch-7.10.0\jdk
if "%JAVA_HOME%" == "" (
  set JAVA="%ES_HOME%\jdk\bin\java.exe"
  set JAVA_HOME="%ES_HOME%\jdk"
  set JAVA_TYPE=bundled jdk
) else (
  set JAVA="%JAVA_HOME%\bin\java.exe"
  set JAVA_TYPE=JAVA_HOME
)
=======================================================环境安装windows=======================================================

=======================================================环境安装linux=======================================================
linux ===>
D:\installation_package\node-v14.15.3-linux-x64.tar.xz
D:\installation_package\kibana-7.10.1-linux-x86_64.tar.gz
D:\installation_package\elasticsearch-7.10.1-linux-x86_64.tar.gz
D:\gentleduo\repository\elasticsearch-head

修改系统变量
[root@server01 ~]# vim /etc/security/limits.conf 
* soft nofile 65535
* hard nofile 65535
* soft memlock unlimited
* hard memlock unlimited
[root@server01 ~]# vim /etc/sysctl.conf
vm.max_map_count = 262144
重启

Elasticsearch(Linux)指定单独JDK版本
修改bin/elasticsearch-env
在下面这一块前面增加下面一句话：JAVA_HOME=/usr/local/elasticsearch/jdk
if [ ! -z "$JAVA_HOME" ]; then
  JAVA="$JAVA_HOME/bin/java"
  JAVA_TYPE="JAVA_HOME"
else
  if [ "$(uname -s)" = "Darwin" ]; then
    # macOS has a different structure
    JAVA="$ES_HOME/jdk.app/Contents/Home/bin/java"
  else
    JAVA="$ES_HOME/jdk/bin/java"
  fi
  JAVA_TYPE="bundled jdk"
fi

编辑配置文件(/usr/local/elasticsearch/config/elasticsearch.yml)========>
# 设置集群名称，集群内所有节点的名称必须一致。
cluster.name: es-prod
# 设置节点名称，集群内节点名称必须唯一,且不可重复
node.name: node-1
# 数据存储目录的路径(用逗号分隔多个位置):
path.data: /usr/local/elasticsearch/data
# 日志存储路径:
path.logs: /usr/local/elasticsearch/logs
# 启动时锁定内存,内存不足2G请勿开启
bootstrap.memory_lock: true
# 将绑定地址设置为特定的IP（IPv4或IPv6
network.host: 192.168.56.110
# 为HTTP设置自定义端口
http.port: 9200
# 在启动此节点时，传递主机的初始列表以执行发现.主机的默认列表是: ["127.0.0.1", "[::1]"]
# 写入候选主节点的设备地址，在开启服务后可以被选为主节点
discovery.seed_hosts: ["192.168.56.110", "192.168.56.111", "192.168.56.112"]
# 初始化一个新的集群时需要此配置来选举master
cluster.initial_master_nodes: ["192.168.56.110", "192.168.56.111", "192.168.56.112"]

es5之后的都不能使用添加启动参数或者修改配置文件等方法启动了，必须要创建用户
1、创建用户：elasticsearch
adduser elasticsearch
2、创建用户密码，需要输入两次
passwd elasticsearch
3、将对应的文件夹权限赋给该用户
chown -R elasticsearch /usr/local/elasticsearch
4、切换至elasticsearch用户
su elasticsearch
5、进入启动目录启动/usr/local/elasticsearch/bin  使用后台启动方式：./elasticsearch -d
6、启动后测试
输入curl server01:9200,如果返回一个json数据说明启动成功
查看集群中节点状况：curl -X GET server01:9200/_cat/nodes?v
7、停止es
正常停止一个进程
kill -SIGTERM 15516
kill -15 15516
彻底杀死进程
kill -KILL 15516
kill -9 123456

Elasticsearch-head插件的安装与配置
1.安装node.js
1.1、通过官网下载二进制安装包
https://nodejs.org/en/download/
1.2、解压安装包
tar -xJf node-v10.16.3-linux-x64.tar.xz -C /usr/local/
1.3、配置环境变量
vi /etc/profile
在文件最后面追加node.js环境变量
export NODE_HOME=/usr/local/node
export PATH=$NODE_HOME/bin:$PATH
1.4、重新加载配置文件并验证是否安装成功
source /etc/profile
node -v
npm -v
2.head插件的安装与配置
2.1、下载并安装head插件
git clone git://github.com/mobz/elasticsearch-head.git
cd elasticsearch-head/
npm install -g
2.2、配置elasticsearch，允许head插件远程访问
cd /usr/local/elasticsearch/config/
vi elasticearch.yml
在配置文件末尾添加如下内容，重新启动elasticsearch服务
http.cors.enabled: true
http.cors.allow-origin: "*"
2.3、启动elasticsearch-head服务
修改/opt/elasticsearch-head/Gruntfile.js，设置远程访问：
connect: {
		server: {
				options: {
						hostname: '远程主机IP'
						port: 9100,
						base: '.',
						keepalive: true
				}
		}
}
修改/opt/elasticsearch-head/_site目录下的app.js文件
this.base_uri = this.config.base_uri || this.prefs.get("app-base_uri") || "http://es服务器的IP:端口";
cd /opt/elasticsearch-head/
npm run start

Kibana安装：
1.下载es对应的kibana版本：同样是这个地址：https://elasticsearch.cn/download/
2.解压：tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz
3.配置：vim config/kibana.yml
　　server.port: 5601    端口
　　server.host: "0.0.0.0" 开放外网访问
　　elasticsearch.hosts: ["http://192.168.56.110:9200","http://192.168.56.111:9200","http://192.168.56.112:9200"]  es的服务器
4.启动服务：./bin/kibana
　　也可以后台启动kibana：nohup ./bin/kibana &
　　这样日志就在当前目录的nohup.out文件中
注意启动用户使用elasticsearch的启动用户保持一致，不允许使用root用户启动。启动提示如下错误
FATAL Error: Unable to write Kibana UUID file, please check the uuid.server configuration value in kibana.yml and ensure Kibana has sufficient permissions to read / write to this file. Error was: EACCES
解决:
# 在root下为 为kibana赋权
chown -R elasticsearch /opt/kibana-7.10.1-linux-x86_64
# 修改kibana所在文件夹的权限
chmod 770 kibana-7.10.1-linux-x86_64
# 切换回用户组 elasticsearch
su elasticsearch
# 启动
./bin/kibana
=======================================================环境安装linux=======================================================

=======================================================基本概念=======================================================
shard========>
index会拆分多个shard，每个shard就会存放这个index的一份数据，这些shard会散落在多台服务器上面，这样做的好处：
1.横向扩展，比如数据量从3T增加到了4T，那么会重新建立一个有4个shard的索引，将数据导入进去
2.数据分布在多个shard，多台服务器上，所有的操作都会在多台服务器上进行分布式执行，提升吞吐量和性能
shard其实就是primary shard，一般称为shard
replica其实就是replica shard，一般简称为replica
（1）index包含多个shard
（2）每个shard都是一个最小工作单元，承载部分数据，lucene实例，完整的建立索引和处理请求的能力
（3）增减节点时，shard会自动在nodes中负载均衡
（4）primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard
（5）replica shard是primary shard的副本，负责容错，以及承担读请求负载
（6）primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改
（7）primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard
（8）primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上
在es中master节点不承载所有的请求，只负责管理es集群的元数据，所以es属于节点平等的分布式架构
############################节点平等的分布式架构############################
（1）节点对等，每个节点都能接收所有的请求
（2）自动请求路由
（3）响应收集
############################节点平等的分布式架构############################

Document、Type、Index========>
Document：文档，ES中最小的数据单元，一个document可以是一条商品数据，通常用JSON数据结构表示，每个index下的type中可以存储多个document。
Type：类型，type是index中的一个逻辑数据分类，一个type下的document有相同的field。
Index：索引，包含很多相似结构的文档数据。每个索引下面可以有一个或多个type，type是index中的一个逻辑数据分类
Elasticsearch    数据库
Document         行
Type             表
Index            库
在7.0以及之后的版本中Type被废弃了。一个index中只有一个默认的type，即_doc：
ES的Type被废弃后，库表合一，Index既可以被认为对应MySQL的Database，也可以认为对应table，也可以这样理解：
ES实例：对应MySQL实例中的一个Database。
Index对应MySQL中的Table。
Document对应MySQL中表的记录。

document数据格式========>
面向文档的搜索分析引擎
（1）应用系统的数据结构都是面向对象的，复杂的
（2）对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦
（3）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能
（4）es的document用json数据格式来表达
比如：employee对象：里面包含了Employee类自己的属性，还有一个EmployeeInfo对象
如果是数据库的话，则是如下设计：
两张表：employee表，employee_info表，将employee对象的数据重新拆开来，变成Employee数据和EmployeeInfo数据
employee表：email，first_name，last_name，join_date，4个字段
employee_info表：bio，age，interests，3个字段；此外还有一个外键字段，比如employee_id，关联着employee表
用es的话数据会存储在一个document中：
{
    "email":      "zhangsan@sina.com",
    "first_name": "san",
    "last_name": "zhang",
    "info": {
        "bio":         "curious and modest",
        "age":         30,
        "interests": [ "bike", "climb" ]
    },
    "join_date": "2017/01/01"
}
这就就明白了es的document数据格式和数据库的关系型数据格式的区别

简单的集群管理========>
（1）快速检查集群的健康状况
es提供了一套api，叫做cat api，可以查看es中各种各样的数据
kibana:
GET /_cat/health?v
linux:
curl -X GET server01:9200/_cat/health?v
green：每个索引的primary shard和replica shard都是active状态的
yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态
red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了
# 检查集群状态，查看是否有节点丢失，有多少分片无法分配
GET /_cluster/health/
# 查看索引级别,找到红色的索引
GET /_cluster/health?level=indices
#查看索引的分片
GET /_cluster/health?level=shards
# Explain 变红的原因
GET /_cluster/allocation/explain
（2）快速查看集群中有哪些索引
GET /_cat/indices?v&pretty
（3）简单的索引操作
创建索引：PUT /test_index?pretty
删除索引：DELETE /test_index?pretty
################################################ES集群Unassigned（脑裂现象）################################################
现象：
后台启动ES集群，由三个节点组成，集群健康值为yellow，节点只有主分片，副本状态为：Unassigned
原因：
在ES磁盘分配分片控制策略中，为了保护数据节点的安全，ES对磁盘进行了限额，并会定时检查各节点数据目录的使用情况：
cluster.info.update.interval // 默认30秒
在达到cluster.routing.allocation.disk.watermark.low(默认85%)时，新索引分片就不会分配到这个节点上了。
在达到cluster.routing.allocation.disk.watermark.high(默认90%)时，就会触发该节点现存分片的数据均衡，把数据挪到其他节点上去。
如何修改：
方案一：删除es集群所在路径不必要大文件，如旧的日志文件，临时文件等，使 Use% 值小于 85%。
方案二：修改ES分片控制策略，提高cluster.routing.allocation.disk.watermark.low的值，该值大于当前ES集群所在路径%Use的值。
curl -X PUT -H "Content-Type:application/json" server01:9200/_cluster/settings -d '{
   "transient":{
      "cluster.routing.allocation.disk.watermark.high":"95%",
      "cluster.routing.allocation.disk.watermark.low":"90%"
    }
}'
################################################ES集群Unassigned（脑裂现象）################################################

document的核心元数据========>
{
  "_index" : "test_index",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "test-key" : "test-val"
  }
}
1、_index元数据
（1）代表一个document存放在哪个index中
（2）类似的数据放在一个索引，非类似的数据放不同索引：product index（包含了所有的商品），sales index（包含了所有的商品销售数据），inventory index（包含了所有库存相关的数据）。如果你把比如product，sales，human resource（employee），全都放在一个大的index里面，比如说company index，不合适的。
（3）index中包含了很多类似的document：类似是什么意思，其实指的就是说，这些document的fields很大一部分是相同的，你说你放了3个document，每个document的fields都完全不一样，这就不是类似了，就不太适合放到一个index里面去了。
（4）索引名称必须是小写的，不能用下划线开头，不能包含逗号：product，website，blog
2、_type元数据
（1）代表document属于index中的哪个类别（type）
（2）一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类：因为一批相同的数据，可能有很多相同的fields，但是还是可能会有一些轻微的不同，可能会有少数fields是不一样的，举个例子，就比如说，商品，可能划分为电子商品，生鲜商品，日化商品，等等。
（3）type名称可以是大写或者小写，但是同时不能用下划线开头，不能包含逗号
3、_id元数据
（1）代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document
（2）我们可以手动指定document的id（put /index/type/id），也可以不指定，由es自动为我们创建一个id
4、_source元数据
_source元数据：就是说在创建一个document的时候，使用的那个放在request body中的json串，默认情况下，在get的时候，会原封不动的给我们返回回来。
定制返回的结果，指定_source中，返回哪些field GET /test_index/test_type/1?_source=test_field1,test_field2
5、_version元数据
第一次创建一个document的时候，它的_version内部版本号就是1；以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1；哪怕是删除，也会对这条数据的版本号加1，我们会发现，在删除一个document之后，可以从一个侧面证明，它不是立即物理删除掉的，因为它的一些版本号等信息还是保留着的。先删除一条document，再重新创建这条document，其实会在delete version基础之上，再把version号加1
=======================================================基本概念=======================================================

=======================================================基本操作=======================================================
基本的CRUD操作（在DSL语句结尾加上?pretty可以更清晰直观的看到执行结果）========>
（1）新增商品：新增文档，建立索引
PUT /index/type/id
{
  "json数据"
}
PUT /ecommerce/_doc/1?pretty
{
    "name" : "gaolujie yagao",
    "desc" :  "gaoxiao meibai",
    "price" :  30,
    "producer" :      "gaolujie producer",
    "tags": [ "meibai", "fangzhu" ]
}
PUT /ecommerce/_doc/2?pretty
{
    "name" : "jiajieshi yagao",
    "desc" :  "youxiao fangzhu",
    "price" :  25,
    "producer" :      "jiajieshi producer",
    "tags": [ "fangzhu" ]
}
PUT /ecommerce/_doc/3?pretty
{
    "name" : "zhonghua yagao",
    "desc" :  "caoben zhiwu",
    "price" :  40,
    "producer" :      "zhonghua producer",
    "tags": [ "qingxin" ]
}
es会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索
（2）查询商品：检索文档
GET /index/type/id
GET /ecommerce/_doc/1?pretty
（3）修改商品：替换文档
PUT /ecommerce/_doc/1?pretty
{
    "name" : "jiaqiangban gaolujie yagao",
    "desc" :  "gaoxiao meibai",
    "price" :  30,
    "producer" :      "gaolujie producer",
    "tags": [ "meibai", "fangzhu" ]
}
替换方式必须带上所有的field，才能去进行信息的修改，否则其他字段将被置空
（4）修改商品：更新文档
POST /ecommerce/_update/1/?pretty
{
  "doc": {
    "name": "jiaqiangban gaolujie yagao"
  }
}
curl -X POST -H 'Content-Type:application/json' http://server01:9200/ecommerce/_update/1 -d '{
    "doc": {
	    "name": "gaolujie yagao"
	}
}'
（5）删除商品：删除文档
DELETE /ecommerce/_doc/1

###############################################搜索方式###############################################
query string search========>
搜索全部商品：GET /ecommerce/_search?pretty
curl -X GET http://server01:9200/ecommerce/_search?pretty
took：耗费了几毫秒
timed_out：是否超时，这里是没有
_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard也可以）
hits.total：查询结果的数量，3个document
hits.max_score：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高
hits.hits：包含了匹配搜索的document的详细数据
query string search的由来，因为search参数都是以http请求的query string来附带的
搜索商品名称中包含yagao的商品，而且按照售价降序排序：GET /ecommerce/product/_search?q=name:yagao&sort=price:desc
适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；但是如果查询请求很复杂，是很难去构建的
在生产环境中，几乎很少使用query string search

query DSL========>
DSL：Domain Specified Language，特定领域的语言
http request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了
查询所有的商品
GET /ecommerce/_search
{
  "query": { "match_all": {} }
}
查询名称包含yagao的商品，同时按照价格降序排序
GET /ecommerce/_search
{
    "query" : {
        "match" : {
            "name" : "yagao"
        }
    },
    "sort": [
        { "price": "desc" }
    ]
}
分页查询商品，总共3条商品，假设每页就显示1条商品，现在显示第2页，所以就查出来第2个商品
GET /ecommerce/_search
{
  "query": { "match_all": {} },
  "from": 1,
  "size": 1
}
指定要查询出来商品的名称和价格就可以
GET /ecommerce/_search
{
  "query": { "match_all": {} },
  "_source": ["name", "price"]
}

query filter========>
搜索商品名称包含yagao，而且售价大于25元的商品
GET /ecommerce/_search
{
    "query" : {
        "bool" : {
            "must" : {
                "match" : {
                    "name" : "yagao" 
                }
            },
            "filter" : {
                "range" : {
                    "price" : { "gt" : 25 } 
                }
            }
        }
    }
}

full-text search（全文检索）======>
增加一条数据：
PUT /ecommerce/_doc/4
{
  "name" : "special yagao",
  "desc" : "special meibai",
  "price" : 50,
  "producer" : "special yagao producer",
  "tags" : [
	"meibai"
  ]
}

phrase search（短语搜索）======>
跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回
phrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回
GET /ecommerce/_search
{
    "query" : {
        "match_phrase" : {
            "producer" : "yagao producer"
        }
    }
}

highlight search（高亮搜索结果）======>
GET /ecommerce/_search
{
    "query" : {
        "match" : {
            "producer" : "producer"
        }
    },
    "highlight": {
        "fields" : {
            "producer" : {}
        }
    }
}
###############################################搜索方式###############################################

###############################################聚合下钻###############################################
在聚合操作前必须将字段的倒排索引改为正排索引：
PUT /ecommerce/_mapping
{
  "properties": {
    "tags": {
      "type": "text",
      "fielddata": true
    }
  }
}
计算每个tag下的商品数量(size=0表示只返回聚合结果，不返回每条数据的具体信息)
GET /ecommerce/_search
{
  "size": 0,
  "aggs": {
    "group_by_tags": {
      "terms": {
        "field": "tags"
      }
    }
  }
}
对名称中包含yagao的商品，计算每个tag下的商品数量
GET /ecommerce/_search
{
  "size": 0, 
  "query": {
    "match": {
      "name": "yagao"
    }
  },
  "aggs": {
    "all_tags": {
      "terms": {
        "field": "tags"
      }
    }
  }
}
先分组，再算每组的平均值，计算每个tag下的商品的平均价格
GET /ecommerce/_search
{
    "size": 0,
    "aggs" : {
        "group_by_tags" : {
            "terms" : { "field" : "tags" },
            "aggs" : {
                "avg_price" : {
                    "avg" : { "field" : "price" }
                }
            }
        }
    }
}
计算每个tag下的商品的平均价格，并且按照平均价格降序排序
GET /ecommerce/_search
{
    "size": 0,
    "aggs" : {
        "all_tags" : {
            "terms" : { "field" : "tags", "order": { "avg_price": "desc" } },
            "aggs" : {
                "avg_price" : {
                    "avg" : { "field" : "price" }
                }
            }
        }
    }
}
按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格
GET /ecommerce/_search
{
  "size": 0,
  "aggs": {
    "group_by_price": {
      "range": {
        "field": "price",
        "ranges": [
          {
            "from": 0,
            "to": 20
          },
          {
            "from": 20,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_tags": {
          "terms": {
            "field": "tags"
          },
          "aggs": {
            "average_price": {
              "avg": {
                "field": "price"
              }
            }
          }
        }
      }
    }
  }
}
###############################################聚合下钻###############################################

###############################################DocumentId###############################################
一般来说，是从某些其他的系统中，导入一些数据到es时，会采取这种方式，就是使用系统中已有数据的唯一标识，作为es中document的id。举个例子，比如说，我们现在在开发一个电商网站，做搜索功能，或者是OA系统，做员工检索功能。这个时候，数据首先会在网站系统或者IT系统内部的数据库中，会先有一份，此时就肯定会有一个数据库的primary key（自增长，UUID，或者是业务编号）。如果将数据导入到es中，此时就比较适合采用数据在数据库中已有的primary key。
如果说，我们是在做一个系统，这个系统主要的数据存储就是es一种，也就是说，数据产生出来以后，可能就没有id，直接就放es一个存储，那么这个时候，可能就不太适合说手动指定document id的形式了，因为你也不知道id应该是什么，此时可以采取下面要讲解的让es自动生成id的方式。
1、手动指定document id
POST /test_index/_doc/1?pretty
{
  "test-key":"test-val"
}
{
  "_index" : "test_index",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 2,
  "_primary_term" : 1
}
2、自动生成document id
POST /test_index/_doc?pretty
{
  "test-key":"test-val"
}
{
  "_index" : "test_index",
  "_type" : "_doc",
  "_id" : "VARAgnwB0oZrrWtReH5u",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 1,
  "_primary_term" : 1
}
自动生成的id，采用GUID算法，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突
3、document的全量替换
（1）语法与创建文档是一样的，如果document id不存在，那么就是创建；如果document id已经存在，那么就是全量替换操作，替换document的json串内容
（2）document是不可变的，如果要修改document的内容，第一种方式就是全量替换，直接对document重新建立索引，替换里面所有的内容
（3）es会将老的document标记为deleted，然后新增我们给定的一个document，当我们创建越来越多的document的时候，es会在适当的时机在后台自动删除标记为deleted的document
4、document的强制创建
（1）创建文档与全量替换的语法是一样的，有时我们只是想新建文档，不想替换文档，如果强制进行创建呢？
（2）PUT /index/type/id?op_type=create，PUT /index/type/id/_create
5、document的删除
（1）DELETE /index/type/id
（2）不会理解物理删除，只会将其标记为deleted，当数据越来越多的时候，在后台自动删除
###############################################DocumentId###############################################

=======================================================基本操作=======================================================

=======================================================mapping映射管理=======================================================
_all metadata的原理和作用========>
GET /test_index/test_type/_search?q=test_field:test   //查找字段test_field的值中包含test的document
GET /test_index/test_type/_search?q=+test_field:test  //查找字段test_field的值中包含test的document
GET /test_index/test_type/_search?q=-test_field:test  //查找字段test_field的值中不包含test的document
GET /test_index/test_type/_search?q=test              //直接可以搜索所有的field，任意一个field包含指定的关键字就可以搜索出来。
真正在搜索的时候，不是对document中的每一个field都进行一次搜索的。es在插入一条document的时候，会自动将多个field的值，全部用字符串的方式串联起来，变成一个长的字符串，作为_all field的值，同时建立索引。
后面如果在搜索的时候，没有对某个field指定搜索，就默认搜索_all field，其中是包含了所有field的值的（es7好像没有而这个元数据了）

exact value========>
2017-01-01，exact value，搜索的时候，必须输入2017-01-01，才能搜索出来；如果你输入一个01，是搜索不出来的

full text========>
（1）缩写 vs. 全程：cn vs. china
（2）格式转化：like liked likes
（3）大小写：Tom vs tom
（4）同义词：like vs love
2017-01-01，2017 01 01，搜索2017，或者01，都可以搜索出来
china，搜索cn，也可以将china搜索出来
likes，搜索like，也可以将likes搜索出来
Tom，搜索tom，也可以将Tom搜索出来
like，搜索love，同义词，也可以将like搜索出来
就不是说单纯的只是匹配完整的一个值，而是可以对值进行拆分词语后（分词）进行匹配，也可以通过缩写、时态、大小写、同义词等进行匹配

映射(mapping)介绍========>
（1）往es里面直接插入数据，es会自动建立索引，同时建立type以及对应的mapping
（2）mapping中就自动定义了每个field的数据类型
（3）不同的数据类型（比如说text和date），可能有的是exact value，有的是full text
（4）exact-value，在建立倒排索引的时候，分词的时候，是将整个值一起作为一个关键词建立到倒排索引中的；full-text，会经历各种各样的处理，分词，normaliztion（时态转换，同义词转换，大小写转换），才会建立到倒排索引中
（5）同时呢，exact-value和full-text类型的field就决定了，在一个搜索过来的时候，对exact-value-field或者是full-text-field进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致；比如说exact-value搜索的时候，就是直接按照整个值进行匹配，full-text-query-string，也会进行分词和normalization再去倒排索引中去搜索
（6）可以用es的dynamic-mapping，让其自动建立mapping，包括自动设置数据类型；也可以提前手动创建index和type的mapping，自己对各个field进行设置，包括数据类型，包括索引行为，包括分词器，等等
内置映射类型(也就是数据类型)
string类型：text,keyword两种
　　text类型：会进行分词，抽取词干，建立倒排索引（相当于full text）
　　keyword类型：就是一个普通字符串，只能完全匹配才能搜索到（相当于exact value）
数字类型：long,integer,short,byte,double,float
日期类型：date
bool(布尔)类型：boolean
binary(二进制)类型：binary
复杂类型：object,nested
geo(地区)类型：geo-point,geo-shape
专业类型：ip,competion

mapping的root object，包括：properties，metadata（_id，_source，_type），settings（number_of_shards）========>
properties：
type 字段类型
index 是否分词
analyzer 分词器类型
_source：
（1）查询的时候，直接可以拿到完整的document，不需要先拿document id，再发送一次请求拿document
（2）partial update基于_source实现
（3）reindex时，直接基于_source实现，不需要从数据库（或者其他外部存储）查询数据再修改
（4）可以基于_source定制返回field
（5）debug query更容易，因为可以直接看到_source

查看mapping结构========>
GET /website/_mapping

测试某索引中的某个字段的分词========>
GET /website/_analyze
{
  "field": "title",
  "text":"my first article"
}

创建索引========>
PUT /my_index
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "my_field": {
        "type": "text",
        "index": true,
		    "analyzer": "whitespace"
      }
    },
    "_source": {"enabled": true}
  }
}

修改索引========>
PUT /my_index/_settings
{
    "number_of_replicas": 1
}

删除索引========>
DELETE /my_index

=======================================================mapping映射管理=======================================================

=======================================================正排、倒排索引=======================================================
1、不分词的所有field（如：long、integer等）可以执行聚合操作；因为在创建索引的时候doc_values的属性默认为true，所以会自动生成doc_value正排索引，针对这些不分词的field执行聚合操作的时候，自动就会用doc_value来执行
的doc_values属性有两个值，true、false。默认为true，即开启。
当doc_values为fasle时，无法基于该字段排序、聚合、在脚本中访问字段值。
当doc_values为true时，会创建正排索引：doc_value，也会写入磁盘文件中，然后呢，os_cache先进行缓存，以提升访问doc_value正排索引的性能,如果os_cache内存大小不足够放得下整个正排索引，doc_value，就会将doc_value的数据写入磁盘文件中
示例1: 关闭 doc_values 属性
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "keyword",
        "doc_values": false
      },
      "age" : {
        "type" : "integer",
        "doc_values": false
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
查询数据并按照age排序，会报错
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"age": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
查询数据并按照name排序，同样会报错
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"name": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
获取age平均值，也会报错
POST student/_search
{
  "aggs":{
    "age_stat": {
      "avg": {"field": "age"}
    }
  },
  "from": 0
}
示例2: 开启 doc_values 属性
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "keyword",
        "doc_values": true
      },
      "age" : {
        "type" : "integer",
        "doc_values": true
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
查询数据并按照age排序，正常执行
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"age": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
查询数据并按照name排序，正常执行
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"name": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
获取age平均值，正常执行
POST student/_search
{
  "aggs":{
    "age_stat": {
      "avg": {"field": "age"}
    }
  },
  "from": 0,
  "size": 0
}

2、对于分词的field（如：text），由于在创建索引的时候不会给它建立doc_value正排索引，所以无法基于该字段排序、聚合；如果一定要对分词的field执行聚合，那么必须将fielddata=true，然后es就会在执行聚合操作的时候，现场将field对应的数据，建立一份fielddata正排索引，fielddata正排索引的结构跟doc_value是类似的，但是只会将fielddata正排索引加载到内存中来，然后基于内存中的fielddata正排索引执行分词field的聚合操作
示例1: 对于分词的field执行aggregation，会报错
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "text",
        "fields" : {
          "keyword" : {
            "type" : "keyword",
            "ignore_above" : 256
          }
        }
      },
      "age" : {
        "type" : "integer"
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
以name进行分组会报错
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name"
      }
    }
  }
}

示例2：使用类型为keyword的内置field，可以正常执行聚合操作
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name.keyword"
      }
    }
  }
}

示例3: 对于分词的field，设置fielddata为true，可以正常执行聚合操作
POST /student/_mapping
{
  "properties": {
    "name": {
      "type": "text",
      "fielddata": true
    }
  }
}
可以正常执行聚合操作
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name"
      }
    }
  }
}
=======================================================正排、倒排索引=======================================================

=================================================query、filter及多搜索条件组合=================================================
1、filter与query对比
filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响
query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序
一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter
除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可

2、filter与query性能
filter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据
query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果

3、多搜索条件组合
POST _bulk
{ "index" : { "_index" : "article", "_id" : "1" } }
{ "title" : "how to make millions", "tag" : "spam", "date" : "2014-01-01", "price" : 29.99,"category" : "ebooks" }
{ "index" : { "_index" : "article", "_id" : "2" } }
{ "title" : "java thread knowledge", "tag" : "java", "date" : "2015-01-01", "price" : 100,"category" : "java" }
{ "index" : { "_index" : "article", "_id" : "3" } }
{ "title" : "how study hadoop", "tag" : "starred", "date" : "2017-01-01", "price" : 299,"category" : "hadoop" }

bool用于多搜索条件组合查询，bool里面可以使用must，must_not，should，filter
例：
GET /article/_search
{
  "query": {
    "bool": {
        "must":     { "match": { "title": "how to make millions" }},
        "must_not": { "match": { "tag":   "spam" }},
        "should": [
            { "match": { "tag": "starred" }}
        ],
        "filter": {
          "bool": { 
              "must": [
                  { "range": { "date": { "gte": "2014-01-01" }}},
                  { "range": { "price": { "gte": 29.99 }}}
              ],
              "must_not": [
                  { "term": { "category": "ebooks" }}
              ]
          }
        }
    }
  }
}
每个子查询都会计算一个document针对它的相关度分数，然后bool综合所有分数，合并为一个分数，但是filter是不会计算分数的

4、单独使用filter进行过滤（注意必须加constant_score）
GET /article/_search
{
  "query": {
    "constant_score": {
        "filter": {
          "bool": { 
              "must": [
                  { "range": { "date": { "gte": "2014-01-01" }}},
                  { "range": { "price": { "gte": 29.99 }}}
              ],
              "must_not": [
                  { "term": { "category": "ebooks" }}
              ]
          }
        }
    }
  }
}

=================================================query、filter及多搜索条件组合=================================================

=================================================term query与match query区别=================================================
1.首先设置索引名称为my_index，设置该索引的full_text字段类型为text，exact_value字段类型为keyword（表示该字段不分词）。
PUT my_index  
{  
  "mappings": {  
	  "properties": {  
  		"full_text": {  
  		  "type":  "text"    
  		},  
  		"exact_value": {  
  		  "type":  "keyword"
  		}  
	  }  
  }  
}
2.添加一条数据
PUT my_index/_doc/1  
{  
  "full_text":   "Quick Foxes!",    
  "exact_value": "Quick Foxes!"     
}
3.执行查询
（1）使用term查询exact_value,搜索内容为Quick Foxes!
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "exact_value": "Quick Foxes!"    
        }  
      }  
    }
    由于exact_value不分词，Quick Foxes!与exact_value的值QuickFoxes!匹配，因此可以匹配
（2）使用term查询full_text,搜索内容为Quick Foxes!
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "full_text": "Quick Foxes!"    
        }  
      }  
    }
	由于full_text字段默认使用标准分析器分词，在倒排索引中被分为quick和foxes，因此使用 Quick Foxes!匹配不到内容
（3）使用term查询full_text,搜索内容为foxes
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "full_text": "foxes"    
        }  
      }
    }
    由于full_text字段默认使用标准分析器分词，在倒排索引中被分为quick和foxes，因此使用 foxes可以匹配到
（4）使用match查询full_text,查询内容为Quick Foxes!
	GET my_index/_doc/_search  
	{  
	  "query": {  
		"match": {  
		  "full_text": "Quick Foxes!"    
		}  
	  }  
	}
    使用match搜索，先分析搜索字符串Quick Foxes!，对它分词，然后搜索full_text中含有quick或者foxes或者两者都包含的文档，由于full_text字在倒排索引中被分为quick和foxes,因此可以匹配到.

总结：
match query搜索的时候，首先会解析查询字符串，进行分词，然后查询，而term query,输入的查询内容是什么，就会按照什么去查询，并不会解析查询内容，对它分词。
=================================================term query与match query区别=================================================

==============================================================分词==============================================================
standard (过滤标点符号)
GET /_analyze
{
  "analyzer": "standard",
  "text": "The programmer's holiday is 1024!"
}
simple (过滤数字和标点符号)
GET /_analyze
{
  "analyzer": "simple",
  "text": "The programmer's holiday is 1024!"
}
whitespace (不过滤，按照空格分隔)
GET /_analyze
{
  "analyzer": "whitespace",
  "text": "The programmer's holiday is 1024!"
}
stop (过滤停顿单词及标点符号，例如is are等等)
GET /_analyze
{
  "analyzer": "stop",
  "text": "The programmer's holiday is 1024!"
}
keyword (视为一个整体不进行任何处理)
GET /_analyze
{
  "analyzer": "keyword",
  "text": "The programmer's holiday is 1024!"
}
中文分词
因为 Elasticsearch 默认的分词器只能按照单字进行拆分，无法具体分析其语意等，所以我们使用analysis-icu来代替默认的分词器。
GET /_analyze
{
  "analyzer": "standard",
  "text": "南京市长江大桥"
}
通过命令./bin/elasticsearch-plugin install analysis-icu进行安装
GET /_analyze
{
  "analyzer": "icu_analyzer",
  "text": "南京市长江大桥"
}
其他的中文分词器
elasticsearch-thulac-plugin 支持中文分词和词性标注功能
https://github.com/microbun/elasticsearch-thulac-plugin
elasticsearch-analysis-ik 支持热更新分词字典及自定义词库
https://github.com/medcl/elasticsearch-analysis-ik

==============================================================分词==============================================================