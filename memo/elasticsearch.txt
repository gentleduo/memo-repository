=======================================================环境安装windows=======================================================
1、下载elasticsearch
https://www.elastic.co/cn/downloads/elasticsearch

2、elasticsearch-head (方便查看ES中的索引及数据)es5以上就需要安装node和grunt，所以安装head插件的前提，是需要把该两项配置好。
	1) node下载地址：https://nodejs.org/en/download/，下载对应环境的node版本，(绿色版路径：D:\installation_package\node-v14.15.1-win-x64，添加至环境变量并重启)
	2) 在node安装路径下，使用命令安装：npm install -g grunt-cli安装grunt。
	3) 安装结束后，使用命令grunt -version查看是否安装成功，
	4) 下载地址：https://github.com/mobz/elasticsearch-head，下载zip包，解压（路径：D:\elasticsearch\elasticsearch-head-master）
	5) 在dos窗口进入到elasticsearch-head路径下，使用命令npm install安装pathomjs
	npm install -g cnpm --registry=https://registry.npm.taobao.org
	6) 在dos窗口进入到elasticsearch-head路径下，使用命令npm run start启用服务
	npm run start
	7) 使用地址：localhost:9100访问，
	elasticsearch-head 无法连接elasticsearch的原因和解决
	首先确定的是，elasticsearch-head启动无误，elasticsearch启动无误。
	点击连接elasticsearch出现这个问题：集群健康值: 未连接
	需要在elasticsearch的elasticsearch.yml文件里面添加
	http.cors.enabled: true
    http.cors.allow-origin: "*"
    分析原因：
	elasticsearch-head发送请求的时候，跨域了，所以变成options，让options去发现有什么可以请求的方法，而options请求没有返回结果。
	
3、Kibana(方便开发通过rest api 调试ES，有代码提示)
https://www.elastic.co/cn/downloads/kibana
http://localhost:5601

4、中文分词elasticsearch-analysis-ik （ik）
	1)下载和es版本一致的ik
	https://github.com/medcl/elasticsearch-analysis-ik/releases
	2)进入es的plugins目录新建一个名为ik的子目录，并将elasticsearch-analysis-ik-7.10.0.zip包解压到该ik目录内
	D:\elasticsearch\elasticsearch-7.10.0\plugins
	3)将elascticsearch和kibana服务重启
	4)测试
	GET _analyze
	{
	  "analyzer": "ik_max_word",
	  "text": "上海自来水来自海上"
	}
	
Elasticsearch(Windows)指定单独JDK版本
修改bin/elasticsearch-env.bat
在下面这一块前面增加下面一句话：set JAVA_HOME=D:\elasticsearch\elasticsearch-7.10.0\jdk
if "%JAVA_HOME%" == "" (
  set JAVA="%ES_HOME%\jdk\bin\java.exe"
  set JAVA_HOME="%ES_HOME%\jdk"
  set JAVA_TYPE=bundled jdk
) else (
  set JAVA="%JAVA_HOME%\bin\java.exe"
  set JAVA_TYPE=JAVA_HOME
)
=======================================================环境安装windows=======================================================

=======================================================环境安装linux=======================================================
linux ===>
D:\installation_package\node-v14.15.3-linux-x64.tar.xz
D:\installation_package\kibana-7.10.1-linux-x86_64.tar.gz
D:\installation_package\elasticsearch-7.10.1-linux-x86_64.tar.gz
D:\gentleduo\repository\elasticsearch-head

修改系统变量
[root@server01 ~]# vim /etc/security/limits.conf 
* soft nofile 65535
* hard nofile 65535
* soft memlock unlimited
* hard memlock unlimited
[root@server01 ~]# vim /etc/sysctl.conf
vm.max_map_count = 262144
重启

Elasticsearch(Linux)指定单独JDK版本
修改bin/elasticsearch-env.bat
在下面这一块前面增加下面一句话：JAVA_HOME=/usr/local/elasticsearch/jdk
if [ ! -z "$JAVA_HOME" ]; then
  JAVA="$JAVA_HOME/bin/java"
  JAVA_TYPE="JAVA_HOME"
else
  if [ "$(uname -s)" = "Darwin" ]; then
    # macOS has a different structure
    JAVA="$ES_HOME/jdk.app/Contents/Home/bin/java"
  else
    JAVA="$ES_HOME/jdk/bin/java"
  fi
  JAVA_TYPE="bundled jdk"
fi

编辑配置文件(/usr/local/elasticsearch/config/elasticsearch.yml)========>
# 设置集群名称，集群内所有节点的名称必须一致。
cluster.name: es-prod
# 设置节点名称，集群内节点名称必须唯一,且不可重复
node.name: node-1
# 数据存储目录的路径(用逗号分隔多个位置):
path.data: /usr/local/elasticsearch/data
# 日志存储路径:
path.logs: /usr/local/elasticsearch/logs
# 启动时锁定内存,内存不足2G请勿开启
bootstrap.memory_lock: true
# 将绑定地址设置为特定的IP（IPv4或IPv6
network.host: 192.168.56.110
# 为HTTP设置自定义端口
http.port: 9200
# 在启动此节点时，传递主机的初始列表以执行发现.主机的默认列表是: ["127.0.0.1", "[::1]"]
# 写入候选主节点的设备地址，在开启服务后可以被选为主节点
discovery.seed_hosts: ["192.168.56.110", "192.168.56.111", "192.168.56.112"]
# 初始化一个新的集群时需要此配置来选举master
cluster.initial_master_nodes: ["192.168.56.110", "192.168.56.111", "192.168.56.112"]

es5之后的都不能使用添加启动参数或者修改配置文件等方法启动了，必须要创建用户
1、创建用户：elasticsearch
adduser elasticsearch
2、创建用户密码，需要输入两次
passwd elasticsearch
3、将对应的文件夹权限赋给该用户
chown -R elasticsearch /usr/local/elasticsearch
4、切换至elasticsearch用户
su elasticsearch
5、进入启动目录启动/usr/local/elasticsearch/bin  使用后台启动方式：./elasticsearch -d
6、启动后测试
输入curl server01:9200,如果返回一个json数据说明启动成功
查看集群中节点状况：curl -X GET server01:9200/_cat/nodes?v
7、停止es
正常停止一个进程
kill -SIGTERM 15516
kill -15 15516
彻底杀死进程
kill -KILL 15516
kill -9 123456

Elasticsearch-head插件的安装与配置
1.安装node.js
1.1、通过官网下载二进制安装包
https://nodejs.org/en/download/
1.2、解压安装包
tar -xJf node-v10.16.3-linux-x64.tar.xz -C /usr/local/
1.3、配置环境变量
vi /etc/profile
在文件最后面追加node.js环境变量
export NODE_HOME=/usr/local/node
export PATH=$NODE_HOME/bin:$PATH
1.4、重新加载配置文件并验证是否安装成功
source /etc/profile
node -v
npm -v
2.head插件的安装与配置
2.1、下载并安装head插件
git clone git://github.com/mobz/elasticsearch-head.git
cd elasticsearch-head/
npm install -g
2.2、配置elasticsearch，允许head插件远程访问
cd /usr/local/elasticsearch/config/
vi elasticearch.yml
在配置文件末尾添加如下内容，重新启动elasticsearch服务
http.cors.enabled: true
http.cors.allow-origin: "*"
2.3、启动elasticsearch-head服务
修改/opt/elasticsearch-head/Gruntfile.js，设置远程访问：
connect: {
		server: {
				options: {
						hostname: '远程主机IP'
						port: 9100,
						base: '.',
						keepalive: true
				}
		}
}
修改/opt/elasticsearch-head/_site目录下的app.js文件
this.base_uri = this.config.base_uri || this.prefs.get("app-base_uri") || "http://es服务器的IP:端口";
cd /opt/elasticsearch-head/
npm run start

Kibana安装：
1.下载es对应的kibana版本：同样是这个地址：https://elasticsearch.cn/download/
2.解压：tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz
3.配置：vim config/kibana.yml
　　server.port: 5601    端口
　　server.host: "0.0.0.0" 开放外网访问
　　elasticsearch.hosts: ["http://192.168.56.110:9200","http://192.168.56.111:9200","http://192.168.56.112:9200"]  es的服务器
4.启动服务：./bin/kibana
　　也可以后台启动kibana：nohup ./bin/kibana &
　　这样日志就在当前目录的nohup.out文件中
注意启动用户使用elasticsearch的启动用户保持一致，不允许使用root用户启动。启动提示如下错误
FATAL Error: Unable to write Kibana UUID file, please check the uuid.server configuration value in kibana.yml and ensure Kibana has sufficient permissions to read / write to this file. Error was: EACCES
解决:
# 在root下为 为kibana赋权
chown -R elasticsearch /opt/kibana-7.10.1-linux-x86_64
# 修改kibana所在文件夹的权限
chmod 770 kibana-7.10.1-linux-x86_64
# 切换回用户组 elasticsearch
su elasticsearch
# 启动
./bin/kibana
=======================================================环境安装linux=======================================================

=======================================================基本概念=======================================================
shard========>
index会拆分多个shard，每个shard就会存放这个index的一份数据，这些shard会散落在多台服务器上面，这样做的好处：
1.横向扩展，比如数据量从3T增加到了4T，那么会重新建立一个有4个shard的索引，将数据导入进去
2.数据分布在多个shard，多台服务器上，所有的操作都会在多台服务器上进行分布式执行，提升吞吐量和性能
shard其实就是primary shard，一般称为shard
replica其实就是replica shard，一般简称为replica
在es中master节点不承载所有的请求，只负责管理es集群的元数据，所以es属于节点平等的分布式架构
############################节点平等的分布式架构############################
（1）节点对等，每个节点都能接收所有的请求
（2）自动请求路由
（3）响应收集
############################节点平等的分布式架构############################

Document、Type、Index========>
Document：文档，ES中最小的数据单元，一个document可以是一条商品数据，通常用JSON数据结构表示，每个index下的type中可以存储多个document。
Type：类型，type是index中的一个逻辑数据分类，一个type下的document有相同的field。
Index：索引，包含很多相似结构的文档数据。每个索引下面可以有一个或多个type，type是index中的一个逻辑数据分类
Elasticsearch    数据库
Document         行
Type             表
Index            库
在7.0以及之后的版本中Type被废弃了。一个index中只有一个默认的type，即_doc：
ES的Type被废弃后，库表合一，Index既可以被认为对应MySQL的Database，也可以认为对应table，也可以这样理解：
ES实例：对应MySQL实例中的一个Database。
Index对应MySQL中的Table。
Document对应MySQL中表的记录。

document数据格式========>
面向文档的搜索分析引擎
（1）应用系统的数据结构都是面向对象的，复杂的
（2）对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦
（3）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能
（4）es的document用json数据格式来表达
比如：employee对象：里面包含了Employee类自己的属性，还有一个EmployeeInfo对象
如果是数据库的话，则是如下设计：
两张表：employee表，employee_info表，将employee对象的数据重新拆开来，变成Employee数据和EmployeeInfo数据
employee表：email，first_name，last_name，join_date，4个字段
employee_info表：bio，age，interests，3个字段；此外还有一个外键字段，比如employee_id，关联着employee表
用es的话数据会存储在一个document中：
{
    "email":      "zhangsan@sina.com",
    "first_name": "san",
    "last_name": "zhang",
    "info": {
        "bio":         "curious and modest",
        "age":         30,
        "interests": [ "bike", "climb" ]
    },
    "join_date": "2017/01/01"
}
这就就明白了es的document数据格式和数据库的关系型数据格式的区别

简单的集群管理========>
（1）快速检查集群的健康状况
es提供了一套api，叫做cat api，可以查看es中各种各样的数据
kibana:
GET /_cat/health?v
linux:
curl -X GET server01:9200/_cat/health?v
green：每个索引的primary shard和replica shard都是active状态的
yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态
red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了
# 检查集群状态，查看是否有节点丢失，有多少分片无法分配
GET /_cluster/health/
# 查看索引级别,找到红色的索引
GET /_cluster/health?level=indices
#查看索引的分片
GET _cluster/health?level=shards
# Explain 变红的原因
GET /_cluster/allocation/explain
（2）快速查看集群中有哪些索引
GET /_cat/indices?v
（3）简单的索引操作
创建索引：PUT /test_index?pretty
删除索引：DELETE /test_index?pretty
################################################ES集群Unassigned（脑裂现象）################################################
现象：
后台启动ES集群，由三个节点组成，集群健康值为yellow，节点只有主分片，副本状态为：Unassigned
原因：
在ES磁盘分配分片控制策略中，为了保护数据节点的安全，ES对磁盘进行了限额，并会定时检查各节点数据目录的使用情况：
cluster.info.update.interval // 默认30秒
在达到cluster.routing.allocation.disk.watermark.low(默认85%)时，新索引分片就不会分配到这个节点上了。
在达到cluster.routing.allocation.disk.watermark.high(默认90%)时，就会触发该节点现存分片的数据均衡，把数据挪到其他节点上去。
如何修改：
方案一：删除es集群所在路径不必要大文件，如旧的日志文件，临时文件等，使 Use% 值小于 85%。
方案二：修改ES分片控制策略，提高cluster.routing.allocation.disk.watermark.low的值，该值大于当前ES集群所在路径%Use的值。
curl -X PUT -H "Content-Type:application/json" server01:9200/_cluster/settings -d '{
   "transient":{
      "cluster.routing.allocation.disk.watermark.high":"95%",
      "cluster.routing.allocation.disk.watermark.low":"90%"
    }
}'
################################################ES集群Unassigned（脑裂现象）################################################

=======================================================基本概念=======================================================

=======================================================基本操作=======================================================
基本的CRUD操作（在DSL语句结尾加上?pretty可以更清晰直观的看到执行结果）========>
（1）新增商品：新增文档，建立索引
PUT /index/type/id
{
  "json数据"
}
PUT /ecommerce/_doc/1?pretty
{
    "name" : "gaolujie yagao",
    "desc" :  "gaoxiao meibai",
    "price" :  30,
    "producer" :      "gaolujie producer",
    "tags": [ "meibai", "fangzhu" ]
}
PUT /ecommerce/_doc/2?pretty
{
    "name" : "jiajieshi yagao",
    "desc" :  "youxiao fangzhu",
    "price" :  25,
    "producer" :      "jiajieshi producer",
    "tags": [ "fangzhu" ]
}
PUT /ecommerce/_doc/3?pretty
{
    "name" : "zhonghua yagao",
    "desc" :  "caoben zhiwu",
    "price" :  40,
    "producer" :      "zhonghua producer",
    "tags": [ "qingxin" ]
}
es会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索
（2）查询商品：检索文档
GET /index/type/id
GET /ecommerce/_doc/1?pretty
（3）修改商品：替换文档
PUT /ecommerce/_doc/1?pretty
{
    "name" : "jiaqiangban gaolujie yagao",
    "desc" :  "gaoxiao meibai",
    "price" :  30,
    "producer" :      "gaolujie producer",
    "tags": [ "meibai", "fangzhu" ]
}
替换方式必须带上所有的field，才能去进行信息的修改，否则其他字段将被置空
（4）修改商品：更新文档
POST /ecommerce/_update/1/?pretty
{
  "doc": {
    "name": "jiaqiangban gaolujie yagao"
  }
}
curl -X POST -H 'Content-Type:application/json' http://server01:9200/ecommerce/_update/1 -d '{
    "doc": {
	    "name": "gaolujie yagao"
	}
}'
（5）删除商品：删除文档
DELETE /ecommerce/_doc/1

###############################################搜索方式###############################################
query string search========>
搜索全部商品：GET /ecommerce/_search?pretty
curl -X GET http://server01:9200/ecommerce/_search?pretty
took：耗费了几毫秒
timed_out：是否超时，这里是没有
_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard也可以）
hits.total：查询结果的数量，3个document
hits.max_score：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高
hits.hits：包含了匹配搜索的document的详细数据
query string search的由来，因为search参数都是以http请求的query string来附带的
搜索商品名称中包含yagao的商品，而且按照售价降序排序：GET /ecommerce/product/_search?q=name:yagao&sort=price:desc
适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；但是如果查询请求很复杂，是很难去构建的
在生产环境中，几乎很少使用query string search

query DSL========>
DSL：Domain Specified Language，特定领域的语言
http request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了
查询所有的商品
GET /ecommerce/_search
{
  "query": { "match_all": {} }
}
查询名称包含yagao的商品，同时按照价格降序排序
GET /ecommerce/_search
{
    "query" : {
        "match" : {
            "name" : "yagao"
        }
    },
    "sort": [
        { "price": "desc" }
    ]
}
分页查询商品，总共3条商品，假设每页就显示1条商品，现在显示第2页，所以就查出来第2个商品
GET /ecommerce/_search
{
  "query": { "match_all": {} },
  "from": 1,
  "size": 1
}
指定要查询出来商品的名称和价格就可以
GET /ecommerce/_search
{
  "query": { "match_all": {} },
  "_source": ["name", "price"]
}

query filter========>
搜索商品名称包含yagao，而且售价大于25元的商品
GET /ecommerce/_search
{
    "query" : {
        "bool" : {
            "must" : {
                "match" : {
                    "name" : "yagao" 
                }
            },
            "filter" : {
                "range" : {
                    "price" : { "gt" : 25 } 
                }
            }
        }
    }
}

full-text search（全文检索）======>
增加一条数据：
PUT /ecommerce/_doc/4
{
  "name" : "special yagao",
  "desc" : "special meibai",
  "price" : 50,
  "producer" : "special yagao producer",
  "tags" : [
	"meibai"
  ]
}

phrase search（短语搜索）======>
跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回
phrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回
GET /ecommerce/_search
{
    "query" : {
        "match_phrase" : {
            "producer" : "yagao producer"
        }
    }
}

highlight search（高亮搜索结果）======>
GET /ecommerce/_search
{
    "query" : {
        "match" : {
            "producer" : "producer"
        }
    },
    "highlight": {
        "fields" : {
            "producer" : {}
        }
    }
}
###############################################搜索方式###############################################

###############################################聚合下钻###############################################
在聚合操作前必须将字段的倒排索引改为正排索引：
PUT /ecommerce/_mapping
{
  "properties": {
    "tags": {
      "type": "text",
      "fielddata": true
    }
  }
}
计算每个tag下的商品数量(size=0表示只返回聚合结果，不返回每条数据的具体信息)
GET /ecommerce/_search
{
  "size": 0,
  "aggs": {
    "group_by_tags": {
      "terms": {
        "field": "tags"
      }
    }
  }
}
对名称中包含yagao的商品，计算每个tag下的商品数量
GET /ecommerce/_search
{
  "size": 0, 
  "query": {
    "match": {
      "name": "yagao"
    }
  },
  "aggs": {
    "all_tags": {
      "terms": {
        "field": "tags"
      }
    }
  }
}
先分组，再算每组的平均值，计算每个tag下的商品的平均价格
GET /ecommerce/_search
{
    "size": 0,
    "aggs" : {
        "group_by_tags" : {
            "terms" : { "field" : "tags" },
            "aggs" : {
                "avg_price" : {
                    "avg" : { "field" : "price" }
                }
            }
        }
    }
}
计算每个tag下的商品的平均价格，并且按照平均价格降序排序
GET /ecommerce/_search
{
    "size": 0,
    "aggs" : {
        "all_tags" : {
            "terms" : { "field" : "tags", "order": { "avg_price": "desc" } },
            "aggs" : {
                "avg_price" : {
                    "avg" : { "field" : "price" }
                }
            }
        }
    }
}
按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格
GET /ecommerce/_search
{
  "size": 0,
  "aggs": {
    "group_by_price": {
      "range": {
        "field": "price",
        "ranges": [
          {
            "from": 0,
            "to": 20
          },
          {
            "from": 20,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_tags": {
          "terms": {
            "field": "tags"
          },
          "aggs": {
            "average_price": {
              "avg": {
                "field": "price"
              }
            }
          }
        }
      }
    }
  }
}
###############################################聚合下钻###############################################

=======================================================基本操作=======================================================

=======================================================mapping映射管理=======================================================
1、映射(mapping)介绍
映射：创建索引的时候，可以预先定义字段的类型以及相关属性
elasticsearch会根据json源数据的基础类型猜测你想要的字段映射，将输入的数据转换成可搜索的索引项，mapping就是我们自己定义的字段数据类型，同时告诉elasticsearch如何索引数据以及是否可以被搜索
作用：会让索引建立的更加细致和完善
类型：静态映射和动态映射
2、内置映射类型(也就是数据类型)
string类型：text,keyword两种
　　text类型：会进行分词，抽取词干，建立倒排索引
　　keyword类型：就是一个普通字符串，只能完全匹配才能搜索到
数字类型：long,integer,short,byte,double,float
日期类型：date
bool(布尔)类型：boolean
binary(二进制)类型：binary
复杂类型：object,nested
geo(地区)类型：geo-point,geo-shape
专业类型：ip,competion
=======================================================mapping映射管理=======================================================

=======================================================正排、倒排索引=======================================================
1、不分词的所有field（如：long、integer等）可以执行聚合操作；因为在创建索引的时候doc_values的属性默认为true，所以会自动生成doc_value正排索引，针对这些不分词的field执行聚合操作的时候，自动就会用doc_value来执行
的doc_values属性有两个值，true、false。默认为true，即开启。
当doc_values为fasle时，无法基于该字段排序、聚合、在脚本中访问字段值。
当doc_values为true时，会创建正排索引：doc_value，也会写入磁盘文件中，然后呢，os_cache先进行缓存，以提升访问doc_value正排索引的性能,如果os_cache内存大小不足够放得下整个正排索引，doc_value，就会将doc_value的数据写入磁盘文件中
示例1: 关闭 doc_values 属性
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "keyword",
        "doc_values": false
      },
      "age" : {
        "type" : "integer",
        "doc_values": false
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
查询数据并按照age排序，会报错
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"age": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
查询数据并按照name排序，同样会报错
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"name": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
获取age平均值，也会报错
POST student/_search
{
  "aggs":{
    "age_stat": {
      "avg": {"field": "age"}
    }
  },
  "from": 0
}
示例2: 开启 doc_values 属性
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "keyword",
        "doc_values": true
      },
      "age" : {
        "type" : "integer",
        "doc_values": true
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
查询数据并按照age排序，正常执行
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"age": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
查询数据并按照name排序，正常执行
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"name": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
获取age平均值，正常执行
POST student/_search
{
  "aggs":{
    "age_stat": {
      "avg": {"field": "age"}
    }
  },
  "from": 0,
  "size": 0
}

2、对于分词的field（如：text），由于在创建索引的时候不会给它建立doc_value正排索引，所以无法基于该字段排序、聚合；如果一定要对分词的field执行聚合，那么必须将fielddata=true，然后es就会在执行聚合操作的时候，现场将field对应的数据，建立一份fielddata正排索引，fielddata正排索引的结构跟doc_value是类似的，但是只会将fielddata正排索引加载到内存中来，然后基于内存中的fielddata正排索引执行分词field的聚合操作
示例1: 对于分词的field执行aggregation，会报错
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "text",
        "fields" : {
          "keyword" : {
            "type" : "keyword",
            "ignore_above" : 256
          }
        }
      },
      "age" : {
        "type" : "integer"
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
以name进行分组会报错
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name"
      }
    }
  }
}

示例2：使用类型为keyword的内置field，可以正常执行聚合操作
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name.keyword"
      }
    }
  }
}

示例3: 对于分词的field，设置fielddata为true，可以正常执行聚合操作
POST /student/_mapping
{
  "properties": {
    "name": {
      "type": "text",
      "fielddata": true
    }
  }
}
可以正常执行聚合操作
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name"
      }
    }
  }
}
=======================================================正排、倒排索引=======================================================

=================================================query、filter及多搜索条件组合=================================================
1、filter与query对比
filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响
query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序
一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter
除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可

2、filter与query性能
filter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据
query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果

3、多搜索条件组合
POST _bulk
{ "index" : { "_index" : "article", "_id" : "1" } }
{ "title" : "how to make millions", "tag" : "spam", "date" : "2014-01-01", "price" : 29.99,"category" : "ebooks" }
{ "index" : { "_index" : "article", "_id" : "2" } }
{ "title" : "java thread knowledge", "tag" : "java", "date" : "2015-01-01", "price" : 100,"category" : "java" }
{ "index" : { "_index" : "article", "_id" : "3" } }
{ "title" : "how study hadoop", "tag" : "starred", "date" : "2017-01-01", "price" : 299,"category" : "hadoop" }

bool用于多搜索条件组合查询，bool里面可以使用must，must_not，should，filter
例：
GET /article/_search
{
  "query": {
    "bool": {
        "must":     { "match": { "title": "how to make millions" }},
        "must_not": { "match": { "tag":   "spam" }},
        "should": [
            { "match": { "tag": "starred" }}
        ],
        "filter": {
          "bool": { 
              "must": [
                  { "range": { "date": { "gte": "2014-01-01" }}},
                  { "range": { "price": { "gte": 29.99 }}}
              ],
              "must_not": [
                  { "term": { "category": "ebooks" }}
              ]
          }
        }
    }
  }
}
每个子查询都会计算一个document针对它的相关度分数，然后bool综合所有分数，合并为一个分数，但是filter是不会计算分数的

4、单独使用filter进行过滤（注意必须加constant_score）
GET /article/_search
{
  "query": {
    "constant_score": {
        "filter": {
          "bool": { 
              "must": [
                  { "range": { "date": { "gte": "2014-01-01" }}},
                  { "range": { "price": { "gte": 29.99 }}}
              ],
              "must_not": [
                  { "term": { "category": "ebooks" }}
              ]
          }
        }
    }
  }
}

=================================================query、filter及多搜索条件组合=================================================

=================================================term query与match query区别=================================================
1.首先设置索引名称为my_index，设置该索引的full_text字段类型为text，exact_value字段类型为keyword（表示该字段不分词）。
PUT my_index  
{  
  "mappings": {  
	  "properties": {  
  		"full_text": {  
  		  "type":  "text"    
  		},  
  		"exact_value": {  
  		  "type":  "keyword"
  		}  
	  }  
  }  
}
2.添加一条数据
PUT my_index/_doc/1  
{  
  "full_text":   "Quick Foxes!",    
  "exact_value": "Quick Foxes!"     
}
3.执行查询
（1）使用term查询exact_value,搜索内容为Quick Foxes!
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "exact_value": "Quick Foxes!"    
        }  
      }  
    }
    由于exact_value不分词，Quick Foxes!与exact_value的值QuickFoxes!匹配，因此可以匹配
（2）使用term查询full_text,搜索内容为Quick Foxes!
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "full_text": "Quick Foxes!"    
        }  
      }  
    }
	由于full_text字段默认使用标准分析器分词，在倒排索引中被分为quick和foxes，因此使用 Quick Foxes!匹配不到内容
（3）使用term查询full_text,搜索内容为foxes
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "full_text": "foxes"    
        }  
      }
    }
    由于full_text字段默认使用标准分析器分词，在倒排索引中被分为quick和foxes，因此使用 foxes可以匹配到
（4）使用match查询full_text,查询内容为Quick Foxes!
	GET my_index/_doc/_search  
	{  
	  "query": {  
		"match": {  
		  "full_text": "Quick Foxes!"    
		}  
	  }  
	}
    使用match搜索，先分析搜索字符串Quick Foxes!，对它分词，然后搜索full_text中含有quick或者foxes或者两者都包含的文档，由于full_text字在倒排索引中被分为quick和foxes,因此可以匹配到.

总结：
match query搜索的时候，首先会解析查询字符串，进行分词，然后查询，而term query,输入的查询内容是什么，就会按照什么去查询，并不会解析查询内容，对它分词。
=================================================term query与match query区别=================================================