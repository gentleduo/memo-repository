=======================================================环境安装windows=======================================================
1、下载elasticsearch
https://www.elastic.co/cn/downloads/elasticsearch

2、elasticsearch-head (方便查看ES中的索引及数据)es5以上就需要安装node和grunt，所以安装head插件的前提，是需要把该两项配置好。
	1) node下载地址：https://nodejs.org/en/download/，下载对应环境的node版本，(绿色版路径：D:\installation_package\node-v14.15.1-win-x64，添加至环境变量并重启)
	2) 在node安装路径下，使用命令安装：npm install -g grunt-cli安装grunt。
	3) 安装结束后，使用命令grunt -version查看是否安装成功，
	4) 下载地址：https://github.com/mobz/elasticsearch-head，下载zip包，解压（路径：D:\elasticsearch\elasticsearch-head-master）
	5) 在dos窗口进入到elasticsearch-head路径下，使用命令npm install安装pathomjs
	npm install -g cnpm --registry=https://registry.npm.taobao.org
	6) 在dos窗口进入到elasticsearch-head路径下，使用命令npm run start启用服务
	npm run start
	7) 使用地址：localhost:9100访问，
	elasticsearch-head 无法连接elasticsearch的原因和解决
	首先确定的是，elasticsearch-head启动无误，elasticsearch启动无误。
	点击连接elasticsearch出现这个问题：集群健康值: 未连接
	需要在elasticsearch的elasticsearch.yml文件里面添加
	http.cors.enabled: true
    http.cors.allow-origin: "*"
    分析原因：
	elasticsearch-head发送请求的时候，跨域了，所以变成options，让options去发现有什么可以请求的方法，而options请求没有返回结果。
	
3、Kibana(方便开发通过rest api 调试ES，有代码提示)
https://www.elastic.co/cn/downloads/kibana
http://localhost:5601

4、中文分词elasticsearch-analysis-ik （ik）
	1)下载和es版本一致的ik
	https://github.com/medcl/elasticsearch-analysis-ik/releases
	2)进入es的plugins目录新建一个名为ik的子目录，并将elasticsearch-analysis-ik-7.10.0.zip包解压到该ik目录内
	D:\elasticsearch\elasticsearch-7.10.0\plugins
	3)将elascticsearch和kibana服务重启
	4)测试
	GET _analyze
	{
	  "analyzer": "ik_max_word",
	  "text": "上海自来水来自海上"
	}
	
Elasticsearch(Windows)指定单独JDK版本
修改bin/elasticsearch-env.bat
在下面这一块前面增加下面一句话：set JAVA_HOME=D:\elasticsearch\elasticsearch-7.10.0\jdk
if "%JAVA_HOME%" == "" (
  set JAVA="%ES_HOME%\jdk\bin\java.exe"
  set JAVA_HOME="%ES_HOME%\jdk"
  set JAVA_TYPE=bundled jdk
) else (
  set JAVA="%JAVA_HOME%\bin\java.exe"
  set JAVA_TYPE=JAVA_HOME
)
=======================================================环境安装windows=======================================================

=======================================================环境安装linux=======================================================
linux ===>
D:\installation_package\node-v14.15.3-linux-x64.tar.xz
D:\installation_package\kibana-7.10.1-linux-x86_64.tar.gz
D:\installation_package\elasticsearch-7.10.1-linux-x86_64.tar.gz
D:\gentleduo\repository\elasticsearch-head

修改系统变量
[root@server01 ~]# vim /etc/security/limits.conf 
* soft nofile 65535
* hard nofile 65535
* soft memlock unlimited
* hard memlock unlimited
[root@server01 ~]# vim /etc/sysctl.conf
vm.max_map_count = 262144
重启

Elasticsearch(Linux)指定单独JDK版本
修改bin/elasticsearch-env.bat
在下面这一块前面增加下面一句话：JAVA_HOME=/usr/local/elasticsearch/jdk
if [ ! -z "$JAVA_HOME" ]; then
  JAVA="$JAVA_HOME/bin/java"
  JAVA_TYPE="JAVA_HOME"
else
  if [ "$(uname -s)" = "Darwin" ]; then
    # macOS has a different structure
    JAVA="$ES_HOME/jdk.app/Contents/Home/bin/java"
  else
    JAVA="$ES_HOME/jdk/bin/java"
  fi
  JAVA_TYPE="bundled jdk"
fi

编辑配置文件
# 设置集群名称，集群内所有节点的名称必须一致。
cluster.name: es-prod
# 设置节点名称，集群内节点名称必须唯一,且不可重复
node.name: node-1
# 数据存储目录的路径(用逗号分隔多个位置):
path.data: /usr/local/elasticsearch/data
# 日志存储路径:
path.logs: /usr/local/elasticsearch/logs
# 启动时锁定内存,内存不足2G请勿开启
bootstrap.memory_lock: true
# 将绑定地址设置为特定的IP（IPv4或IPv6
network.host: 192.168.56.110
# 为HTTP设置自定义端口
http.port: 9200
# 在启动此节点时，传递主机的初始列表以执行发现.主机的默认列表是: ["127.0.0.1", "[::1]"]
# 写入候选主节点的设备地址，在开启服务后可以被选为主节点
discovery.seed_hosts: ["192.168.56.110", "192.168.56.111", "192.168.56.112"]
# 初始化一个新的集群时需要此配置来选举master
cluster.initial_master_nodes: ["192.168.56.110", "192.168.56.111", "192.168.56.112"]

es5之后的都不能使用添加启动参数或者修改配置文件等方法启动了，必须要创建用户
1、创建用户：elasticsearch
adduser elasticsearch
2、创建用户密码，需要输入两次
passwd elasticsearch
3、将对应的文件夹权限赋给该用户
chown -R elasticsearch /usr/local/elasticsearch
4、切换至elasticsearch用户
su elasticsearch
5、进入启动目录启动/usr/local/elasticsearch/bin  使用后台启动方式：./elasticsearch -d
6、启动后测试
输入curl server01:9200,如果返回一个json数据说明启动成功
查看集群中节点状况：curl -X GET server01:9200/_cat/nodes?v
7、停止es
正常停止一个进程
kill -SIGTERM 15516
kill -15 15516
彻底杀死进程
kill -KILL 15516
kill -9 123456

Elasticsearch-head插件的安装与配置
1.安装node.js
1.1、通过官网下载二进制安装包
https://nodejs.org/en/download/
1.2、解压安装包
tar -xJf node-v10.16.3-linux-x64.tar.xz -C /usr/local/
1.3、配置环境变量
vi /etc/profile
在文件最后面追加node.js环境变量
export NODE_HOME=/usr/local/node
export PATH=$NODE_HOME/bin:$PATH
1.4、重新加载配置文件并验证是否安装成功
source /etc/profile
node -v
npm -v
2.head插件的安装与配置
2.1、下载并安装head插件
git clone git://github.com/mobz/elasticsearch-head.git
cd elasticsearch-head/
npm install -g
2.2、配置elasticsearch，允许head插件远程访问
cd /usr/local/elasticsearch/config/
vi elasticearch.yml
在配置文件末尾添加如下内容，重新启动elasticsearch服务
http.cors.enabled: true
http.cors.allow-origin: "*"
2.3、启动elasticsearch-head服务
修改/opt/elasticsearch-head/Gruntfile.js，设置远程访问：
connect: {
		server: {
				options: {
						hostname: '远程主机IP'
						port: 9100,
						base: '.',
						keepalive: true
				}
		}
}
修改/opt/elasticsearch-head/_site目录下的app.js文件
this.base_uri = this.config.base_uri || this.prefs.get("app-base_uri") || "http://es服务器的IP:端口";
cd /opt/elasticsearch-head/
npm run start

Kibana安装：
1.下载es对应的kibana版本：同样是这个地址：https://elasticsearch.cn/download/
2.解压：tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz
3.配置：vim config/kibana.yml
　　server.port: 5601    端口
　　server.host: "0.0.0.0" 开放外网访问
　　elasticsearch.hosts: ["http://192.168.56.110:9200","http://192.168.56.111:9200","http://192.168.56.112:9200"]  es的服务器
4.启动服务：./bin/kibana
　　也可以后台启动kibana：nohup ./bin/kibana &
　　这样日志就在当前目录的nohup.out文件中
注意启动用户使用elasticsearch的启动用户保持一致，不允许使用root用户启动。启动提示如下错误
FATAL Error: Unable to write Kibana UUID file, please check the uuid.server configuration value in kibana.yml and ensure Kibana has sufficient permissions to read / write to this file. Error was: EACCES
解决:
# 在root下为 为kibana赋权
chown -R elasticsearch /opt/kibana-7.10.1-linux-x86_64
# 修改kibana所在文件夹的权限
chmod 770 kibana-7.10.1-linux-x86_64
# 切换回用户组 elasticsearch
su elasticsearch
# 启动
./bin/kibana
=======================================================环境安装linux=======================================================

=======================================================基本概念=======================================================
shard========>
index会拆分多个shard，每个shard就会存放这个index的一份数据，这些shard会散落在多台服务器上面，这样做的好处：
1.横向扩展，比如数据量从3T增加到了4T，那么会重新建立一个有4个shard的索引，将数据导入进去
2.数据分布在多个shard，多台服务器上，所有的操作都会在多台服务器上进行分布式执行，提升吞吐量和性能
shard其实就是primary shard，一般称为shard
replica其实就是replica shard，一般简称为replica
=======================================================基本概念=======================================================

=======================================================mapping映射管理=======================================================
1、映射(mapping)介绍
映射：创建索引的时候，可以预先定义字段的类型以及相关属性
elasticsearch会根据json源数据的基础类型猜测你想要的字段映射，将输入的数据转换成可搜索的索引项，mapping就是我们自己定义的字段数据类型，同时告诉elasticsearch如何索引数据以及是否可以被搜索
作用：会让索引建立的更加细致和完善
类型：静态映射和动态映射
2、内置映射类型(也就是数据类型)
string类型：text,keyword两种
　　text类型：会进行分词，抽取词干，建立倒排索引
　　keyword类型：就是一个普通字符串，只能完全匹配才能搜索到
数字类型：long,integer,short,byte,double,float
日期类型：date
bool(布尔)类型：boolean
binary(二进制)类型：binary
复杂类型：object,nested
geo(地区)类型：geo-point,geo-shape
专业类型：ip,competion
=======================================================mapping映射管理=======================================================

=======================================================正排、倒排索引=======================================================
1、不分词的所有field（如：long、integer等）可以执行聚合操作；因为在创建索引的时候doc_values的属性默认为true，所以会自动生成doc_value正排索引，针对这些不分词的field执行聚合操作的时候，自动就会用doc_value来执行
的doc_values属性有两个值，true、false。默认为true，即开启。
当doc_values为fasle时，无法基于该字段排序、聚合、在脚本中访问字段值。
当doc_values为true时，会创建正排索引：doc_value，也会写入磁盘文件中，然后呢，os_cache先进行缓存，以提升访问doc_value正排索引的性能,如果os_cache内存大小不足够放得下整个正排索引，doc_value，就会将doc_value的数据写入磁盘文件中
示例1: 关闭 doc_values 属性
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "keyword",
        "doc_values": false
      },
      "age" : {
        "type" : "integer",
        "doc_values": false
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
查询数据并按照age排序，会报错
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"age": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
查询数据并按照name排序，同样会报错
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"name": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
获取age平均值，也会报错
POST student/_search
{
  "aggs":{
    "age_stat": {
      "avg": {"field": "age"}
    }
  },
  "from": 0
}
示例2: 开启 doc_values 属性
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "keyword",
        "doc_values": true
      },
      "age" : {
        "type" : "integer",
        "doc_values": true
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
查询数据并按照age排序，正常执行
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"age": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
查询数据并按照name排序，正常执行
POST student/_search
{
  "query": {
    "match_all": {}
  },
  "sort" : [
    {"name": {"order": "desc"}}
  ],
  "from": 0,
  "size": 1
}
获取age平均值，正常执行
POST student/_search
{
  "aggs":{
    "age_stat": {
      "avg": {"field": "age"}
    }
  },
  "from": 0,
  "size": 0
}

2、对于分词的field（如：text），由于在创建索引的时候不会给它建立doc_value正排索引，所以无法基于该字段排序、聚合；如果一定要对分词的field执行聚合，那么必须将fielddata=true，然后es就会在执行聚合操作的时候，现场将field对应的数据，建立一份fielddata正排索引，fielddata正排索引的结构跟doc_value是类似的，但是只会将fielddata正排索引加载到内存中来，然后基于内存中的fielddata正排索引执行分词field的聚合操作
示例1: 对于分词的field执行aggregation，会报错
PUT student
{
  "mappings" : {
    "properties" : {
      "name" : {
        "type" : "text",
        "fields" : {
          "keyword" : {
            "type" : "keyword",
            "ignore_above" : 256
          }
        }
      },
      "age" : {
        "type" : "integer"
      }
    }
  }
}
插入数据
POST _bulk
{ "index" : { "_index" : "student", "_id" : "1" } }
{ "name" : "张三", "age": 12 }
{ "index" : { "_index" : "student", "_id" : "2" } }
{ "name" : "李四", "age": 10 }
{ "index" : { "_index" : "student", "_id" : "3" } }
{ "name" : "王五", "age": 11 }
以name进行分组会报错
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name"
      }
    }
  }
}

示例2：使用类型为keyword的内置field，可以正常执行聚合操作
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name.keyword"
      }
    }
  }
}

示例3: 对于分词的field，设置fielddata为true，可以正常执行聚合操作
POST /student/_mapping
{
  "properties": {
    "name": {
      "type": "text",
      "fielddata": true
    }
  }
}
可以正常执行聚合操作
GET /student/_search 
{
  "aggs": {
    "group_by_name_field": {
      "terms": {
        "field": "name"
      }
    }
  }
}
=======================================================正排、倒排索引=======================================================

=================================================query、filter及多搜索条件组合=================================================
1、filter与query对比
filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响
query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序
一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter
除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可

2、filter与query性能
filter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据
query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果

3、多搜索条件组合
POST _bulk
{ "index" : { "_index" : "article", "_id" : "1" } }
{ "title" : "how to make millions", "tag" : "spam", "date" : "2014-01-01", "price" : 29.99,"category" : "ebooks" }
{ "index" : { "_index" : "article", "_id" : "2" } }
{ "title" : "java thread knowledge", "tag" : "java", "date" : "2015-01-01", "price" : 100,"category" : "java" }
{ "index" : { "_index" : "article", "_id" : "3" } }
{ "title" : "how study hadoop", "tag" : "starred", "date" : "2017-01-01", "price" : 299,"category" : "hadoop" }

bool用于多搜索条件组合查询，bool里面可以使用must，must_not，should，filter
例：
GET /article/_search
{
  "query": {
    "bool": {
        "must":     { "match": { "title": "how to make millions" }},
        "must_not": { "match": { "tag":   "spam" }},
        "should": [
            { "match": { "tag": "starred" }}
        ],
        "filter": {
          "bool": { 
              "must": [
                  { "range": { "date": { "gte": "2014-01-01" }}},
                  { "range": { "price": { "gte": 29.99 }}}
              ],
              "must_not": [
                  { "term": { "category": "ebooks" }}
              ]
          }
        }
    }
  }
}
每个子查询都会计算一个document针对它的相关度分数，然后bool综合所有分数，合并为一个分数，但是filter是不会计算分数的

4、单独使用filter进行过滤（注意必须加constant_score）
GET /article/_search
{
  "query": {
    "constant_score": {
        "filter": {
          "bool": { 
              "must": [
                  { "range": { "date": { "gte": "2014-01-01" }}},
                  { "range": { "price": { "gte": 29.99 }}}
              ],
              "must_not": [
                  { "term": { "category": "ebooks" }}
              ]
          }
        }
    }
  }
}

=================================================query、filter及多搜索条件组合=================================================

=================================================term query与match query区别=================================================
1.首先设置索引名称为my_index，设置该索引的full_text字段类型为text，exact_value字段类型为keyword（表示该字段不分词）。
PUT my_index  
{  
  "mappings": {  
	  "properties": {  
  		"full_text": {  
  		  "type":  "text"    
  		},  
  		"exact_value": {  
  		  "type":  "keyword"
  		}  
	  }  
  }  
}
2.添加一条数据
PUT my_index/_doc/1  
{  
  "full_text":   "Quick Foxes!",    
  "exact_value": "Quick Foxes!"     
}
3.执行查询
（1）使用term查询exact_value,搜索内容为Quick Foxes!
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "exact_value": "Quick Foxes!"    
        }  
      }  
    }
    由于exact_value不分词，Quick Foxes!与exact_value的值QuickFoxes!匹配，因此可以匹配
（2）使用term查询full_text,搜索内容为Quick Foxes!
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "full_text": "Quick Foxes!"    
        }  
      }  
    }
	由于full_text字段默认使用标准分析器分词，在倒排索引中被分为quick和foxes，因此使用 Quick Foxes!匹配不到内容
（3）使用term查询full_text,搜索内容为foxes
    GET my_index/_doc/_search  
    {  
      "query": {  
        "term": {  
          "full_text": "foxes"    
        }  
      }
    }
    由于full_text字段默认使用标准分析器分词，在倒排索引中被分为quick和foxes，因此使用 foxes可以匹配到
（4）使用match查询full_text,查询内容为Quick Foxes!
	GET my_index/_doc/_search  
	{  
	  "query": {  
		"match": {  
		  "full_text": "Quick Foxes!"    
		}  
	  }  
	}
    使用match搜索，先分析搜索字符串Quick Foxes!，对它分词，然后搜索full_text中含有quick或者foxes或者两者都包含的文档，由于full_text字在倒排索引中被分为quick和foxes,因此可以匹配到.

总结：
match query搜索的时候，首先会解析查询字符串，进行分词，然后查询，而term query,输入的查询内容是什么，就会按照什么去查询，并不会解析查询内容，对它分词。
=================================================term query与match query区别=================================================