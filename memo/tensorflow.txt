/**************************************************tensorflow**************************************************/

https://www.cnblogs.com/denny402/p/6931338.html
http://blog.csdn.net/xzy_thu/article/details/76220654

tensorflow slim
http://blog.csdn.net/lijiancheng0614/article/details/77727445

http://python.jobbole.com/82604/

//Graph和Session
http://blog.csdn.net/xierhacker/article/details/53860379

//数据读取之TFRecords
https://www.cnblogs.com/upright/p/6136265.html

//windows环境下tensorflow安装
http://blog.csdn.net/sb19931201/article/details/53648615
http://blog.csdn.net/github_38705794/article/details/76767632

//交叉熵（Cross-Entropy）
http://blog.csdn.net/rtygbwwwerr/article/details/50778098
1.什么是信息量？
假设X是一个离散型随机变量，其取值集合为X，概率分布函数为p(x)=Pr(X=x),x∈X，我们定义事件X=x0的信息量为：
I(x0)=−log(p(x0))，可以理解为，一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：
事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219
事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014
可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。而对于小王而言，考试及格是大概率事件，在事件B发生前，大家普遍认为事件B的发生几乎是确定的，因此当某次考试小王及格这个事件发生时并不会引入太多的信息量，相应的I值也非常的低。
2.什么是熵？
那么什么又是熵呢？还是通过上边的例子来说明，假设小明的考试结果是一个0-1分布XA只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。
即：
HA(x)=−[p(xA)log(p(xA))+(1−p(xA))log(1−p(xA))]=0.4690
对应小王的熵：
HB(x)=−[p(xB)log(p(xB))+(1−p(xB))log(1−p(xB))]=0.0114
虽然小明考试结果的不确定性较低，毕竟十次有9次都不及格，但是也比不上小王（1000次考试只有一次才可能不及格，结果相当的确定）
我们再假设一个成绩相对普通的学生小东，他及格的概率是P(xC)=0.5,即及格与否的概率是一样的，对应的熵：
HC(x)=−[p(xC)log(p(xC))+(1−p(xC))log(1−p(xC))]=1
其熵为1，他的不确定性比前边两位同学要高很多，在成绩公布之前，很难准确猜测出他的考试结果。
可以看出，熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。
对于一个随机变量X而言，它的所有可能取值的信息量的期望（E[I(x)]
）就称为熵。
X的熵定义为：
H(X)=Eplog1p(x)=−∑x∈Xp(x)logp(x)
如果p(x)是连续型随机变量的pdf，则熵定义为：
H(X)=−∫x∈Xp(x)logp(x)dx
为了保证有效性，这里约定当p(x)→0时,有p(x)logp(x)→0

//正则化防止过拟合
https://www.zhihu.com/question/20700829/answer/119314862
https://www.cnblogs.com/alexanderkun/p/6922428.html
https://www.jianshu.com/p/70487abdf96b
https://www.sohu.com/a/272568277_814235

//tensorflow variable
http://zhangzhenyuan163.blog.163.com/blog/static/858193892017313102658544

//TensorFlow保存和恢复模型的方法总结
http://www.yueye.org/2017/summary-of-save-and-restore-models-in-tensorflow.html

//Tensorflow框架实现中的“三”种图
https://zhuanlan.zhihu.com/p/31308381

//获取模型中所有张量的名称以及值
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py

//视频中识别人类行为动作
Google Atomic Visual Actions
http://www.sohu.com/a/199636045_473283

卷积和池化的作用：
https://www.zhihu.com/question/36686900/answer/130890492
https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
http://www.hackcv.com/index.php/archives/104/?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io

命令行运行Python脚本时传入参数的三种方式
http://blog.csdn.net/weixin_35653315/article/details/72886718

tensorflow学习 tf.image.sample_distorted_bounding_box
http://blog.csdn.net/tz_zs/article/details/77920116

python多线程 Queue
https://www.zhihu.com/question/23474039
https://www.jianshu.com/p/d063804fb272

//window下启动tensorboard
http://blog.csdn.net/flying_sfeng/article/details/69943260

使用Tensorflow目标检测API
http://www.atyun.com/12866_%E6%97%A0%E4%BA%BA%E9%9B%B6%E5%94%AE%E8%83%8C%E5%90%8E%E7%9A%84%E7%A7%98%E5%AF%86%EF%BC%9A%E4%BD%BF%E7%94%A8tensorflow%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bapi%E5%AE%9E%E7%8E%B0%E6%9B%B4%E6%99%BA.html
https://towardsdatascience.com/building-a-toy-detector-with-tensorflow-object-detection-api-63c0fdf2ac95

http://blog.csdn.net/w5688414/article/details/78970874

https://www.zhihu.com/question/61173908

https://github.com/tensorflow/models/tree/master/research/object_detection

使用tensorflow实现自然场景文字检测,keras/pytorch实现crnn+ctc实现不定长中文OCR识别
https://github.com/chineseocr/chinese-ocr

https://github.com/meijieru/crnn.pytorch
https://github.com/xiaomaxiao/keras_ocr
https://github.com/eragonruan/text-detection-ctpn

https://www.cloud.tencent.com/developer/article/1038712
https://blog.csdn.net/u013293750/article/details/73188934
https://github.com/solivr/tf-crnn/blob/master/src/model.py
https://www.zhihu.com/question/41949741

ctc loss原理：
https://blog.csdn.net/dream_catcher_10/article/details/48526479
https://blog.csdn.net/Left_Think/article/details/76370453
https://blog.csdn.net/luodongri/article/details/77005948

PyTorch 安装起来很简单, 它自家网页上就有很方便的选择方式 (网页升级改版后可能和下图有点不同):
http://pytorch.org/

PASCAL VOC数据集分析
http://host.robots.ox.ac.uk/pascal/VOC/voc2012/

ImportError: cannot import name 'bbox' 
https://github.com/eragonruan/text-detection-ctpn/issues/59
ImportError: No module named 'yaml'
https://github.com/bokeh/bokeh/issues/4089

###################################################Tensorflow目标物检测###################################################
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md
http://blog.csdn.net/jiaoshengha/article/details/78684191
https://stackoverflow.com/questions/44833085/tensorflow-object-detection-killed-before-starting

http://shartoo.github.io/tf_obj_detect_api_train_owndata/

https://lijiancheng0614.github.io/2017/08/22/2017_08_22_TensorFlow-Object-Detection-API/
1. 安装或升级protoc

2. 下载代码并编译
下载tensorflow/models的代码：
git clone https://github.com/tensorflow/models.git
进入到models文件夹(/home/pythontemp/models/research)，编译Object Detection API的代码：
protoc object_detection/protos/*.proto --python_out=.

Add Libraries to PYTHONPATH
When running locally, the tensorflow/models/research/ and slim directories should be appended to PYTHONPATH. This can be done by running the following from tensorflow/models/research/:
# From tensorflow/models/research/
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
Note: This command needs to run from every new terminal you start. If you wish to avoid running this manually, you can add it as a new line to the end of your ~/.bashrc file.

PYTHONPATH是Python搜索路径，默认我们import的模块都会从PYTHONPATH里面寻找。
打印PYTHONPATH：
import sys
print(sys.path)

3. 将images转换为tf_records
    3.1 标注文件
	3.2 生成label_map.pbtxt文件
	3.3 创建tf_record文件
        先创建一个create_xx_tf_record.py文件，单独用来处理训练数据。可以直接从object_detection工程下的
		create_pacal_tf_record.py（如果是每个图片只有一个分类，可以使用create_pet_tf_record.py）复制而来。
	    具体路径：/home/pythontemp/models/research/object_detection/dataset_tools
		修改起始参数配置：
			data_dir: 数据目录，包含了图片和标注的目录
			output_dir:输出目录，图片转换为tf_record之后存储的位置
			label_map_path:上面提到的xx_label_map.pbtxt
		python create_pet_tf_record.py 
		--data_dir=/home/pythontemp/models/research/ 
		--output_dir=/home/pythontemp/models/research/ 
		--label_map_path=/home/pythontemp/models/research/toy_label_map.pbtxt
    3.4 创建 .config 配置文件
		目录tensorflow\models\object_detection\samples\configs下有各种配置文件，当前工程使用的是 faster_rcnn_inception_resnet_v2_robot.config，将其修改为适应当前数据的配置。
		主要修改了这些参数：
			num_classes： 分类数目。视数据分类数目而定，当前数据集只有3个分类，修改为3
			fine_tune_checkpoint：此处应该为空白，之前修改成github上下载的faster_rcnn的ckpt文件会导致无法训练的情况。
			from_detection_checkpoint： 设置为true
			num_steps: 训练步数。如果数据集较小，可以修改为较小。pets数据集包含7393张图片设置为20万次，当前数据集只有500张，设置为一万次应该差不多。可以在训练的时候查看loss增减情况来修改步数。

		
4. 模型训练
Tensorflow detection model zoo
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

需要指定的参数是：
    pipeline_config_path:上面提到的.config配置文件
    train_dir: 训练模型过程中保存的ckpt文件（tensorflow的权重文件）

/usr/local/python2/bin/python2.7 train.py \
--logtostderr \
--pipeline_config_path=/home/pythontemp/models/research/faster_rcnn_inception_resnet_v2_atrous_coco.config \
--train_dir=/software/train_dir
	
/usr/local/python2/bin/tensorboard --logdir=/software/train_dir

3 转换权重文件
训练完成之后的权重文件大概是会包含如下文件:
model.ckpt-${CHECKPOINT_NUMBER}.data-00000-of-00001,
model.ckpt-${CHECKPOINT_NUMBER}.index
model.ckpt-${CHECKPOINT_NUMBER}.meta
这些文件无法直接使用，eval.py 所使用的权重文件是.pb。需要做一步转换，object_detection工程中已经包含了该工具export_inference_graph.py，运行指令为：
/usr/local/python2/bin/python2.7 object_detection/export_inference_graph.py \
--input_type image_tensor \
--pipeline_config_path /home/pythontemp/models/research/faster_rcnn_inception_resnet_v2_atrous_coco.config \
--trained_checkpoint_prefix /software/train_dir/model.ckpt-69 \
--output_directory /software/output_inference/output_inference_graph.pb

pipeline_config_path :pipeline的配置路径，使用的是上面训练所使用的.config文件
trained_checkpoint_prefix :上一步保存tensorflow的权重文件ckpt的。精确到step数目，比如为xxx/model.ckpt-8876
output_directory ：最终输出的可以用来做inference得文件（到具体文件名称）

/usr/local/python2/bin/python2.7 object_detection/eval.py \
--logtostderr \
--checkpoint_dir /software/output_inference/output_inference_graph.pb \
--eval_dir /software/eval_img \
--pipeline_config_path /home/pythontemp/models/research/faster_rcnn_inception_resnet_v2_atrous_coco.config

###################################################Tensorflow目标物检测###################################################

###################################################TensorflowOnSpark环境安装###################################################
https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN
Install Python (w/ grid node access):
If you have access privileges to install/update the Python distribution on your grid nodes, just ensure that you have Python 2.7+ or 3.5+ installed on each node. Then, you can pip install tensorflow tensorflowonspark on each node (along with any other dependencies you might need for your application).

Install Python (w/o grid node access):
If you do not have access to install/update Python on your grid nodes (e.g. hosted environments), you can create a Python.zip "distribution" that can be shipped to the Spark executors at runtime, as follow. Note: we're using Python 2.7.12 here as an example, but TensorFlowOnSpark also supports versions 3.5 or later.

Install TensorFlowOnSpark
If you did not pip install tensorflowonspark into your Python distribution, you can clone this repo and build a zip package for Spark that will be shipped at execution time. This has the advantage that you can make updates to the code without re-installing it on all your grid nodes:
git clone git@github.com:yahoo/TensorFlowOnSpark.git
pushd TensorFlowOnSpark
zip -r tfspark.zip tensorflowonspark
popd


基于Hadoop分布式集群YARN模式下的TensorFlowOnSpark平台搭建
http://www.cnblogs.com/heimianshusheng/p/6768019.html
https://www.jianshu.com/p/98da344dbf22
1) 安装python
  a) 安装python2.7 wget https://www.python.org/ftp/python/2.7.12/Python-2.7.12.tgz
  b) 下载最新版的pip  wget https://bootstrap.pypa.io/get-pip.py
  c) 安装python2.7
		进入/usr/local/python2/bin目录 
		在这个目录中用./python2.7 get-pip.py命令安装pip
  d) ./pip install pydoop #安装pydoop，以能用python使用hadoop（目前pydoop只支持python2.7及以下）
  e) ./pip install tensorflow
2) 下载ecosystem
$ git clone https://github.com/tensorflow/ecosystem.git /software
C:\Users\gentl\ecosystem

3) 下载protoc
https://github.com/google/protobuf/releases/download/v3.5.1/protobuf-cpp-3.5.1.tar.gz
tar -zvxf protobuf-cpp-3.5.1.tar.gz -C /usr/loacl
cd /usr/local/protobuf-3.5.1 #进入protobuf目录下准备进行编译
./configure                  #默认路径是/usr/local/lib
make
make check
sudo make install

4) 下载maven
http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz
tar -zvxf apache-maven-3.5.2-bin.tar.gz -C /usr/loacl
export MAVEN_HOME=/usr/local/apache-maven-3.5.2

5) 生成tensorflow-hadoop-1.0-SNAPSHOT.jar
先下载tensorflow和tensorflowonspark
git clone https://github.com/yahoo/TensorFlowOnSpark.git
git clone https://github.com/yahoo/tensorflow.git
一定要注意你的路径，官网给出如下protoc --proto_path=$TF_SRC_ROOT --java_out=src/main/java/ $TF_SRC_ROOT/tensorflow/core/example/{example,feature}.proto
--proto_path是你下载的tensorflow的路径，--java_out是ecosystem下的/hadoop/src/main/java。以下是我的路径。
a.$ protoc --proto_path=/software/TensorFlowOnSpark/tensorflow --java_out=/software/ecosystem/hadoop/src/main/java/ /software/TensorFlowOnSpark/tensorflow/tensorflow/core/example/{example,feature}.proto
b.编译
请cd 到你的[ecosystem路径]/hadoop下执行
$ mvn clean package
这样就生成了 tensorflow-hadoop-1.0-SNAPSHOT.jar,位于[ecosystem路径]/hadoop/target下。
将 tensorflow-hadoop-1.0-SNAPSHOT.jar放到集群上
hadoop fs -put tensorflow-hadoop-1.0-SNAPSHOT.jar

6) 为Spark创建Python压缩包，并上传到集群上
cd /usr/local/python2
zip -r Python.zip *
hadoop fs -put ./Python.zip
hadoop -put命令在不指定路径的情况下 默认会放在“/usr/{用户}/”这个目录下

7) 安装TensorFlowOnSpark
git clone git@github.com:yahoo/TensorFlowOnSpark.git
pushd TensorFlowOnSpark
zip -r tfspark.zip tensorflowonspark
popd

8) 下载mnist，压缩后上传文件到hdfs
cd /home/data
zip -r mnist.zip *
hadoop fs -put mnist.zip

https://github.com/yahoo/TensorFlowOnSpark/issues/56
###################################################TensorflowOnSpark环境安装###################################################

###################################################git clone下载加速###################################################
centos7 安装及使用shadowsocks客户端
http://blog.csdn.net/yanzi1225627/article/details/51121507
http://blog.csdn.net/lell3538/article/details/50812161
https://www.jianshu.com/p/824912d9afda

一、安装shadowsocks
yum install python-pip    
pip install shadowsocks

vim /etc/shadowsocks.json
{
    "server":"67.216.218.190",  # Shadowsocks服务器地址
    "server_port":443,  # Shadowsocks服务器端口
    "local_address": "127.0.0.1", # 本地IP
    "local_port":1080,  # 本地端口
    "password":"MGQwNmEyOG", # Shadowsocks连接密码
    "timeout":300,  # 等待超时时间
    "method":"aes-256-cfb",  # 加密方式
    "fast_open": false,  # true或false。开启fast_open以降低延迟，但要求Linux内核在3.7+
    "workers": 1  #工作线程数
}

在使用centos7的软件包管理程序yum安装python-pip的时候会报一下错误：
No package python-pip available.
Error: Nothing to do
说没有python-pip软件包可以安装。

这是因为像centos这类衍生出来的发行版，他们的源有时候内容更新的比较滞后，或者说有时候一些扩展的源根本就没有。
所以在使用yum来search  python-pip的时候，会说没有找到该软件包。因此为了能够安装这些包，需要先安装扩展源EPEL。
EPEL(http://fedoraproject.org/wiki/EPEL) 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。
首先安装epel扩展源：
sudo yum -y install epel-release
然后安装python-pip
sudo yum -y install python-pip
安装完之后别忘了清除一下cache
sudo yum clean all

二、启动shawodsocks 
nohup sslocal -c /etc/shadowsocks.json /dev/null 2>&1 &
nohup sslocal -c /etc/shadowsocks.json 1>/home/pythonLogs/sslocal.log 2>&1 &
//是shadowsocks后台运行
然后加入开机自启动
echo " nohup sslocal -c /etc/shadowsocks.json /dev/null 2>&1 &" /etc/rc.local
查看后台进程
[root@kcw ~]# ps aux |grep sslocal |grep -v "grep"
 
利用shadowsocks的socks5代理，配置好后明显加速。用下面两条命令配置好后，保持shadowsocks客户端开启就行了。
git config --global http.proxy 'socks5://127.0.0.1:1080'  

三、安装Privoxy
上述安好了shadowsocks，但它是socks5代理，我们在shell里执行的命令，发起的网络请求现在还不支持socks5代理，只支持http／https代理。为此我们需要安装privoxy代理，它能把电脑上所有http请求转发给shadowsocks。 

安装privoxy
yum install privoxy

配置privoxy
修改配置文件/etc/privoxy/config
listen-address 127.0.0.1:8118 # 8118 是默认端口，不用改
forward-socks5t / 127.0.0.1:1080 . #转发到本地端口，注意最后有个点

设置http、https代理
# vi /etc/profile 在最后添加如下信息 
PROXY_HOST=127.0.0.1 
export all_proxy=http://$PROXY_HOST:8118 
export ftp_proxy=http://$PROXY_HOST:8118 
export http_proxy=http://$PROXY_HOST:8118 
export https_proxy=http://$PROXY_HOST:8118 
export no_proxy=localhost,172.16.0.0/16,192.168.0.0/16.,127.0.0.1,10.10.0.0/16 
# 重载环境变量 
source /etc/profile

依次打开shadowsocks和privoxy
nohup sslocal -c /etc/shadowsocks.json 1>/home/logs/sslocal.log 2>&1 &
privoxy --user privoxy /etc/privoxy/config

测试代理
curl -I www.google.com
###################################################git clone下载加速###################################################

###################################################Tensorflow切换使用CPU/GPU###################################################
TensorFlow程序可以通过tf.device函数来指定运行每一个操作的设备，这个设备可以是本地的CPU或者GPU，也可是是某一台远程的服务器。
TensorFlow会给每一个可用的设备一个名称，tf.device函数可以通过设备的名称来指定执行运算的设备。比如CPU在TensorFlow中的名称为/cpu:0。
在默认情况下，即使机器有很多个CPU，TensorFlow也不会区分它们，所有的CPU都使用/cpu:0作为名称。而一台机器上不同GPU的名称是不同的，
第n个GPU在TensorFlow中的名称为/gpu:n。比如第一个GPU的名称为/gpu:0，第二个GPU名称为/gpu:1，以此类推。
在配置好GPU环境的TensorFlow中，如果操作没有明确地指定运行设备，那么TensorFlow会优先选择GPU。
在Tensorflow程序中，单独使用"with tf.("/cpu:0"):"这个语句，而不做其他限制，
实际上默认tensorflow程序占用所有可以使用的内存资源和CPU核，比如如果你的linux服务器是8核CPU，那么该程序会迅速占用可以使用的任意CPU，使用接近100%

在TensorFlow中，不是所有的操作都可以被放在GPU上，如果强行将无法放在GPU上的操作指定到GPU上，那么程序将会报错。
为避免这个问题，TensorFlow在生成会话时可以指定allow_soft_placement参数。当allow_soft_placement参数设置为True时，如果运算无法由GPU执行，
那么TensorFlow会自动将它放到CPU上执行。以下代码给出了一个使用allow_soft_placement参数的样例。
import tensorflow as tf
a_cpu = tf.Variable(0, name="a_cpu")
with tf.device('/gpu:0'):
	a_gpu = tf.Variable(0, name="a_gpu")
	# 通过allow_soft_placement参数自动将无法放在GPU上的操作放回CPU上。
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))
sess.run(tf.global_variables_initializer())

通过设置device_count随意使用CPU或GPU的方法
device_count：设备的数量映射。key为设备的名称（比如”CPU”或者”GPU”），而value为该类型设备的数量的最大值。如果没有设置的话，系统会自动设置合适的数值。
import tensorflow as tf
num_cores = 4
if GPU:
    num_GPU = 1
    num_CPU = 1
if CPU:
    num_CPU = 1
    num_GPU = 0

config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\
        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\
        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})
session = tf.Session(config=config)
通过指定当我只想使用CPU时，有0个GPU设备来做到这一点的。通过这种方法，可以指定使用多少GPU和CPU！此外，还可以设置要使用的CPU核数。
感觉device_count = {'GPU' : num_GPU}) num_GPU=0:使用CPU ; num_GPU=1:使用GPU，这种说法不对；
其实device_count的两个key:CPU和GPU都必须设置，你单独设置GPU的话实际上是系统默认会给CPU这个key一个合适的值。
例如如果你设置成device_count = {'GPU': 1,'CPU': 0}这样是会报错的，因为程序运行不可能不需要CPU。

验证发现tensorflow强制使用CPU必须使用以下方式:
CUDA_VISIBLE_DEVICES='' python3 train.py 或者在python脚本前加CUDA_VISIBLE_DEVICES=''
否则无论是with tf.device("/cpu:0")的方式还是device_count = {'GPU' : 0}的方式显存都会被占掉。
###################################################Tensorflow切换使用CPU/GPU###################################################

##################################################################导数##################################################################
证明y=x∧2 的导数是y=2x
f(x)'=lim(t→0）f(x+t)-f(x)/t
=lim（t→0）[(x+t)^2-x^2]/t
=lim（t→0)(2xt+t^2)/t
=lim(t→0)(2x+t)
=2x.
##################################################################导数##################################################################

###################################################################对数指数###################################################################
若aⁿ=b(a>0，且a≠1)，称为a的n次幂等于b。在这里，a叫作底数，n叫作指数，
b叫作以a为底的n次幂。

若写成对数形式就是：
n=loga(b) (a>0,a≠1)
在这里，a仍然叫作底数，b叫作真数，而n叫作以a为底b的对数。
由此可见，指数和对数都是n，即它们是指同一个东西，只是在不同场合叫不同的名字。

对数的底数为什么不能为负?
底数是-2,真数是-8,结果不还是3吗?

首先我知道你肯定清楚对数的含义,即底数的某个次方的值是真数,而对数的结果就是次方数.
你举得例子也很清楚,也是正确的,但对数的底不能为负的原因并不是不存在负数的底数,而是人为的规定,人们为了简化对数的求解与研究,人为规定了对数的底必须是正数,即大于零的数,这样由于正数的任何次方的数都大于零,所以真数自然而然的肯定要大于零.
人们为什么强行把对数的底数规定为正数呢,先要想一想指数函数,指数函数的底我们也强行规定为正,因为底为负的指数函数,它的图像是不连续的,我们很难研究它的特性,而对数又是指数函数的反函数,所以可以很容易理解为什么人们也把对数的底规定为正数.
###################################################################对数指数###################################################################

####################################################################线性函数和非线性函数######################################################
如果从系统状态空间表达式来观察，线性系统和非线性系统最明显的区别方法就是线性系统遵从叠加原理，而非线性系统不然。
所谓叠加原理举个例子就是：
f(x)=2x,f(y)=2y,f(x+y)=2(x+y)=2x+2y=f(x)+f(y)
举个反例：
f(x)=2x^2,f(y)=2y^2,f(x)+f(y)=2(x^2+y^2),但f(x+y)=2(x+y)^2，两个显然不等。
换句话说，线性系统的表达式中只有状态变量的一次项，高次、三角函数以及常数项都没有，只要有任意一个非线性环节就是非线性系统。
####################################################################线性函数和非线性函数######################################################

####################################################################正态分布均值和标准差######################################################
https://zh.wikihow.com/%E8%AE%A1%E7%AE%97%E5%9D%87%E5%80%BC%E3%80%81%E6%A0%87%E5%87%86%E5%B7%AE%E5%92%8C%E6%A0%87%E5%87%86%E8%AF%AF%E5%B7%AE

获得一组你想要分析的数据。这些信息也称为样本。
  例如，一个由5个学生组成的班级接受了一次测试，测试结果为12, 55, 74, 79和90。
	
均值:μ=∑x/N
计算均值。把所有数值相加，再除以总体大小：
  均值 (μ) = ΣX/N，这里的 Σ 是求和（加法）符号， xi 是每个单一数值，而N则是总体大小。
  在上例中，均值 μ 就是 (12+55+74+79+90)/5 = 62。

标准差:√∑(X-μ)²/N
计算标准差。它表征总体的分布情况。 标准差 = σ = sqrt [(Σ((X-μ)^2))/(N)].
  对以上给出的例子，标准差是 sqrt[((12-62)^2 + (55-62)^2 + (74-62)^2 + (79-62)^2 + (90-62)^2)/(5)] = 27.4。
  （注意，如果要求样本的标准差，则应除以n-1，即样本大小减1。

方差：∑(X-μ)²/N
方差是标准差的平方

正态分布：
只依赖于数据集的两个特征：样本的均值和方差。
正态分布一般满足下述条件：
约 68.2％ 的点在 -1 到 1 个标准偏差范围内。
约 95.5％ 的点在 -2 到 2 个标准偏差范围内。
约 99.7％ 的点在 -3 至 3 个标准偏差范围内。


例如，两组数的集合 {1, 4, 9, 14} 和 {5, 6, 8, 9} 其平均值都是7，但第二个集合里的数字明显与7距离“更近”，通过公式算出第一个集合的标准差约为4.9，第二个约为1.5。
标准差计算流程如下：首先计算出该组数据里每一个数字与平均值的差，然后将所有的得出差进行平方，接下来求出均值，最后再开方。

####################################################################正态分布均值和标准差######################################################

#####################################使用tensorflow查询机器上是否存在可用的gpu设备#####################################
def is_gpu_available(cuda_only=True):
  """
  code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/test.py
  Returns whether TensorFlow can access a GPU.
  Args:
    cuda_only: limit the search to CUDA gpus.
  Returns:
    True iff a gpu device of the requested kind is available
  """
  from tensorflow.python.client import device_lib as _device_lib
  if cuda_only:
    return any((x.device_type == 'GPU') for x in _device_lib.list_local_devices())
  else:
    return any((x.device_type == 'GPU' or x.device_type == 'SYCL')for x in _device_lib.list_local_devices())
list = is_gpu_available()
print(list)
#####################################使用tensorflow查询机器上是否存在可用的gpu设备#####################################

#####################################使用tensorflow获取可用的gpu设备编号#####################################
def get_available_gpus():
    """
    code from http://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow
    """
    from tensorflow.python.client import device_lib as _device_lib
    local_device_protos = _device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']
list = get_available_gpus()
print(list)
#####################################使用tensorflow获取可用的gpu设备编号#####################################

#######################################################TensorFlow保存和恢复模型#######################################################
meta file: describes the saved graph structure, includes GraphDef, SaverDef, and so on; then apply tf.train.import_meta_graph('/tmp/model.ckpt.meta'), 
will restore Saver and Graph.

index file: it is a string-string immutable table(tensorflow::table::Table). Each key is a name of a tensor and its value is a serialized BundleEntryProto. 
Each BundleEntryProto describes the metadata of a tensor: which of the "data" files contains the content of a tensor, 
the offset into that file, checksum, some auxiliary data, etc.

data file: it is TensorBundle collection, save the values of all variables.
#######################################################TensorFlow保存和恢复模型#######################################################

#######################################################关于numpy mean函数的axis参数#######################################################
理解多维矩阵的"求和"、"平均"操作确实太恶心了，numpy提供的函数里还有一堆参数，搞得晕头转向的，这里做个笔记，提醒一下自己， 下面是例程
import numpy as np
X = np.array([[1, 2], [4, 5], [7, 8]])
print np.mean(X, axis=0, keepdims=True)
print np.mean(X, axis=1, keepdims=True)
结果是分别是
                 [[ 1.5]
 [[ 4.  5.]]      [ 4.5]    
                  [ 7.5]]
我个人比较raw的认识就是，axis=0，那么输出矩阵是1行，求每一列的平均（按照每一行去求平均）；axis=1，输出矩阵是1列，
求每一行的平均（按照每一列去求平均）。还可以这么理解，axis是几，那就表明哪一维度被压缩成1。

再举个更复杂点的例子，比如我们输入为batch = [128, 28, 28]，可以理解为batch=128，图片大小为28×28像素，我们相求这128个图片的均值，应该这么写
m = np.mean(batch, axis=0)
输出结果m的shape为(28,28)，就是这128个图片在每一个像素点平均值。
#######################################################关于numpy mean函数的axis参数#######################################################

#######################################################numpy.std() 计算矩阵标准差#######################################################
    >>> a = np.array([[1, 2], [3, 4]])  
    >>> np.std(a) # 计算全局标准差  
    1.1180339887498949  
    >>> np.std(a, axis=0) # axis=0计算每一列的标准差  
    array([ 1.,  1.])  
    >>> np.std(a, axis=1) # 计算每一行的标准差  
    array([ 0.5,  0.5])  
#######################################################numpy.std() 计算矩阵标准差#######################################################

#################################################name_scope、variable_scope#################################################
1)
tf.placeholder() 占位符。* trainable==False *
tf.Variable() 一般变量用这种方式定义。 * 可以选择 trainable 类型 *
tf.get_variable() 一般都是和 tf.variable_scope() 配合使用，从而实现变量共享的功能。 * 可以选择 trainable 类型 *

2)
tf.name_scope() 并不会对 tf.get_variable() 创建的变量有任何影响。
tf.name_scope() 主要是用来管理命名空间的，这样子让我们的整个模型更加有条理。而 tf.variable_scope() 的作用是为了实现变量共享，它和 tf.get_variable() 来完成变量共享的功能。
import tensorflow as tf
# 设置GPU按需增长
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)

with tf.name_scope('nsc1'): 
    v1 = tf.Variable([1], name='v1') 
	with tf.variable_scope('vsc1'): 
	    v2 = tf.Variable([1], name='v2') 
		v3 = tf.get_variable(name='v3', shape=[]) 
print 'v1.name: ', v1.name 
print 'v2.name: ', v2.name 
print 'v3.name: ', v3.name

with tf.name_scope('nsc1'):
    v4 = tf.Variable([1], name='v4')
print 'v4.name: ', v4.name

输出结果为：
v1.name:  nsc1/v1:0
v2.name:  nsc1/vsc1/v2:0
v3.name:  vsc1/v3:0
v4.name:  nsc1_1/v4:0

3)
import tensorflow as tf 
# 设置GPU按需增长 
config = tf.ConfigProto() 
config.gpu_options.allow_growth = True 
sess = tf.Session(config=config) 
# 下面是定义一个卷积层的通用方式 
def conv_relu(kernel_shape, bias_shape): 
    # Create variable named "weights". 
    weights = tf.get_variable("weights", kernel_shape, initializer=tf.random_normal_initializer()) 
    # Create variable named "biases". 
    biases = tf.get_variable("biases", bias_shape, initializer=tf.constant_initializer(0.0)) 
    return None

def my_image_filter(): 
    # 按照下面的方式定义卷积层，非常直观，而且富有层次感 
    with tf.variable_scope("conv1"): 
        # Variables created here will be named "conv1/weights", "conv1/biases". 
        relu1 = conv_relu([5, 5, 32, 32], [32]) 
    with tf.variable_scope("conv2"): 
        # Variables created here will be named "conv2/weights", "conv2/biases". 
        return conv_relu( [5, 5, 32, 32], [32])

with tf.variable_scope("image_filters") as scope: 
# 下面我们两次调用 my_image_filter 函数，但是由于引入了 变量共享机制 
# 可以看到我们只是创建了一遍网络结构。 
    result1 = my_image_filter() 
    #scope.reuse_variables相当于tf.variable_scope函数使用参数reuse=True生成上下文管理器。；
	scope.reuse_variables() 
	result2 = my_image_filter()
	
# 看看下面，完美地实现了变量共享！！！ 
vs = tf.trainable_variables() 
print 'There are %d train_able_variables in the Graph: ' % len(vs) 
for v in vs: 
    print v

输出结果为：
There are 4 train_able_variables in the Graph: 
Tensor("image_filters/conv1/weights/read:0", shape=(5, 5, 32, 32), dtype=float32) 
Tensor("image_filters/conv1/biases/read:0", shape=(32,), dtype=float32) 
Tensor("image_filters/conv2/weights/read:0", shape=(5, 5, 32, 32), dtype=float32) 
Tensor("image_filters/conv2/biases/read:0", shape=(32,), dtype=float32)
#################################################name_scope、variable_scope#################################################

#################################################tf.nn.dropout#################################################
一、tf.nn.dropout函数
def dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
x，你自己的训练、测试数据等
keep_prob，每一个元素被保存下的概率
……，其它参数不咋用，不介绍了
输出是：
A Tensor of the same shape of x
然后我们看看官方API是怎么说这个函数的：
With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0. The scaling is so that the expected sum is unchanged.

注意，输出的非0元素是原来的 "1/keep_prob" 倍！说了这么多，下面给一个程序例子：
import tensorflow as tf

dropout = tf.placeholder(tf.float32)
x = tf.Variable(tf.ones([10, 10]))
y = tf.nn.dropout(x, dropout)

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

print sess.run(y, feed_dict = {dropout: 0.4})
运行的结果如下：
[[ 0.   0.   2.5  2.5  0.   0.   2.5  2.5  2.5  2.5]
 [ 0.   2.5  2.5  2.5  2.5  2.5  0.   2.5  0.   2.5]
 [ 2.5  0.   0.   2.5  0.   0.   2.5  0.   2.5  0. ]
 [ 0.   2.5  2.5  2.5  2.5  0.   0.   2.5  0.   2.5]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   2.5  2.5]
 [ 2.5  2.5  2.5  0.   2.5  0.   0.   2.5  2.5  2.5]
 [ 0.   2.5  2.5  2.5  0.   2.5  2.5  0.   0.   0. ]
 [ 0.   2.5  0.   2.5  0.   0.   2.5  2.5  0.   0. ]
 [ 2.5  2.5  2.5  2.5  2.5  0.   0.   2.5  0.   0. ]
 [ 2.5  0.   0.   0.   0.   0.   2.5  2.5  0.   2.5]]

分析一下运行结果：

输入和输出的tensor的shape果然是一样的
不是0的元素都变成了原来的 "1/keep_prob" 倍

特点分析完毕，小总结一下，dropout这个概念看起来好高大上，然而在程序中实现竟然如此简单！说白了，tensorflow中的dropout就是：每一个元素被保存下的概率为keep_prob，保存下来的元素的值变为原来的1/keep_prob大小，其他没有被保存下来的元素值变为0！

tf.nn.rnn_cell.DropoutWrapper()详解
所谓dropout,就是指网络中每个单元在每次有数据流入时以一定的概率(keep_prob)正常工作，否则输出0值。这是是一种有效的正则化方法，可以有效防止过拟合。

函数原型：
tf.nn.rnn_cell.DrououtWrapper(cell, input_keep_prob=1.0, output_keep_prob=1.0)
参数说明：
cell：输入的循环神经网络的cell，可以设定为BasicLSTMCell等；
input_keep_prob：对每一层RNN的输入进行dropout
output_keep_prob：对每一层的RNN的输出进行dropout
state_keep_prob：对每一层的RNN的中间传递的隐层状态，通常是tensorflow的cell返回的turple中的第二个值，也就是(output,state)中德state，进行dropout(以前都是不建议在state中间进行dropout的，比如这篇文章recurrent neural network regularization，但是tensorflow提供给你选择，你可以选择rnn中间的隐层传递的时候不进行dropout，默认1.0就是不进行dropout)

import tensorflow as tf
import os
 
# 输入的batch_size为3，time_step为4，step_length为2
batch_size = 3
time_steps = 4
step_length = 2
 
# time_major=True时，设置x为输入数据的shape=(batch_size,time_step,input_dim)
x = tf.ones([time_steps, batch_size, step_length])
# 使用dynamic_rnn时，x不需要使用x = tf.unstack(x, time_steps, 1)展开成一系列元组
# 设置cell层的神经元数量
num_hidden = 5
cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=num_hidden, state_is_tuple=True)
cell_drop = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1.0, output_keep_prob=0.8)
# 给cell_drop层初始化，也就是全部赋值0
# 我们的输入input shape=[batch_size, num_steps]，我们刚刚定义好的cell会依次接收num_steps个输入然后产生最后的state（n-tuple，n表示堆叠的层数）
# 但是一个batch内有batch_size这样的seq，因此就需要[batch_size，s]来存储整个batch每个seq的状态。
state = cell_drop.zero_state(batch_size, tf.float32)
# 输出output和states，cell_drop层的输入数据为x
outputs, states = tf.nn.dynamic_rnn(cell_drop, x, initial_state=state, dtype=tf.float32, time_major=True)
 
with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	x_inputs, outputs_data, states_data = sess.run([x, outputs, states])
	print("输入数据x及其shape:")
	# x的shape=(3,4,2),此时x的shape=(batch_size,time_step,input_dim)
	print(x_inputs)
	print(x_inputs.shape)
	print("输出数据outputs及其shape:")
	# outputs的shape=(4,3,6)，即每个time_steps步的batch_size个样本在所有神经元上的输出值的集合
	# 注意这里只记录了第二个cell层的每个time_steps步batch_size个样本在所有神经元(6个)上的输出值的集合
	print(outputs_data)
	print(outputs_data.shape)
	print("输出数据states及其长度:")
	# states是c和h两个元组，对应最后一个time_steps的batch_size个样本在所有神经元中的输出ct和ht，ct和ht的每个元组的shape=(3,5)
	# 元组记录了batch_size个样本在最后一个时刻时在所有神经元上的输出ct和ht
	print(states_data)
	print(states_data.c.shape, states_data.h.shape)

运行结果如下：
输入数据x及其shape:
[[[1. 1.]
  [1. 1.]
  [1. 1.]]
 
 [[1. 1.]
  [1. 1.]
  [1. 1.]]
 
 [[1. 1.]
  [1. 1.]
  [1. 1.]]
 
 [[1. 1.]
  [1. 1.]
  [1. 1.]]]
(4, 3, 2)
输出数据outputs及其shape:
[[[ 0.00798632 -0.         -0.12428287  0.07095867  0.07479376]
  [ 0.00798632 -0.1175172  -0.12428287  0.07095867  0.07479376]
  [ 0.00798632 -0.1175172  -0.12428287  0.07095867  0.07479376]]
 
 [[ 0.01011041 -0.20690382 -0.20885429  0.          0.13673507]
  [ 0.01011041 -0.20690382 -0.20885429  0.12855332  0.13673507]
  [ 0.01011041 -0.20690382 -0.20885429  0.12855332  0.        ]]
 
 [[ 0.0084379  -0.27218974 -0.26277053  0.17633878  0.18750422]
  [ 0.0084379  -0.27218974 -0.26277053  0.17633878  0.18750422]
  [ 0.0084379  -0.         -0.26277053  0.17633878  0.18750422]]
 
 [[ 0.00436407 -0.         -0.          0.21631798  0.22881263]
  [ 0.00436407 -0.31942025 -0.29690948  0.21631798  0.22881263]
  [ 0.00436407 -0.31942025 -0.29690948  0.21631798  0.        ]]]
(4, 3, 5)
输出数据states及其长度:
LSTMStateTuple(c=array([[ 0.00653617, -0.72157025, -0.7466825 ,  0.27136636,  0.4279937 ],
       [ 0.00653617, -0.72157025, -0.7466825 ,  0.27136636,  0.4279937 ],
       [ 0.00653617, -0.72157025, -0.7466825 ,  0.27136636,  0.4279937 ]],
      dtype=float32), h=array([[ 0.00349125, -0.2555362 , -0.23752758,  0.17305438,  0.18305011],
       [ 0.00349125, -0.2555362 , -0.23752758,  0.17305438,  0.18305011],
       [ 0.00349125, -0.2555362 , -0.23752758,  0.17305438,  0.18305011]],
      dtype=float32))
(3, 5) (3, 5)
 
Process finished with exit code 0

我们可以发现此时outputs的输出每个time_steps的15个值中只有12个值不为0，这与output_keep_prob=0.8的比例一致。但是此时的states的ht输出和outputs的最后一个时刻的ht输出并不相等，原因是states的ht输出没有dropout，但是outputs的每个时刻的ht输出都进行了dropout（dropout时候非零值都会除以0.8，所以可以发现outputs的每个非零值都比states上同一个位置的值要大一些）。

也就是说，output_keep_prob控制的是outputs的每个时刻的ht输出的值的dropout比例，但是不会去dropout states的值（但要注意，因为states是最后一个时刻的ct和ht输出，它们由前一个时刻的ct-1和ht-1输出来计算，而ht-1是被dropout过的）。

#################################################tf.nn.dropout#################################################

##############################################################Tensor和Variable##############################################################
It's true that a Variable can be used any place a Tensor can, but the key differences between the two are that a Variable maintains its state across multiple calls to run() and a variable's value can be updated by backpropagation (it can also be saved, restored etc as per the documentation).

These differences mean that you should think of a variable as representing your model's trainable parameters (for example, the weights and biases of a neural network), while you can think of a Tensor as representing the data being fed into your model and the intermediate representations of that data as it passes through your model.
大意基本如下：
虽然可以使用Tensor的地方都可以使用Variable，但他们的主要区别在于，Variable的state在多个run()的调用中可以保持不变，并且其数值可以被更新，存储等等
所以你可以把Variable想成一个变量，代表着你模型中可以被训练的参数，比如权重值等，而对于Tensor，你可以将其想成数据（从输入，到中间通过权值变化，再到输出）的这个流程。

最后一句话没太想好怎么翻译，我自己的理解是，你的原始数据，与经过权值变化后得到的新数据，与经过激活函数后得到的新数据，都可以看做是tensor。不知道理解的对不对
##############################################################Tensor和Variable##############################################################

########################################【tensorflow】打印Tensorflow graph中的所有变量#########################################
http://blog.csdn.net/shwan_ma/article/details/78879620
【tensorflow】打印Tensorflow graph中的所有变量
一般来说，打印tensorflow变量的函数有两个：
tf.trainable_variables () 和 tf.all_variables()
不同的是：
tf.trainable_variables () 指的是需要训练的变量
tf.all_variables() 指的是所有变量

值得注意的是，在输出变量名时，要对整个graph进行初始化

一、打印需要训练的变量名称
sess.run(tf.global_varibales_initializer())
variable_name = [v.name for c in tf.trainable_variables()]
print(variable_names)

二、打印需要训练的变量名称和变量值
variable_names = [v.name for v in tf.trainable_variables()]
values = sess.run(variable_names)
for k,v in zip(variable_names, values):
    print("Variable: ", k)
    print("Shape: ", v.shape)
    print(v)
########################################【tensorflow】打印Tensorflow graph中的所有变量#########################################

##############################################【tensorflow】中的loss function实现##############################################
https://blog.csdn.net/mao_xiao_feng/article/details/53382790
Tensorflow中定义的交叉熵函数如下：
def softmax_cross_entropy_with_logits(_sentinel=None,  # pylint: disable=invalid-name
                                      labels=None, logits=None,
                                      dim=-1, name=None):
    """Computes softmax cross entropy between `logits` and `labels`."""
logits: 神经网络的最后一层输出，如果有batch的话，它的大小为[batch_size, num_classes], 单样本的话大小就是num_classes
labels: 样本的实际标签，大小与logits相同。且必须采用labels=y_，logits=y的形式将参数传入。
具体的执行流程大概分为两步，第一步首先是对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率，对于单样本而言，就是输出一个num_classes大小的向量[Y1,Y2,Y3,....], 其中Y1,Y2,Y3分别表示属于该类别的概率， softmax的公式为：
softmax(x)i=exp(xi)∑jexp(xj)
第二步是对softmax输出的向量[Y1,Y2,Y3,...]和样本的实际标签做一个交叉熵，公式如下：
Hy′(y)=−∑iy′ilog(yi)
其中y′i指代实际标签向量中的第i个值，yi就是softmax的输出向量[Y1,Y2,Y3,...]中的第i个元素的值。
显而易见。预测yi越准确，结果的值就越小（前面有负号），最后求一个平均，就得到我们想要的loss了

这里需要注意的是，这个函数返回值不是一个数，而是一个向量，如果要求交叉熵，我们要在做一步tf.resuce_sum操作，就是对向量里面的所有元素求和, 最后就能得到Hy′(y),如果要求loss，则需要做一步tf.reduce_mean操作，对向量求均值.
warning：
Tenosrflow中集成的交叉熵操作是施加在未经过Softmax处理的logits上, 这个操作的输入logits是未经缩放的, 该操作内部会对logits使用Softmax操作。
参数labels，ligits必须有相同的shape,如:[batch_size, num_classes]和相同的类型, 如:[(float16, float32, float64)中的一种]。

下面这段代码可以测试上面的理论：

# coding=utf-8
import tensorflow as tf  

with tf.device("/cpu:0"):
    # 神经网络的输出
    logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  
    # 使用softmax的输出
    y=tf.nn.softmax(logits)  
    # 正确的标签
    y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])
    # 正确的标签(labels的每一行需要进行one_hot表示)
    y_2=tf.constant([2,2,2])
    # 计算交叉熵  
    cross_entropy = -tf.resuce_sum(y_*tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
    # 使用tf.nn.softmax_cross_entropy_with_logits()函数直接计算神经网络的输出结果的交叉熵。
    # 但是不能忘记使用tf.reduce_sum()!!!!
    cross_entropy2 = tf.resuce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))
    # 使用tf.nn.softmax_cross_entropy_with_logits()函数直接计算神经网络的输出结果的交叉熵
    cross_entropy3 = tf.resuce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_2))
    with tf.Session() as sess:
        softmax=sess.run(y)
        c_e = sess.run(cross_entropy)
        c_e2 = sess.run(cross_entropy2)
        c_e3 = sess.run(cross_entropy3)
        print("step1:softmax result=", softmax)
        print("step2:cross_entropy result=", c_e)
        print("Function(softmax_cross_entropy_with_logits) result=", c_e2)
        print("Function(sparse_softmax_cross_entropy_with_logits) result=", c_e3)
	
输出结果：

step1:softmax result= [[ 0.09003057  0.24472848  0.66524094]
 [ 0.09003057  0.24472848  0.66524094]
 [ 0.09003057  0.24472848  0.66524094]]
step2:cross_entropy result= 1.22282
Function(softmax_cross_entropy_with_logits) result= 1.22282
Function(sparse_softmax_cross_entropy_with_logits) result= 1.22282

其中tf.clip_by_calue()函数可将一个tensor的元素数值限制在指定的范围内，这样可以防止一些错误运算，起到数值检查的作用。
从结果可以看出softmax_cross_entropy_with_logits()与我们个公式逻辑是相符合的，整个过程可以大概了解到softmax_cross_entropy_with_logits()的操作情况。

tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)
该函数与tf.nn.softmax_cross_entropy_with_logits()函数十分相似，唯一的区别在于labels的shape，该函数的labels要求是排他性的即只有一个正确的类别，如果labels的每一行不需要进行one_hot表示，可以使用tf.nn.sparse_softmax_cross_entropy_with_logits()。

Demo

下面的代码列举了sparse_softmax_cross_entropy_with_logits()和softmax_cross_entropy_with_logits()的输入输出
def cost_compute(logits, target_inputs, num_classes):
    # shape = [batch_size * num_steps, ]
    # labels'shape = [batch_size * num_steps, num_classes]
    # logits'shape = [shape = [batch_size * num_steps, num_classes]]
    # 这里可以使用tf.nn.sparse_softmax_cross_entropy_with_logits()和tf.nn.softmax_cross_entropy_with_logits()两种方式来计算rnn
    # 但要注意labels的shape。
    # eg.1
    # loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(target_inputs, [-1]),
    #                                                       logits=logits, name='loss')

    # eg.2
    targets = tf.one_hot(target_inputs, num_classes)  # [batch_size, seq_length, num_classes]
    # 不能使用logit.get_shape(), 因为在定义logit时shape=[None, num_steps], 这里使用会报错
    # y_reshaped = tf.reshape(targets, logits.get_shape())  # y_reshaped: [batch_size * seq_length, num_classes]
    loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(targets, [-1, num_classes]),
                                                   logits=logits, name='loss')

    cost = tf.reduce_mean(loss, name='cost')
    return cost

tf.one_hot()函数示例

import tensorflow as tf  

CLASS=8  
label=tf.constant([0,1,2,3,4,5,6,7])    
new_label = tf.one_hot(label,CLASS)  
with tf.Session() as sess:
    print('label1:','\r\n',sess.run(label))
    print('after one_hot:','\r\n',sess.run(new_label))

tf.argmax(input, dimension, name=None)函数示例	

import tensorflow as tf

b = tf.constant([[1,2,3],[3,2,1],[4,5,6],[6,5,4]])
with tf.Session() as sess:
    print(sess.run(tf.argmax(b, 0))) #dimension=0 按列找 返回最大数值的下标
    print(sess.run(tf.argmax(b, 1))) #dimension=1 按行找 返回最大数值的下标

numpy在全局和每行每列的最大值(最小值同理)
>>> a = np.arange(9).reshape((3,3))
>>> a
array([[0, 1, 2],
  [9, 4, 5],
  [6, 7, 8]])
>>> print(np.max(a))  #全局最大
8
>>> print(np.max(a,axis=0)) #每列最大
[6 7 8]
>>> print(np.max(a,axis=1)) #每行最大
[2 5 8]
np.argmin/np.argmax是numpy库中的成员函数
print(np.argmin(a, axis=0)) # 按每列求出最小值的索引
print(np.argmin(a, axis=1)) # 按每行求出最小值的索引

矩阵增加一行或者一列
np.row_stack((nx,row))
np.column_stack((nx,row))
例：
a,b,c = [3,1,1,2,4],[6,5,1,4,4],[0.2,0.7,0.9,0.5,0.8]
npa = np.array(a)
npb = np.array(b)
npc = np.array(c)
npAll = np.row_stack((npa,npb))
npAll = np.row_stack((npAll,npc))
print(npAll)
[[ 3.   1.   1.   2.   4. ]
 [ 6.   5.   1.   4.   4. ]
 [ 0.2  0.7  0.9  0.5  0.8]]

numpy中的argsort函数进行实现对二维数组中的某一行或者某一列排序
data = data[data[:,2].argsort()] #按照第3列对行排序
data = data[:,data[2].argsort()]

取numpy数组的某几行某几列方法
data[0,:] #取第一行
data[:,0] #取第一列
data[0:2] #取前面两列
data[:,[0,1] #取前面两行
##############################################【tensorflow】中的loss function实现##############################################

####################################################【tensorflow】Model保存####################################################
保存模型
保存模型是整个内容的第一步，当然也十分简单。无非是创建一个saver，并在一个Session里完成保存。比如：
saver = tf.train.Saver()
with tf.Session() as sess:
  saver.save(sess, model_name)

以上代码在0.11以下版本的TensorFlow里会保存与下面类似的3个文件：
checkpoint
model.ckpt-1000.meta
model.ckpt-1000.ckpt

在0.11及以上版本的TensorFlow里则会保存与下类似的4个文件：
checkpoint
model.ckpt-1000.index
model.ckpt-1000.data-00000-of-00001
model.ckpt-1000.meta

其中checkpoint列出保存的所有模型以及最近的模型；meta文件是模型定义的内容；ckpt（或data和index）文件是保存的模型数据；内里细节无需过多关注，如果想了解，stackOverflow上有一个解释的回答。：
https://stackoverflow.com/questions/41265035/tensorflow-why-there-are-3-files-after-saving-the-model/45033500#45033500
meta file: describes the saved graph structure, includes GraphDef, SaverDef, and so on; then apply tf.train.import_meta_graph('/tmp/model.ckpt.meta'), will restore Saver and Graph.
index file: it is a string-string immutable table(tensorflow::table::Table). Each key is a name of a tensor and its value is a serialized BundleEntryProto. Each BundleEntryProto describes the metadata of a tensor: which of the "data" files contains the content of a tensor, the offset into that file, checksum, some auxiliary data, etc.
data file: it is TensorBundle collection, save the values of all variables.

tensorflow模型持久化的几种方式
1) 利用tf.train.Saver保存模型(神经网络的网络结构，变量的取值，模型文件列表)
这种保存方式会生成的第一个文件为model.ckpt.meta,它保存了Tensorflow计算图的结构(这里可以简单理解为神经网络的网络结构)
第二个文件为model.ckpt(或model.ckpt.data和model.ckpt.index)这个文件保存了Tensorflow程序中每一个变量的取值。
最后一个文件为checkpoint文件，这个文件保存了一个目录下所有的模型文件列表。

保存模型代码
import tensorflow as tf
with tf.device("/cpu:0"):
    v1 = tf.Variable(tf.constant(1.0, shape=[1],dtype=tf.float32), name="v1")
    v2 = tf.Variable(tf.constant(2.0, shape=[1],dtype=tf.float32), name="v2")
    result = v1 + v2
    init_op = tf.initialize_all_variables()
    saver = tf.train.Saver()
    with tf.Session() as sess:
        sess.run(init_op)
        saver.save(sess,"Saved_model\\model.ckpt")

加载模型代码
在加载模型的程序中也是先定义了Tensorflow计算图上的所有运算，并声明了一个tf.train.Saver类。两段代码唯一不同的是，在加载模型的代码中没有运行变量的初始化过程，而是将变量的值通过已经保存的模型加载进来。如果不希望重复图上的运算，也可以直接加载已经持久化的图。
import tensorflow as tf
with tf.device("/cpu:0"):
    v1 = tf.Variable(tf.constant(1.0, shape=[1],dtype=tf.float32), name="v1")
    v2 = tf.Variable(tf.constant(2.0, shape=[1],dtype=tf.float32), name="v2")
    result = v1 + v2 
    saver = tf.train.Saver()
    with tf.Session() as sess:  
        saver.restore(sess,"Saved_model\\model.ckpt")  
        print(sess.run(result))
	
import tensorflow as tf
with tf.device("/cpu:0"):
    #直接加载持久化图
    saver = tf.train.import_meta_graph("Saved_model\\model.ckpt.meta")
    with tf.Session() as sess:  
        saver.restore(sess,"Saved_model\\model.ckpt")
        #通过张量的名称来获取张量
        print(sess.run(tf.get_default_graph().get_tensor_by_name("add:0")))
		
2) 利用export_meta_graph保存模型(神经网络的网络结构)
Tensorflow中并没有一个官方的定义说 collection 是什么。简单的理解，它就是为了方便用户对图中的操作和变量进行管理，而创建的一个概念。它可以说是一种“集合”，通过一个 key（string类型）来对一组 Python 对象进行命名的集合。这个key既可以是tensorflow在内部定义的一些key，也可以是用户自己定义的名字（string）。
Tensorflow 内部定义了许多标准 Key，全部定义在了 tf.GraphKeys 这个类中。其中有一些常用的，tf.GraphKeys.TRAINABLE_VARIABLES, tf.GraphKeys.GLOBAL_VARIABLES 等等。tf.trainable_variables() 与 tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) 是等价的；tf.global_variables() 与 tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) 是等价的。
对于用户定义的 key，我们举一个例子。例如：
pred = model_network(X)
loss=tf.reduce_mean(…, pred, …)
train_op=tf.train.AdamOptimizer(lr).minimize(loss)
这样一段 Tensorflow程序，用户希望特别关注 pred, loss train_op 这几个操作，那么就可以使用如下代码，将这几个变量加入到 collection 中去。(假设我们将其命名为 “training_collection”)
tf.add_to_collection("training_collection", pred)
tf.add_to_collection("training_collection", loss)
tf.add_to_collection("training_collection", train_op)
并且可以通过 Train_collect = tf.get_collection(“training_collection”) 得到一个python list，其中的内容就是 pred, loss, train_op的 Tensor。这通常是为了在一个新的 session 中打开这张图时，方便我们获取想要的操作。比如我们可以直接工通过get_collection() 得到 train_op，然后通过 sess.run(train_op)来开启一段训练，而无需重新构建 loss 和optimizer。
通过export_meta_graph保存图，并且通过 add_to_collection 将 train_op 加入到 collection中：
with tf.Session() as sess:
  pred = model_network(X)
  loss=tf.reduce_mean(…,pred, …)
  train_op=tf.train.AdamOptimizer(lr).minimize(loss)
  tf.add_to_collection("training_collection", train_op)
  Meta_graph_def = 
      tf.train.export_meta_graph(tf.get_default_graph(), 'my_graph.meta')
通过 import_meta_graph将图恢复（同时初始化为本 Session的 default 图），并且通过 get_collection 重新获得 train_op，以及通过 train_op 来开始一段训练（ sess.run() ）。
with tf.Session() as new_sess:
  tf.train.import_meta_graph('my_graph.meta')
  train_op = tf.get_collection("training_collection")[0]
  new_sess.run(train_op)

保存模型代码
import tensorflow as tf
with tf.Session() as sess:
    v1 = tf.Variable(tf.constant(1.0, shape=[1],dtype=tf.float32), name="v1")
    v2 = tf.Variable(tf.constant(2.0, shape=[1],dtype=tf.float32), name="v2")
    result = v1 + v2
    init_op = tf.initialize_all_variables()
    tf.add_to_collection("init_op", init_op)
    tf.add_to_collection("add_op", result)
    tf.train.export_meta_graph(filename='Saved_model\\my_graph.meta')
	
加载模型代码
import tensorflow as tf
with tf.device("/cpu:0"):
    with tf.Session() as new_sess:
        tf.train.import_meta_graph('Saved_model\\my_graph.meta')
        init_op = tf.get_collection("init_op")[0]
        add_op = tf.get_collection("add_op")[0]
        new_sess.run(init_op)
        print(new_sess.run(add_op))
		
3) 利用convert_variables_to_constants和write_graph保存模型成pb文件，其实就是graph_def(meta文件(graph_def)虽然不能保存 Variable，但可以保存Constant,所以可以利用convert_variables_to_constants将图中的变量及其取值转化为常量)

保存模型代码
import tensorflow as tf
w1 = tf.Variable(20.0, name="w1")
w2 = tf.Variable(30.0, name="w2")
b1= tf.Variable(2.0,name="bias")
w3 = tf.add(w1,w2)
#记住要定义name，后面需要用到
out = tf.multiply(w3,b1,name="out")
# 转换Variable为constant，并将网络写入到文件
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # 这里需要填入输出tensor的名字
    graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ["out"])
    tf.train.write_graph(graph, 'pretrained', 'graph.pb', as_text=False)
    #用下面这种方式保存也是可以的
    #with tf.gfile.GFile("pretrained\\graph.pb","wb") as f:
    #    f.write(graph.SerializeToString())
	
加载模型代码
with tf.Session() as sess:
    with open('pretrained\\graph.pb', 'rb') as graph:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(graph.read())
        output = tf.import_graph_def(graph_def, return_elements=['out:0'])
        print(sess.run(output))
		
但是很多时候，我们拿到的是别人的checkpoint文件，即meta、index、data等文件。这种情况下，需要将data文件里面变量转为常量保存到meta文件中。思路也很简单，先将checkpoint文件加载，再重新保存一次即可。

训练和保存模型代码如下
w1 = tf.Variable(20.0, name="w1")
w2 = tf.Variable(30.0, name="w2")
b1= tf.Variable(2.0,name="bias")
w3 = tf.add(w1,w2)
#记住要定义name，后面需要用到
out = tf.multiply(w3,b1,name="out")
# 转换Variable为constant，并将网络写入到文件
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    # 这里需要填入输出tensor的名字
    saver.save(sess, 'checkpoint_dir\\MyModel', global_step=1000)
此时，模型文件如下：
checkpoint
MyModel-1000.data-00000-of-00001
MyModel-1000.index
MyModel-1000.meta

如果我们只有以上4个模型文件，但是可以看到训练源码。那么，将这4个文件导出为一个pb文件方法如下：
with tf.Session() as sess:
    #初始化变量(貌似没必要)
    #sess.run(tf.global_variables_initializer())
    #获取最新的checkpoint，其实就是解析了checkpoint文件
    latest_ckpt = tf.train.latest_checkpoint("checkpoint_dir\\")
    #加载图
    restore_saver = tf.train.import_meta_graph('checkpoint_dir\\MyModel-1000.meta')
    #恢复图，即将weights等参数加入图对应位置中
    restore_saver.restore(sess, latest_ckpt)
    #将图中的变量转为常量
    output_graph_def = tf.graph_util.convert_variables_to_constants(
        sess, sess.graph_def , ["out"])
    #将新的图保存到"pretrained\\graph.pb"文件中
    tf.train.write_graph(output_graph_def, 'pretrained', "graph.pb", as_text=False)
	
加载模型代码
with tf.Session() as sess:
    with open('pretrained\\graph.pb', 'rb') as graph:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(graph.read())
        output = tf.import_graph_def(graph_def, return_elements=['out:0'])
        print(sess.run(output))
		
Operation没法从函数返回值中得到，可以用如下方法，从全局graph中查看当前所有Operation：
g = tf.get_default_graph()
opts = g.get_operations()
####################################################【tensorflow】Model保存####################################################

######################################################【tensorflow】Queue######################################################
在TensorFlow中用Queue的经典模式有两种，都是配合了QueueRunner和Coordinator一起使用的。
第一种，显式的创建QueueRunner，然后调用它的create_threads方法启动线程。例如下面这段代码：
import tensorflow as tf

# 1000个4维输入向量，每个数取值为1-10之间的随机数
data = 10 * np.random.randn(1000, 4) + 1
# 1000个随机的目标值，值为0或1
target = np.random.randint(0, 2, size=1000)

# 创建Queue，队列中每一项包含一个输入数据和相应的目标值
queue = tf.FIFOQueue(capacity=50, dtypes=[tf.float32, tf.int32], shapes=[[4], []])

# 批量入列数据（这是一个Operation）
enqueue_op = queue.enqueue_many([data, target])
# 出列数据（这是一个Tensor定义）
data_sample, label_sample = queue.dequeue()

# 创建包含4个线程的QueueRunner
qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)

with tf.Session() as sess:
    # 创建Coordinator
    coord = tf.train.Coordinator()
    # 启动QueueRunner管理的线程
    enqueue_threads = qr.create_threads(sess, coord=coord, start=True)
    # 主线程，消费100个数据
    for step in range(100):
        if coord.should_stop():
            break
        data_batch, label_batch = sess.run([data_sample, label_sample])
    # 主线程计算完成，停止所有采集数据的进程
    coord.request_stop()
    coord.join(enqueue_threads)
	
第二种，使用全局的start_queue_runners方法启动线程。
import tensorflow as tf

# 同时打开多个文件，显示创建Queue，同时隐含了QueueRunner的创建
filename_queue = tf.train.string_input_producer(["data1.csv","data2.csv"])
reader = tf.TextLineReader(skip_header_lines=1)
# Tensorflow的Reader对象可以直接接受一个Queue作为输入
key, value = reader.read(filename_queue)

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    # 启动计算图中所有的队列线程
    threads = tf.train.start_queue_runners(coord=coord)
    # 主线程，消费100个数据
    for _ in range(100):
        features, labels = sess.run([data_batch, label_batch])
    # 主线程计算完成，停止所有采集数据的进程
    coord.request_stop()
    coord.join(threads)
在这个例子中，tf.train.string_input_produecer会将一个隐含的QueueRunner添加到全局图中（类似的操作还有tf.train.shuffle_batch等）。
由于没有显式地返回QueueRunner来用create_threads启动线程，这里使用了tf.train.start_queue_runners方法直接启动tf.GraphKeys.QUEUE_RUNNERS集合中的所有队列线程。
这两种方式在效果上是等效的。
######################################################【tensorflow】Queue######################################################


############################################################【tensorflow】迁移学习############################################
所谓迁移学习，就是将一个问题上训练好的模型通过简单的调整使其使用于一个新的问题。比如利用ImageNet数据集上训练好的Inception-v3模型来解决一个新的图像分类问题。根据论文DeCAF:A Deep Convolutional Activation Feature for Generic Visual Recogniton中的结论，可以保留训练好的Inception-v3模型中所有卷积层的参数，只替换最后一层全
连接层。在最后一层全连接层之前的网络层称之为瓶颈层(bottleneck)
将新的图像通过训练好的卷积神经网络直到瓶颈层的过程可以看到是对图像进行特征提取的过程。在训练好的Inception-v3模型中，因为将瓶颈层的输出再通过一个单层的全连接层
神经网络可以很好的区分1000分类的图像，所以有理由认为瓶颈层输出的节点向量可以被作为任何图像的一个更加精简且表达能力更强的特征向量。于是，在新数据集上，可以直接
利用这个训练好的神经网络对图像进行特征特取，然后再将提取得到的特征向量作为输入来训练一个新的单层全连接神经网络处理新的分类问题。
一般来说，在数据量足够的情况下，迁移学习的效果不如完全重新训练。但是迁移学习所需要的训练时间和训练样本数要远远小于训练完整的模型。
############################################################【tensorflow】迁移学习############################################

############################################################【tensorflow】sess.run############################################
执行sess.run()时，tensorflow是否计算了整个图
我们在编写代码的时候，总是要先定义好整个图，然后才调用sess.run()。那么调用sess.run()的时候，程序是否执行了整个图
import tensorflow as tf
state = tf.Variable(0.0,dtype=tf.float32)
one = tf.constant(1.0,dtype=tf.float32)
new_val = tf.add(state, one)
update = tf.assign(state, new_val) #返回tensor， 值为new_val
update2 = tf.assign(state, 10000)  #没有fetch，便没有执行
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)
    for _ in range(3):
        print(sess.run(update))
和上个程序差不多，但我们这次仅仅是fetch “update”,输出是1.0 , 2.0, 3.0,可以看出，tensorflow并没有计算整个图，只是计算了与想要fetch 的值相关的部分

sess.run() 中的feed_dict
我们都知道feed_dict的作用是给使用placeholder创建出来的tensor赋值。其实，他的作用更加广泛：feed使用一个值临时替换一个op的输出结果. 你可以提供 feed数据作为run()调用的参数.feed只在调用它的方法内有效, 方法结束, feed就会消失.
import tensorflow as tf
y = tf.Variable(1)
b = tf.identity(y)
with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run(b,feed_dict={y:3})) #使用3替换掉tf.Variable(1)的输出结果，所以打印出来3 feed_dict{y.name:3} 和上面写法等价
    print(sess.run(b))  #由于feed只在调用他的方法范围内有效，所以这个打印的结果是 1
############################################################【tensorflow】sess.run############################################

############################################################【tensorflow】CNN#################################################
卷积层：
参数解释：
    tf.nn.conv2d(X, w, strides = [1, 1, 1, 1], padding = 'SAME')
    X 表示从上一层输入的数据。比如在输入层，input[0,:,:,:]表示第一张图片，input[1,:,:,:]表示第二张图片，以此类推。
    w 表示一个卷积核，比如一个3*3的高斯核(卷积层的权重)
    strides 中的四个参数，第一个是 the number of images，第二个是 the hight of images，第三个是 the width of images，第四个是 the number of channels
	        strides[0]和strides[3]的两个1是默认值(因为卷积层的步长只对矩阵的长和宽有效)
    padding 表示图像周围是否需要填充，如果选择'SAME'参数，表示图像的输入和输出维度是一样的，那么在卷积的时候，模型就会在图像的周围填充上0。
            如果选择的是'VALID'参数，那么图像维度将会被改变，具体改变多少，视具体数据而定。

池化层：
参数解释：
    tf.nn.max_pool(actvied, ksize=[1, 3, 3, 1], strides = [1, 2, 2, 1], padding = 'SAME')
    actvied 表示从上一层输入的数据(和tf.nn.conv2d函数中的第一个参数一致)
	ksize 表示过滤器的尺寸(虽然给出的是长度为4的一维数组，但这个数组的第一个和最后一个数必须为1。这意味着池化层的过滤器是不可以跨不同输入样例或者节点矩阵深度的)
	strides 表示步长(和tf.nn.conv2d函数中的步长的意义一样，而且第一维和最后一维也只能为1。这意味着在Tensorflow中，池化层不能减少节点矩阵的深度或者输入样例的个数)
    padding 表示图像周围是否需要填充，如果选择'SAME'参数，表示图像的输入和输出维度是一样的，那么在卷积的时候，模型就会在图像的周围填充上0。
            如果选择的是'VALID'参数，那么图像维度将会被改变，具体改变多少，视具体数据而定。
############################################################【tensorflow】CNN#################################################

############################################################【tensorflow】NLP#################################################
词嵌入
输入模型的input和target都是用词典id表示的。例如一个句子，“我/是/学生”，这三个词在词典中的序号分别是0,5,3，那么上面的句子就是[0,5,3]。显然这个是不能直接用的，我们要把词典id转化成向量,也就是embedding形式。实现的方法很简单：
第一步，构建一个矩阵，就叫embedding好了，尺寸为[vocab_size, embedding_size]，分别表示词典中单词数目，以及要转化成的向量的维度。一般来说，向量维度越高，能够表现的信息也就越丰富。
第二步，使用tf.nn.embedding_lookup(embedding,input_ids) 假设input_ids的长度为len，那么返回的张量尺寸就为[len,embedding_size]。举个例子：
# 示例代码
import tensorflow as tf
import numpy as np

sess = tf.InteractiveSession()

embedding = tf.Variable(np.identity(5,dtype=np.int32))
#embedding = tf.Variable([[9,5,1,4,0],[1,7,6,5,1],[8,2,3,5,2],[7,1,8,3,3],[8,2,5,7,4]],dtype=np.int32)
input_ids = tf.placeholder(dtype=tf.int32,shape=[None])
input_embedding = tf.nn.embedding_lookup(embedding,input_ids)

sess.run(tf.initialize_all_variables())
print(sess.run(embedding))
#[[1 0 0 0 0]
# [0 1 0 0 0]
# [0 0 1 0 0]
# [0 0 0 1 0]
# [0 0 0 0 1]]
print(sess.run(input_embedding,feed_dict={input_ids:[1,2,3,0,3,2,1]}))
#[[0 1 0 0 0]
# [0 0 1 0 0]
# [0 0 0 1 0]
# [1 0 0 0 0]
# [0 0 0 1 0]
# [0 0 1 0 0]
# [0 1 0 0 0]]
############################################################【tensorflow】NLP#################################################

############################################################BiLSTM#################################################
tf.nn.bidirectional_dynamic_rnn()函数详解
bidirectional_dynamic_rnn(
cell_fw, # 前向RNN
cell_bw, # 后向RNN
inputs, # 输入
sequence_length=None,# 这个参数是一个向量，表示batch中每一个输入序列的实际长度（可选，默认为输入序列的最大长度）
initial_state_fw=None,  # 前向的初始化状态（可选）
initial_state_bw=None,  # 后向的初始化状态（可选）
dtype=None, # 初始化和输出的数据类型（可选）
parallel_iterations=None,
swap_memory=False, 
time_major=False, 
scope=None)
值得注意的是，当inputs的张量形状为[batch_size,max_len,embeddings_num]时，time_major=False。当inputs的形状为[max_len,batch_size,embeddings_num]时，time_major=True。一般我们将输入的格式为[batch_size,max_len,embeddings_num]，因此time_major的默认值为False。
另外每个batch中的max_len（也就是文本长度）必须保持一致，不够的必须进行0-padding

outputs为(output_fw,output_bw)，是一个包含前向cell输出tensor和后向cell输出tensor组成的元组。当time_major=False时，output_fw和output_bw的形状为[batch_size,max_len,hiddens_num]。在此情况下，最终的outputs可以用tf.concat([output_fw,output_bw],-1)或tf.cocat([output_fw,output_bw],2)，这里面的[output_fw,output_bw]可以直接用outputs进行代替。

output_states为(output_state_fw,output_state_bw)，包含了前向和后向最后的隐藏状态的组成的元组。output_state_fw和output_state_bw的类型为LSTMStateTuple，由（c,h）组成，分别代表memorycell和hiddenstate.（c，h均为[batch_size,hidden_size]）

sequence_length参数如果指定的话，会影响output_states中的output_state_fw
比如：
import tensorflow as tf
import numpy as np

# 创建输入数据
# batch=2, 第一个序列中的time_step=4,
# 第二个序列中time_step=2, 不够直接补0.0
data = np.array([[[1, 2, 3],
                  [4, 5, 6],
                  [5, 6, 4],
                  [1, 2, 1]],

                  [[3, 2, 4],
                   [2, 2, 2],
                   [0, 0, 0],
                   [0, 0, 0]]
                 ])/10.0

cell = tf.nn.rnn_cell.LSTMCell(num_units=5, state_is_tuple=True)

#如果指定sequence_length的话第二个序列中的output_fw中超过2步的结果将会被置零。
data_lengths = [4, 2]
outputs, states = tf.nn.bidirectional_dynamic_rnn(
    cell_fw=cell, cell_bw=cell, dtype=tf.float64, sequence_length=data_lengths, inputs=data
)

#outputs, states = tf.nn.bidirectional_dynamic_rnn(
#    cell_fw=cell, cell_bw=cell, dtype=tf.float64, inputs=data
#)

output_fw, output_bw = outputs
states_fw, states_bw = states

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    states_shape = tf.shape(states)
    print(states_shape.eval())
    o_f = output_fw
    o_b = output_bw
    print('o_f\n', sess.run(o_f))
    print('o_b\n', sess.run(o_b))

tf.concat()：tensorflow中用来拼接张量的函数。用法:
tf.concat([tensor1, tensor2, tensor3,...], axis)

先给出tf源代码中的解释:
t1 = [[1, 2, 3], [4, 5, 6]]
t2 = [[7, 8, 9], [10, 11, 12]]
tf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
tf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]

# tensor t3 with shape [2, 3]
# tensor t4 with shape [2, 3]
tf.shape(tf.concat([t3, t4], 0))  # [4, 3]
tf.shape(tf.concat([t3, t4], 1))  # [2, 6]

这里解释了当axis=0和axis=1的情况，怎么理解这个axis呢？其实这和numpy中的np.concatenate()用法是一样的。
axis=0     代表在第0个维度拼接
axis=1     代表在第1个维度拼接 
对于一个二维矩阵，第0个维度代表最外层方括号所框下的子集，第1个维度代表内部方括号所框下的子集。维度越高，括号越小。

对于这种情况，我可以再解释清楚一点: 
对于[[],[]]和[[],[]]，低维拼接等于拿掉最外面括号，高维拼接是拿掉里面的括号(保证其他维度不变)。注意：tf.concat()拼接的张量只会改变一个维度，其他维度是保存不变的。比如两个shape为[2,3]的矩阵拼接，要么通过axis=0变成[4,3]，要么通过axis=1变成[2,6]。

这样就可以理解多维矩阵的拼接了，可以用axis的设置来从不同维度进行拼接。 
对于三维矩阵的拼接，自然axis取值范围是[0, 1, 2]。
对于axis等于负数的情况
负数在数组索引里面表示倒数(countdown)。比如，对于列表ls = [1,2,3]而言，ls[-1] = 3，表示读取倒数第一个索引对应值。
axis=-1表示倒数第一个维度，对于三维矩阵拼接来说，axis=-1等价于axis=2。同理，axis=-2代表倒数第二个维度，对于三维矩阵拼接来说，axis=-2等价于axis=1。

############################################################BiLSTM#################################################

##########################################################【tensorflow】cuda安装#############################################
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory
Please make sure that
PATH includes /usr/local/cuda-8.0/bin
LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64

libcudnn.so.6: cannot open shared object file: No such file or directory

cuDNN（CUDA Deep Neural Network），官网：https://developer.nvidia.com/cudnn
安装
相比标准的cuda，它在一些常用的神经网络操作上进行了性能的优化，比如卷积，pooling，归一化，以及激活层等等。
在理解上面这段的基础上，我们可以猜测配置cuDNN时是要对cuda进行一些修改，所以我们要先安装cuda。cuDNN下载需要注册，这个过程耐心点也很快。下面以ubuntu为例说明如何配置cuDNN进行神经网络的加速。 
1. 下载cuDNN压缩包
https://developer.nvidia.com/rdp/cudnn-archive上下载cudnn相应版本的压缩包（可能需要注册或登录）
如果这个压缩包不是.tgz格式的，把这个压缩包重命名为.tgz格式。
例如：下载CUDA8对应的cuDNN 则选择：
Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0
cuDNN v6.0 Library for Linux
2. 解压
tar -zxvf cudnn-7.0-linux-x64-v3.0-prod.tgz
tar -zxvf cudnn-8.0-linux-x64-v6.0.tgz 
3. 拷贝
解压后会看到一个cuda文件夹，里面包含了include以及lib64两个子目录。将这两个文件夹里的文件复制到cuda对应的安装目录。这里以cuda的安装目录为/usr/local/cuda/为例：
sudo cp cuDNN/cuda/include/cudnn.h /usr/local/cuda/include  
sudo cp cuDNN/cuda/lib64/* /usr/local/cuda/lib64  
4. 链接
#下面的操作在/usr/local/cuda/lib64/目录下进行  
sudo rm -rf libcudnn.so libcudnn.so.7.0#删除两个符号链接；  
sudo ln -s libcudnn.so.7.0.64 libcudnn.so.7.0  
sudo ln -s libcudnn.so.7.0 libcudnn.so

sudo rm -rf libcudnn.so libcudnn.so.6#删除两个符号链接；  
sudo ln -s libcudnn.so.6.0.21 libcudnn.so.6  
sudo ln -s libcudnn.so.6 libcudnn.so

5. 最后，重置cudnn.h文件的读写权限：
sudo chmod a+r /usr/local/cuda/include/cudnn.h
使用
在编译caffe（或者其他深度学习库）时，只需要在make的配置文件Makefile.config中将USE_CUDNN取消注释即可。

安装cuda时 提示toolkit installation failed using unsupported compiler解决方法
在安装cuda的时候，有时候会提示toolkit installation failed using unsupported compiler。这是因为GCC版本不合适所导致的。
解决的方法很简单，直接在安装命令之后加-override再安装，一般来说就没什么问题了。如：
sudo ./cuda_6.0.37_linux_64.run -override
##########################################################【tensorflow】cuda安装#############################################

##########################################################【tensorflow】cuda安装#############################################
import tensorflow as tf

graph1 = tf.Graph()
with graph1.as_default():
    #注意！恢复器必须要在新创建的图里面生成,否则会出错。
    saver1 = tf.train.import_meta_graph("Saved_model/model.ckpt.meta")
sess1=tf.Session(graph=graph1)#创建新的sess
saver1.restore(sess1, "Saved_model/model.ckpt")
print(sess1.run(graph1.get_tensor_by_name("add:0")))

graph2 = tf.Graph()
with graph2.as_default():
    #注意！恢复器必须要在新创建的图里面生成,否则会出错
    saver2 = tf.train.import_meta_graph("Saved_model/model.ckpt.meta")
sess2=tf.Session(graph=graph2)#创建新的sess
saver2.restore(sess2, "Saved_model/model.ckpt")
print(sess2.run(graph2.get_tensor_by_name("add_1:0")))
##########################################################【tensorflow】cuda安装#############################################

############################################word2vec词向量训练及gensim的使用#############################################
from gensim.models import Word2Vec
model = Word2Vec(sentences, sg=1, size=100, window=5, min_count=5, negative=3, sample=0.001, hs=1, workers=4)

1.sg=1是skip-gram算法，对低频词敏感；默认sg=0为CBOW算法。
2.size是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。
3.window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）。
4.min_count是对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。
5.negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3。
6.hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。
7.workers控制训练的并行，此参数只有在安装了Cpython后才有效，否则只能使用单核。

sentences：可以是一个list，对于大语料集，建议使用BrownCorpus,Text8Corpus或·ineSentence构建。
·  sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。
·  size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。
·  window：表示当前词与预测词在一个句子中的最大距离是多少
·  alpha: 是学习速率
·  seed：用于随机数发生器。与初始化词向量有关。
·  min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5
·  max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。
·  sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)
·  workers参数控制训练的并行数。
·  hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。
·  negative: 如果>0,则会采用negativesamp·ing，用于设置多少个noise words
·  cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。
·  hashfxn： hash函数来初始化权重。默认使用python的hash函数
·  iter： 迭代次数，默认为5
·  trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的函数。
·  sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。
·  batch_words：每一批的传递给线程的单词的数量，默认为10000
############################################word2vec词向量训练及gensim的使用#############################################

############################################viterbi_decode 和 crf_decode #############################################
viterbi_decode和crf_decode实现了相同功能，前者是numpy的实现，后者是 tensor 的实现，本文为了验证两者的解码结果是一致的。
测试环境：python3.6 + tensorflow1.8
import tensorflow as tf
from tensorflow.contrib.crf import viterbi_decode
from tensorflow.contrib.crf import crf_decode

score = [[
    [1, 2, 3],
    [2, 1, 3],
    [1, 3, 2],
    [3, 2, 1]
]]  # (batch_size, time_step, num_tabs)
transition = [
    [2, 1, 3],
    [1, 3, 2],
    [3, 2, 1]
]   # (num_tabs, num_tabs)
lengths = [len(score[0])]   # (batch_size, time_step)

# numpy
print("[numpy]")
np_op = viterbi_decode(
   score=np.array(score[0]),
   transition_params=np.array(transition))
print(np_op[0])
print(np_op[1])
print("=============")

# tensorflow
score_t         = tf.constant(score, dtype=tf.int64)
transition_t    = tf.constant(transition, dtype=tf.int64)
lengths_t       = tf.constant(lengths, dtype=tf.int64)
tf_op = crf_decode(
    potentials=score_t,
    transition_params=transition_t,
    sequence_length=lengths_t)
with tf.Session() as sess:
    paths_tf, scores_tf = sess.run(tf_op)
    print("[tensorflow]")
    print(paths_tf)
    print(scores_tf)
可见结果是一致的，说明 crf_decode 就是 viterbi_decode 的tensorflow版本。
decode返回结果:
1) 根据logits和转移矩阵算出来的最高分的标签序列
2) 最高分数
例如上面的例子的输出为：
[[2 0 2 0]]：这个就是最高分的标签序列
[19]       ：这个就是最高分
计算说明见crf_decode.png
############################################viterbi_decode 和 crf_decode #############################################

python train_image_classifier.py \
    --train_dir=train_logs \
    --dataset_dir=/home/pythontemp/data/train \
    --num_samples=3320 \
    --num_classes=5 \
    --labels_to_names_path=/home/pythontemp/data/labels.txt \
    --model_name=inception_resnet_v2 \
    --checkpoint_path=/home/pythontemp/inception_resnet_v2_2016_08_30.ckpt \
    --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \
    --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \
    --clone_on_cpu=True \
    --max_number_of_steps=1000 \
    --batch_size=32
	
python train_image_classifier.py \
    --train_dir=train_logs \
    --dataset_dir=/home/pythontemp/data/train \
    --num_samples=3320 \
    --num_classes=5 \
    --labels_to_names_path=/home/pythontemp/data/labels.txt \
    --model_name=inception_resnet_v2 \
    --checkpoint_path=/home/pythontemp/models/research/slim/train_logs \
    --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \
    --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \
    --clone_on_cpu=True \
    --max_number_of_steps=1000 \
    --batch_size=32

CUDA_VISIBLE_DEVICES="0" python train_image_classifier.py --train_dir=D:\jupyterworkspace\slim\train_logs --dataset_dir=D:\data\flower_train --num_samples=3320 --num_classes=5 --labels_to_names_path=D:\data\flower_labels\labels.txt --model_name=inception_resnet_v2 --checkpoint_path=D:\data\checkpoint\inception_resnet_v2_2016_08_30.ckpt --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits --max_number_of_steps=10 --batch_size=32