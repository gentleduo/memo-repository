gensim模块
Word2vec需要使用第三方gensim模块， gensim模块依赖numpy和scipy两个包，因此需要依次下载对应版本的numpy、scipy、gensim。

Wiki数据获取
到wiki官网下载中文语料，下载完成后会得到命名为zhwiki-latest-pages-articles.xml.bz2的文件，大小约为1.3G，里面是一个XML文件。
下载地址如下：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2

将XML的Wiki数据转换为text格式

执行xml2txt.py脚本(参数1：Wiki数据；参数2：转化后的文本文件)
python xml2txt.py zhwiki-latest-pages-articles.xml.bz2 wiki.zh.txt

xml2txt.py脚本内容如下：
from gensim.corpora import WikiCorpus
#inp, outp = sys.argv[1:3]
inp = 'zhwiki-latest-pages-articles.xml.bz2'
outp = 'wiki.zh.txt'
space = ' '
output = open(outp, 'w')
wiki =WikiCorpus(inp, lemmatize=False, dictionary=[])#gensim里的维基百科处理类WikiCorpus
for text in wiki.get_texts():#通过get_texts将维基里的每篇文章转换位1行text文本，并且去掉了标点符号等内容
	output.write(space.join(text) + "\n")
output.close()

利用OpenCC工具将中文繁体替换成简体(yum install opencc)
opencc -i wiki.zh.txt -o wiki.zh.simp.txt -c t2s.json

#####################################################word2vec训练参数#####################################################
sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。
hs: 即我们的word2vec两个解法的选择了（解法是指：通过神经网络语言模型预测目标词的概率；例：softmax），如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。
negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。
（关于negative的值：论文中说5-20个词适合小数据集， 2-5个词适合大数据集。）
sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)

采样率
word2vec 的C代码使用了一个等式来计算是否留着这个word。
我们使用 wi 来表示单词，z(wi) 表示它出现在词库中的概率。比如花生在1bilion（10亿）的词库中出现了1,000词，那么z(花生)=1E−6。
然后有个叫‘sample‘的参数控制了降采样的程度，一般设置为0.001。这个值越小代表更容易扔掉一些词。 
下面这个等式 P(wi) 代表保留这个词的概率 
P(wi)=(√z(wi)/0.001−−−−−−−−−+1)×0.001/z(wi) 
可以看到如果单词出现频率很低， 
z(wi)<=0.0026 的情况下，P(wi)=1，我们不会把这些词扔掉；
当z(wi)<=0.00746 的情况下，P(wi)=0.5;
如果出现频率很高，z(wi)==1 的情况下，P(wi)=0.033, 有很低的概率我们keep这个word。当然，如果每个训练样本对都有这个词，这种情况不会出现。

Negative Sampling
训练神经网络 意味着输入一个训练样本调整weight，让它预测这个训练样本更准。换句话说，每个训练样本将会影响网络中所有的weight。
像我们之前讨论的一样，我们词典的大小意味着我们有好多weight，所有都要轻微的调整。
Negative sampling 解决了这个问题，每次我们就修改了其中一小部分weight，而不是全部。
当训练（fox，quick）这个词对的时候，quick这个词的概率是1，其他的都是0。通过negative sample，我们只是随机的选了一部分negative词（假设是5个）来update weight。（这些negative 词就是我们希望是0的。）
论文中说5-20个词适合小数据集， 2-5个词适合大数据集。
回忆我们原来模型每次运行都需要 300×10,000 (译者按，其实没有减少数量，但是运行过程中，减少了需要载入的数量。) 现在只要 300×(1+5) 减少了好多。 
而在输出层始终只要update 输入单词的weight 就好了。（不论有没有采用negative sampling） 
译者按：因为输入层就像lookup table，当然其他的都不影响的。
#####################################################word2vec训练参数#####################################################

#####################################################One-hot#####################################################
（1）词频做向量值

Bag-of-words model (BoW model)最早出现在自然语言处理（Natural Language Processing）和信息检索（Information Retrieval）领域.。该模型忽略掉文本的语法和语序等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的。BoW使用一组无序的单词(words)来表达一段文字或一个文档.。近年来，BoW模型被广泛应用于计算机视觉中。

基于文本的BoW模型的一个简单例子如下：
首先给出两个简单的文本文档如下：
        John likes to watch movies. Mary likes too.
        John also likes to watch football games.

基于上述两个文档中出现的单词，构建如下一个词典 (dictionary)：
       {"John": 1, "likes": 2,"to": 3, "watch": 4, "movies": 5,"also": 6, "football": 7, "games": 8,"Mary": 9, "too": 10}

上面的词典中包含10个单词, 每个单词有唯一的索引, 那么每个文本我们可以使用一个10维的向量来表示。如下：
       [1, 2, 1, 1, 1, 0, 0, 0, 1, 1]
       [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]
该向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词在文本中出现的频率。
#####################################################One-hot#####################################################

#####################################################TF-IDF算法#####################################################
1、TF-IDF算法介绍
TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。
TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
（1）TF是词频(Term Frequency)
词频(TF) = 某个词在文章中出现的次数
考虑到文章有长短之分，为了便于不同文章的比较，进行词频标准化。
词频(TF) = 某个词在文章中出现的次数 / 文章的总词数
或者：
词频(TF) = 某个词在文章中出现的次数 / 该文章出现次数最多的词出现的次数
（2） IDF是逆向文件频率(Inverse Document Frequency)
逆向文件频率 (IDF) ：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。
如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。
（3）TF-IDF实际上是：TF * IDF
某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。

NLTK实现TF-IDF算法：
from nltk.text import TextCollection
from nltk.tokenize import word_tokenize
 
#首先，构建语料库corpus
sents=['this is sentence one','this is sentence two','this is sentence three']
sents=[word_tokenize(sent) for sent in sents]  #对每个句子进行分词
print(sents)                                   #输出分词后的结果
corpus=TextCollection(sents)                   #构建语料库
print(corpus)                                  #输出语料库
 
#计算语料库中"one"的tf值
tf=corpus.tf('one',corpus)                     # 1/12
print(tf)
 
#计算语料库中"one"的idf值
idf=corpus.idf('one')                          #log(3/1)
print(idf)
 
#计算语料库中"one"的tf-idf值
tf_idf=corpus.tf_idf('one',corpus)
print(tf_idf)

使用sklearn提取文本的tfidf特征：
CountVectorizer是通过fit_transform函数将文本中的词语转换为词频矩阵
get_feature_names()可看到所有文本的关键字
vocabulary_可看到所有文本的关键字和其位置
toarray()可看到词频矩阵的结果：
vectorizer = CountVectorizer()
count = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())  
print(vectorizer.vocabulary_)
print(count.toarray())

TfidfTransformer是统计CountVectorizer中每个词语的tf-idf权值：
transformer = TfidfTransformer()
tfidf_matrix = transformer.fit_transform(count)
print(tfidf_matrix.toarray())

TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值
TfidfVectorizer的关键参数：
max_df：这个给定特征可以应用在 tf-idf 矩阵中，用以描述单词在文档中的最高出现率。假设一个词（term）在 80% 的文档中都出现过了，那它也许（在剧情简介的语境里）只携带非常少信息。
min_df：可以是一个整数（例如5）。意味着单词必须在 5 个以上的文档中出现才会被纳入考虑。设置为 0.2；即单词至少在 20% 的文档中出现 。

#####################################################TF-IDF算法#####################################################