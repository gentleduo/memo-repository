==========================================================概率论==========================================================
1.联合概率
联合概率指的是包含多个条件且所有条件同时成立的概率，记作P(X=a,Y=b)或P(a,b)，有的书上也习惯记作P(ab)，但是这种记法个人不太习惯，所以下文采用以逗号分隔的记法。
2.边缘概率
边缘概率是与联合概率对应的，P(X=a)或P(Y=b)，这类仅与单个随机变量有关的概率称为边缘概率
3.条件概率
条件概率表示在条件Y=b成立的情况下，X=a的概率，记作P(X=a|Y=b)或P(a|b)

例：
红J 红Q 红K 红J
红Q 红K 红1 红2
黑K 黑1 黑2 红3
黑3 黑4 黑5 黑6
将上述表格中的牌分成两个类别（数字牌和人头牌(X)、红色和黑色(Y)）来统计它们所占的比例，其中的每一种分类都包含了整个样本整体，得到的统计表如下：
        Y=数字牌  Y=人头牌
X=红色  3/16      6/16
X=黑色  6/16      1/16
这其中我们可以看到：若按颜色划分类别P(X=红色)(9/16)+P(X=黑色)(7/16)=1，若按牌的类别划分P(Y=数字牌)(9/16)+P(Y=人头牌)(7/16)=1，而这两个分类其实是你中有我，我中有你，因为本身都在16张牌中，怎么分两个类别都会有交叉。
表格中的概率值很清晰地表达了X和Y的联合概率，所谓联合概率，就是既满足X条件，又满足Y条件的概率，两个条件的满足是站在同一起跑线的，同时它们所对应的包围圈（对于这样的一个包围圈，其中各类概率的总和为1，而这个1所容纳的范围只是相对而言的）也是总体的16张扑克牌。
而对于条件概率，和联合概率就不一样了，首先它所对应的包围圈更为狭小，比如说P(Y=数字牌|X=红色)这个条件概率所对应的包围圈只限定在X=红色这个圈中，而红色的牌只有9张，9张牌中我再按照牌的类别进行划分，便得到P(Y=数字牌|X=红色)=3/9，P(Y=人头牌|X=红色)=6/9。当你看到条件概率的公式：P(Y=b|X=a)=P(X=a,Y=b)/P(X=a)，公式中的包围圈是16张牌这个整体。这只是分了两类，分成多类也是类似的道理。
边缘概率是相对于联合概率而言的，只是抹去了其中分类的数量，比如上面的例子，单一就从颜色的类别上说，P(X=红色)=9/16，P(X=黑色)=7/16，这就是边缘分布。多个类别也可以以此类推。

两个事件的独立性 (概率论)
两个事件A和B是独立的当且仅当Pr(A ∩ B) = Pr(A)Pr(B)。这里，A ∩ B是A和B的交集，即为A和B两个事件都会发生的事件。
更一般地，任意个事件都是互相独立的当且仅当对其任一有限子集A1, ..., An，会有 Pr(A1 ∩ ... ∩ An) = Pr(A1)...Pr(An)
若两个事件A和B是独立的，则其B给之A的条件概率和A的“无条件概率”一样，即 Pr(A | B) = Pr(A)
Pr(A | B) = Pr(A)的推导过程如下：
条件概率Pr(A | B)的定义为：Pr(A | B) = Pr(A ∩ B) / Pr(B)
由于A和B是独立的，则有Pr(A ∩ B) = Pr(A)Pr(B)，再将Pr(A ∩ B) = Pr(A)Pr(B)代入Pr(A | B) = Pr(A ∩ B) / Pr(B)中得到：Pr(A | B) = Pr(A)

互斥事件（不相容事件）
事件A和B的交集为空，A与B就是互斥事件，也叫互不相容事件。也可叙述为：不可能同时发生的事件。如A∩B为不可能事件（A∩B=Φ），那么称事件A与事件B互斥，其含义是：事件A与事件B在任何一次试验中不会同时发生。 若A与B互斥，则P(A+B)=P(A)+P(B)且P(A)+P(B)≤1；若a是A的对立事件则P(A)=1-P(a)
1、互斥事件定义中事件A与事件B不可能同时发生是指若事件A发生，事件B就不发生或者事件B发生，事件A就不发生。如，粉笔盒里有3支红粉笔，2支绿粉笔，1支黄粉笔，现从中任取1支，记事件A为取得红粉笔，记事件B为取得绿粉笔，则A与B不能同时发生，即A与B是互斥事件。
2、对立事件的定义中的事件A与B不能同时发生，且事件A与B中“必有一个发生”是指事件A不发生，事件B就一定发生或者事件A发生，事件B就不发生。如，投掷一枚硬币，事件A为正面向上，事件B为反面向上，则事件A与事件B必有一个发生且只有一个发生。所以，事件A与B是对立事件，但1中的事件A与B就不是对立事件，因为事件A与B可能都不发生。事件A的对立事件通常记作A。
3、如果事件A与B互斥，那么事件A+B发生(即A、B中恰有一个发生)的概率，等于事件A、B分别发生的概率的和，即P(A+B)=P(A)+P(B)，此公式可以由特殊情形中的既是互斥事件又是等可能性事件推导得到。一般地，如果事件A1、A2、…、An彼此互斥，那么事件A1+A2+…+An发生(即A1、A2、…、An中有一个发生)的概率，等于这n个事件分别发生的概率的和，即P(A1+A2+…+An)=P(A1)+P(A2)+…+P(An)。
4、对立事件是一种特殊的互斥事件。特殊有两点：其一，事件个数特殊(只能是两个事件)；其二，发生情况特殊(有且只有一个发生)。若A与B是对立事件，则A与B互斥且A+B为必然事件，故A+B发生的概率为1，即P(A+B)=P(A)+P(B)=1。

不相容与独立性
事件A和B的交集为空，即意味着两个事件不相容，你会不会直观的感觉到，事件A和事件B二者看上去没啥关系，二者就是相互独立的？这个说法看似很有道理，然而事实上却恰巧相反。若事件A和事件B互不相容，并且能够保证两个事件发生的概率：P(A)>0且P(B)>0成立，则他们永远不会相互独立。这是为什么呢？我们直接抠定义就好了。这是因为：首先有A∩B=ϕ，那么显然有联合概率P(A∩B)=0，而由于P(A)和P(B)均大于0，则有P(A)P(B)≠0。因此，从P(A∩B)≠P(A)P(B)的结果来看，并不满足事件A和事件B相互独立的基本条件。其实，这个结果从常理上来说我们也很好理解，由于事件A和事件B不相容，则如果事件B发生，则意味着事件A一定不会发生，那么这就实际上说明了：事件B的发生就给事件A的发生引入了额外的信息。那么，二者显然就不是互相独立的了。

条件独立
在概率论和统计学中，两事件R和B在给定的另一事件Y发生时条件独立，类似于统计独立性，就是指当事件Y发生时，R发生与否和B发生与否就条件概率分布而言是独立的。换句话讲，R和B在给定Y发生时条件独立，当且仅当已知Y发生时，知道R发生与否无助于知道B发生与否，同样知道B发生与否也无助于知道R发生与否。
R和B在给定Y发生时条件独立，用概率论的标准记号表示为：Pr(R ∩ B | Y) = Pr(R | Y)Pr(B | Y)、也可等价的表示为：Pr(R | B ∩ Y) = Pr(R | Y)
两个随机变量X和Y在给定第三个随机变量Z的情况下条件独立当且仅当它们在给定Z时的条件概率分布互相独立，也就是说，给定Z的任一值，X的概率分布和Y的值无关，Y的概率分布也和X的值无关。

==========================================================概率论==========================================================

==========================================================信息论==========================================================

如果X是一个离散型随机变量，取值空间为R，其概率分布为p(x)=P(X=x),x∈R，那么X的熵H(X)定义为式

H(X)=-∑p(x)㏒2p(x)

由于在公示中对数以2为底，该公式定义的熵的单位为二进制(比特)，通常将㏒2p(x)简写成㏒p(x)

熵又称为自信息量(self-information)，可以视为描述一个随机变量的不确定性的数量，它表示信息源X每发一个符号(不论发什么符号)所提供的平均信息量，一个随机变量的熵越大，它的不确定性越大，那么正确估计其值的可能性就越小，越不确定的随机变量越需要大的信息量用来确定其值。

假设a,b,c,d,e,f6个字符在某一简单的语言中随机出现，每个字符出现的概率分别为：1/8,1/4,1/8,1/4,1/8,1/8，那么，每个字符的熵为：

H(X)=-∑p(x)㏒2p(x)

    =-[4×1/8㏒1/8﹢2×1/4㏒1/4] = 2.5(比特)

这个结果表明，我们可以设计一种编码，传输一个字符平均只需要2.5个比特：

字符: a   b   c   d   e   f

编码: 100 00  101 01  110 111

现实含义：比如我们要用二进制来表示出上述6个字符构成的一段比特流，通常的做法是所有的字符一律按3个比特来编码，但是可以根据出现概率高的字符用比较短的编码，出现概率低的字符用长一点的编码为原则来压缩整个比特流。熵的意思为：要表达这6个字符平均每个字符最低需要用2.5个比特位，即压缩的极限。

假设一个系统的状态的概率都很平均，那他的熵就越大，就说明这个系统很混乱不确定很大

假设一个系统的状态虽然多，但是某一个状态出现的概率在99%以上，那他的熵也会很小，那系统的不确定性也很小

==========================================================信息论==========================================================

==========================================================线性代数==========================================================
令A是一个m×n的矩阵，其列秩为r.因此矩阵A的列空间的维度是r.令c1,c2,.....,cr是A的列空间的一组基，构成m×n矩阵C的列向量C=[c1,c2,.....,cr]，并使得A的每个列向量是C的r个列向量的线性组合. 由矩阵乘法的定义，存在一个r×n矩阵R, 使得A=CR.(A的(i,j)元素是ci与R的第j 个行向量的点积.)
现在，由于A=CR,A的每个行向量是R的行向量的线性组合，这意味着A的行向量空间被包含于R的行向量空间之中. 因此A的行秩≤R的行秩. 但R仅有r行, 所以R的行秩≤r=A的列秩. 这就证明了A的行秩≤A的列秩.
把上述证明过程中的“行”与“列”交换，利用对偶性质同样可证A的列秩≤A的行秩。更简单的方法是考虑A的转置矩阵A^T，则A的列秩=A^T的行秩≤A^T的列秩 =A的行秩. 这证明了A的列秩等于A的行秩. 证毕.
==========================================================线性代数==========================================================