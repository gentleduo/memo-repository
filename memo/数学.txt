==========================================================概率论==========================================================
1.联合概率
联合概率指的是包含多个条件且所有条件同时成立的概率，记作P(X=a,Y=b)或P(a,b)，有的书上也习惯记作P(ab)，但是这种记法个人不太习惯，所以下文采用以逗号分隔的记法。
2.边缘概率
边缘概率是与联合概率对应的，P(X=a)或P(Y=b)，这类仅与单个随机变量有关的概率称为边缘概率
3.条件概率
条件概率表示在条件Y=b成立的情况下，X=a的概率，记作P(X=a|Y=b)或P(a|b)

例：
红J 红Q 红K 红J
红Q 红K 红1 红2
黑K 黑1 黑2 红3
黑3 黑4 黑5 黑6
将上述表格中的牌分成两个类别（数字牌和人头牌(X)、红色和黑色(Y)）来统计它们所占的比例，其中的每一种分类都包含了整个样本整体，得到的统计表如下：
        Y=数字牌  Y=人头牌
X=红色  3/16      6/16
X=黑色  6/16      1/16
这其中我们可以看到：若按颜色划分类别P(X=红色)(9/16)+P(X=黑色)(7/16)=1，若按牌的类别划分P(Y=数字牌)(9/16)+P(Y=人头牌)(7/16)=1，而这两个分类其实是你中有我，我中有你，因为本身都在16张牌中，怎么分两个类别都会有交叉。
表格中的概率值很清晰地表达了X和Y的联合概率，所谓联合概率，就是既满足X条件，又满足Y条件的概率，两个条件的满足是站在同一起跑线的，同时它们所对应的包围圈（对于这样的一个包围圈，其中各类概率的总和为1，而这个1所容纳的范围只是相对而言的）也是总体的16张扑克牌。
而对于条件概率，和联合概率就不一样了，首先它所对应的包围圈更为狭小，比如说P(Y=数字牌|X=红色)这个条件概率所对应的包围圈只限定在X=红色这个圈中，而红色的牌只有9张，9张牌中我再按照牌的类别进行划分，便得到P(Y=数字牌|X=红色)=3/9，P(Y=人头牌|X=红色)=6/9。当你看到条件概率的公式：P(Y=b|X=a)=P(X=a,Y=b)/P(X=a)，公式中的包围圈是16张牌这个整体。这只是分了两类，分成多类也是类似的道理。
边缘概率是相对于联合概率而言的，只是抹去了其中分类的数量，比如上面的例子，单一就从颜色的类别上说，P(X=红色)=9/16，P(X=黑色)=7/16，这就是边缘分布。多个类别也可以以此类推。

两个事件的独立性 (概率论)
两个事件A和B是独立的当且仅当Pr(A ∩ B) = Pr(A)Pr(B)。这里，A ∩ B是A和B的交集，即为A和B两个事件都会发生的事件。
更一般地，任意个事件都是互相独立的当且仅当对其任一有限子集A1, ..., An，会有 Pr(A1 ∩ ... ∩ An) = Pr(A1)...Pr(An)
若两个事件A和B是独立的，则其B给之A的条件概率和A的“无条件概率”一样，即 Pr(A | B) = Pr(A)
Pr(A | B) = Pr(A)的推导过程如下：
条件概率Pr(A | B)的定义为：Pr(A | B) = Pr(A ∩ B) / Pr(B)
由于A和B是独立的，则有Pr(A ∩ B) = Pr(A)Pr(B)，再将Pr(A ∩ B) = Pr(A)Pr(B)代入Pr(A | B) = Pr(A ∩ B) / Pr(B)中得到：Pr(A | B) = Pr(A)

互斥事件（不相容事件）
事件A和B的交集为空，A与B就是互斥事件，也叫互不相容事件。也可叙述为：不可能同时发生的事件。如A∩B为不可能事件（A∩B=Φ），那么称事件A与事件B互斥，其含义是：事件A与事件B在任何一次试验中不会同时发生。 若A与B互斥，则P(A+B)=P(A)+P(B)且P(A)+P(B)≤1；若a是A的对立事件则P(A)=1-P(a)
1、互斥事件定义中事件A与事件B不可能同时发生是指若事件A发生，事件B就不发生或者事件B发生，事件A就不发生。如，粉笔盒里有3支红粉笔，2支绿粉笔，1支黄粉笔，现从中任取1支，记事件A为取得红粉笔，记事件B为取得绿粉笔，则A与B不能同时发生，即A与B是互斥事件。
2、对立事件的定义中的事件A与B不能同时发生，且事件A与B中“必有一个发生”是指事件A不发生，事件B就一定发生或者事件A发生，事件B就不发生。如，投掷一枚硬币，事件A为正面向上，事件B为反面向上，则事件A与事件B必有一个发生且只有一个发生。所以，事件A与B是对立事件，但1中的事件A与B就不是对立事件，因为事件A与B可能都不发生。事件A的对立事件通常记作A。
3、如果事件A与B互斥，那么事件A+B发生(即A、B中恰有一个发生)的概率，等于事件A、B分别发生的概率的和，即P(A+B)=P(A)+P(B)，此公式可以由特殊情形中的既是互斥事件又是等可能性事件推导得到。一般地，如果事件A1、A2、…、An彼此互斥，那么事件A1+A2+…+An发生(即A1、A2、…、An中有一个发生)的概率，等于这n个事件分别发生的概率的和，即P(A1+A2+…+An)=P(A1)+P(A2)+…+P(An)。
4、对立事件是一种特殊的互斥事件。特殊有两点：其一，事件个数特殊(只能是两个事件)；其二，发生情况特殊(有且只有一个发生)。若A与B是对立事件，则A与B互斥且A+B为必然事件，故A+B发生的概率为1，即P(A+B)=P(A)+P(B)=1。

不相容与独立性
事件A和B的交集为空，即意味着两个事件不相容，你会不会直观的感觉到，事件A和事件B二者看上去没啥关系，二者就是相互独立的？这个说法看似很有道理，然而事实上却恰巧相反。若事件A和事件B互不相容，并且能够保证两个事件发生的概率：P(A)>0且P(B)>0成立，则他们永远不会相互独立。这是为什么呢？我们直接抠定义就好了。这是因为：首先有A∩B=ϕ，那么显然有联合概率P(A∩B)=0，而由于P(A)和P(B)均大于0，则有P(A)P(B)≠0。因此，从P(A∩B)≠P(A)P(B)的结果来看，并不满足事件A和事件B相互独立的基本条件。其实，这个结果从常理上来说我们也很好理解，由于事件A和事件B不相容，则如果事件B发生，则意味着事件A一定不会发生，那么这就实际上说明了：事件B的发生就给事件A的发生引入了额外的信息。那么，二者显然就不是互相独立的了。

条件独立
在概率论和统计学中，两事件R和B在给定的另一事件Y发生时条件独立，类似于统计独立性，就是指当事件Y发生时，R发生与否和B发生与否就条件概率分布而言是独立的。换句话讲，R和B在给定Y发生时条件独立，当且仅当已知Y发生时，知道R发生与否无助于知道B发生与否，同样知道B发生与否也无助于知道R发生与否。
R和B在给定Y发生时条件独立，用概率论的标准记号表示为：Pr(R ∩ B | Y) = Pr(R | Y)Pr(B | Y)、也可等价的表示为：Pr(R | B ∩ Y) = Pr(R | Y)
两个随机变量X和Y在给定第三个随机变量Z的情况下条件独立当且仅当它们在给定Z时的条件概率分布互相独立，也就是说，给定Z的任一值，X的概率分布和Y的值无关，Y的概率分布也和X的值无关。

独立与条件独立
事件A和事件B相互独立和在事件C发生的基础上条件独立是不是等价的呢？直观上看觉得似乎应该能，但是事实上呢？我们看看下面这个例子：
假设依次抛掷两枚均匀的硬币，事件A表示第一枚硬币正面向上，事件B表示第二枚硬币正面向上。首先，事件A和事件B肯定是相互独立的。那我们此时引入一个条件事件C，事件C表示两次试验的结果不同。那么显然，概率P(A∩B|C)=0因为在两次试验结果不同的前提条件下，压根不可能发生两次都是正面的情况。而另一方面呢？显然两个单独的条件概率P(A|C)≠0，P(B|C)≠0，因此P(A∩B|C)≠P(A|C)P(B|C)，也就是说事件A和事件B不满足事件C发生下的条件独立的要求。这个例子非常明确的说明了，独立和条件独立并不等价。

概率函数（分布律）
用函数形式给出每个取值发生的概率，pi=P(X=ai)(i=1,2,3,4,5,6)，只对离散型变量有意义，实际上是对概率分布的数学描述。
在这个函数里，自变量（X）是随机变量的取值，因变量（pi）是取值的概率

概率分布
给出了所有取值及其对应的概率（少一个也不行），只对离散型变量有意义。

概率分布函数（累积概率函数）
给出取值小于某个值的概率，是概率的累加形式，即：F(x)：F(xi)=P(x<xi)=sum(P(x1),P(x2),……,P(xi))（对于离散型变量）或求积分（对于连续型变量）

概率密度函数
某点的概率密度函数即为概率在该点的变化率(或导数)，很容易误以为该点概率密度值为概率值. 
比如: 距离(概率)和速度(概率密度)的关系.
某一点的速度, 不能以为是某一点的距离
一个物体，问你它在某一个点处的质量是多少？因为一个点是无限小的，所以点的质量一定为0。然而这个物体是由无数个点组成的，假如我们又需要求它质量，怎么办呢？于是引入密度的概念（质量相对于体积的导数），最后再把密度积分就可以得到质量m了。
同理，如果在[0，1]上随机取点，求取在某一点处的概率，点的长度无限小，此概率一定为0。这时情况和上面所述类似，我们需要引入概率密度p，这样我们就可以求所取点落在某一段(a,b)上的概率了。
总结:为什么要叫概率密度，因为它和物理上的密度本质上是一样的。物体在某些位置密度大，证明在这些位置“比较重”。同理，在某一段上概率密度大，证明样本落在这一段的比重大。想想正态分布钟形曲线，中间概率密度大，证明取中间的值最多。
你做题的时候一般就两种。一.告诉你概率密度函数，求分布函数，积分就好了。二.告诉你分布函数，求概率密度函数，求导就好了。就像初中物理题告诉你物体的密度让你求质量。告诉你质量让你求密度。

全概率公式
全概率公式为概率论中的重要公式，它将对一复杂事件A的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题。
内容：如果事件B1、B2、B3...Bn构成一个完备事件组，即它们两两互不相容(Bi∩Bj=∅)，其和为全集；并且P(Bi)大于0，则对任一事件A有P(A)=P(AB1)+P(AB2)+...+P(ABn)=P(A|B1)P(B1)+P(A|B2)P(B2)+...+P(A|Bn)P(Bn)。

贝叶斯定理
贝叶斯定理是关于随机事件A和B的条件概率（或边缘概率）的一则定理。其中P（A|B）是在B发生的情况下A发生的可能性。
贝叶斯定理也称贝叶斯推理，早在18世纪，英国学者贝叶斯曾提出计算条件概率的公式用来解决如下一类问题：假设H[1],H[2]…,H[n]互斥且构成一个完备事件组，已知它们的概率P(H[i]),i=1,2,…,n,现观察到某事件A与H[1],H[2]…,H[n]相伴随机出现，且已知条件概率P(A|H[i])，求P(H[i]|A)。
推导过程：
根据条件概率：P(A|B)和P(B|A)，按照乘法法则P(A∩B)=P(A)P(B|A)=P(B)P(A|B) ==> P(A|B)=P(B|A)P(A)/P(B)和P(B|A)=P(A|B)P(B)/P(A) 
B1,B2,B3,...,Bn互斥且构成一个完备事件组，则可以把事件A看作结果，把事件B1,B2,...,Bn看成是导致这个结果的各种原因。根据全概率公式，那么可以得到：
P(A)=P(B1)P(A|B1)+ P(B2)P(A|B2)+...+P(Bn)P(A|Bn)
就是由各种原因推理出结果事件发生的概率，是由因到果。
但是，更重要、更实际的应用场景是，我们在日常生活中常常是观察到某种现象，然后去反推造成这种现象的各种原因的概率。简单点说，就是由果推因。
贝叶斯公式P(Bi|A)=P(ABi)/P(A)=P(Bi)P(A|Bi)/∑jP(Bj)P(A|Bj)，最终求得的就是条件概率P(Bi|A)，就是在观察到结果事件A已经发生的情况下，我们推断结果事件A是由原因Bi造成的概率的大小
那么我们可以说，单纯的概率P(Bi)我们叫做先验概率，指的是在没有别的前提信息情况下的概率值，这个值一般需要借助我们的经验估计得到。
而条件概率P(Bi|A)，我们把它叫做是后验概率，它代表了在获得了信息A之后Bi出现的概率，可以说后验概率是先验概率在获取了新信息之后的一种修正。
比如，贝叶斯公式应用的一个常见例子是病理推断，某个病人出现打喷嚏的症状，我们希望对造成这个结果的三种可能原因（1：感冒；2：过敏；3：其他原因）进行分析判断，推断属于各个原因的概率，例如，原因是感冒的概率，也就是求条件概率：P(B1|A)的值。我们只要知道在这三种原因下出现打喷嚏的概率，也就是P(A|B1)，P(A|B2)，P(A|B3)，以及三种原因的先验概率：P(B1)，P(B2)，P(B3)，就能通过贝叶斯公式P(Bi|A)=P(Bi)P(A|Bi)/∑jP(Bj)P(A|Bj)求得，而上述这些需要我们知道的值，基本上都可以通过历史统计数据得到。
==========================================================概率论==========================================================

==========================================================线性代数==========================================================
令A是一个m×n的矩阵，其列秩为r.因此矩阵A的列空间的维度是r.令c1,c2,.....,cr是A的列空间的一组基，构成m×n矩阵C的列向量C=[c1,c2,.....,cr]，并使得A的每个列向量是C的r个列向量的线性组合. 由矩阵乘法的定义，存在一个r×n矩阵R, 使得A=CR.(A的(i,j)元素是ci与R的第j 个行向量的点积.)
现在，由于A=CR,A的每个行向量是R的行向量的线性组合，这意味着A的行向量空间被包含于R的行向量空间之中. 因此A的行秩≤R的行秩. 但R仅有r行, 所以R的行秩≤r=A的列秩. 这就证明了A的行秩≤A的列秩.
把上述证明过程中的“行”与“列”交换，利用对偶性质同样可证A的列秩≤A的行秩。更简单的方法是考虑A的转置矩阵A^T，则A的列秩=A^T的行秩≤A^T的列秩 =A的行秩. 这证明了A的列秩等于A的行秩. 证毕.
==========================================================线性代数==========================================================

==========================================================高等数学==========================================================
∫是连续值求和
∑是离散值求和
∏是求积，连乘

导数、偏导数以及微分
导数描述的是函数在一点处的变化快慢的趋势，是一个变化的速率，微分描述的是函数从一点（移动一个无穷小量）到另一点的变化幅度，是一个变化的量。
导数（Derivative），也叫导函数值。又名微商，是微积分中的重要基础概念。当函数y=f(x)的自变量x在一点x0上产生一个增量Δx时，函数输出值的增量Δy与自变量增量Δx的比值在Δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f’（x0）或df(x0)/dx
f(x) = lim f(x+△x) - f(x) / △x
在一元函数中，导数就是函数的变化率。对于二元函数的“变化率”，由于自变量多了一个，情况就要复杂的多。
x方向的偏导
设有二元函数z=f(x,y)，点(x0,y0)是其定义域D内一点。把y固定在y0而让x在x0有增量△x，相应地函数z=f(x,y)有增量（称为对x的偏增量）△z=f(x0+△x,y0)-f(x0,y0)。如果△z与△x之比当△x→0时的极限存在，那么此极限值称为函数z=f(x,y)在(x0,y0)处对x的偏导数，记作f'x(x0,y0)或函数z=f(x,y)在(x0,y0)处对x的偏导数，实际上就是把y固定在y0看成常数后，一元函数z=f(x,y0)在x0处的导数。f'x(x0,y0) = lim f(x0+△x,y0) - f(x0,y0) / △x
y方向的偏导
同样，把x固定在x0，让y有增量△y，如果极限存在那么此极限称为函数z=(x,y)在(x0,y0)处对y的偏导数。记作f'y(x0,y0)。
f'y(x0,y0) = lim f(x0,y0+△y) - f(x0,y0) / △y

推导微积分 d(xy)＝xd(y)+yd(x) 
设z=xy，则两个偏导数分别为z(x)=y，z(y)=x所以，dz=z(x)·dx+z(y)·dy=yd(x)+xd(y)

令u(x)，v(x) 求u*v的导数
则根据导数定义：
u'(x)=lim(t->0) [u(x+t)-u(x)]/t
v'(x)=lim(t->0) [v(x+t)-v(x)]/t
(u*v)'=lim(t->0) [u(x+t)v(x+t)-u(x)v(x)]/t
==>lim(t->0) [u(x+t)v(x+t)-u(x)v(x+t)+u(x)v(x+t)-u(x)v(x)]/t
==>lim(t->0) v(x+t)[u(x+t)-u(x)]/t+lim(t->0) u(x)[v(x+t)-v(x)]/t
==>v(x)*u'(x)+u(x)*v'(x) 

==========================================================高等数学==========================================================

==========================================================信息量和信息熵==========================================================
信息量和信息熵的概念最早是出现在通信理论中的，其概念最早是由信息论鼻祖香农在其经典的论文《通信的数学理论》中提出的。如今，这些概念不仅仅是通信领域中的基础概念，也被广泛的应用到了其他的领域中，比如机器学习。这两个概念的提出当时是为了给通信中的信号传输过程提供一个数学统计上的刻画，更准确的说，给无噪音系统中无损压缩的信号传输过程提供了一个所需最小信息量的理论值，这个值就是信息熵。
一般来说信息源会发出消息，这些消息会被编码以便于传送，编码后，通过传输器把这些信号传输给接收方，在传输的过程中，会有噪音干扰，这就会造成信号发生变化或者漂移，导致接收到的信号和原来发出的信号并不一定一致，所以通信领域中，一个更为基本的问题就是如何基于接收到的信号来推断真实的信号。本文中，信息量和信息熵所关心的问题是，对于事前的随机信源，假设整个系统是没有噪音干扰的，那么为了实现无损通信，即实现信息的无损传输，应该给信源赋予多大的通道容量，或者说应该给信源赋予多大的带宽？
对于这个问题，为了更好的理解，我们可以一步步的深入。首先，我们可以假设一个随机信源，其会随机的发出4种信号S1，S2，S3，S4，很显然，对于一次的信号发出，我们如果想要可以记录下这次的信号，因为事前我们并不知道具体会发出哪一种信号，所以如果我们是用二进制数编码信号的话，我们可以用2个bit的长度，并约定S1为00,S2为01,S3为10,S4为11，这样，无论该信源发出那种信号，我们都可以用2bit长度来进行完整的编码，这样子，对于该信源，每次信号的传输需要的通道的容量大小就是2bit。所以我们知道，2bit的容量确实可以无损的传输该信源的信息。同样的，如果该信源可以发出8个信号，那么3bit的容量可以实现无损传输，N个信号可以用log2N个bit实现无损传输。
但那只是最基本的第一步。虽然我们已经知道，对于有N个信号的信源，我们总是可以通过log2N个bit实现无损传输，但是，让我们设想一种情景，假如前文提到的可以发出4个信号的信源，发出S1的概率是90%,剩下三个信号的概率是分别是7%，2%，1%，如果我们还是一直用2个bit来传输，那么91%的时间都只是在传输S1，而编码S1这个状态，我们实际上只需要1个bit就够了，即我们可以约定S1为0，S2为1，S3为10，S4为11。这样的话，该信源在97%的时间只需要1个bit，剩下的3%的时间才需要2个bit，这样每个信号所需要的容量是不一样的，这个方案显然比我们原来一直用2bit去编码所有信号更好。这个方案的关键在于，我们不再给每种信号赋予相同的容量，而是通过每种信号出现的概率，赋予其不同的容量去编码，并且是赋予出现概率越大的信号越小的容量。
但是在现实中，通信系统往往是多信源的，即往往一个通信系统不会只针对某个单一的信源，而是同时会传输多种信号。如果有多个独立信源的信号同时在同一通道中传输，我们是不是可以进一步改进我们的通道容量分配呢？即对于多个独立信源通信系统，是否有一个理论上的最小带宽来实现无损传输呢？由于是多个独立信源，那么根据大数定律，其所需要的容量的极限值实际上就是每个信源的期望容量之和。所以关键的问题在于，如何在多信源语境下，求每个信源所需要的期望容量。
注意我们的目标是在多信源语境下，求每个信源所需要的最小的期望容量。根据我们之前的方案描述，对于每个信号，我们赋予了其所需要的容量，所以现在关键在于如何改进每个信号所需要的容量，即如何量化每个信号所需要容量，这里我们就引出了“信息量”概念。信息量就是用来量化一个可能性事件，或者一个随机信号在使得总体容量最小的目标下，其所被赋予的容量大小。下面我们来粗略的推导信息量的表达式。
首先需要明确的是，这个概念是在多信源语境下产生的一个统计量，脱离了多信源语境，这个统计量便失去了意义，因为只有在信源数量足够多的语境下，其统计性质才能转化为现实应用层面的意义。就好比大数定律只有在n足够大的情况下，收敛程度才是我们现实上可以接受的。假设存在这么一个量化可能性事件的信息量测度，那么其应该满足哪些性质呢？由于我们是在多信源的语境下，因此，这个测度不应该依赖于具体信源的信号数目，因为根据我们之前的针对单个信源的方案，每个信号所分配的容量是不会超过[log2N]+1的（注：其中[x]表示不超过x的取整函数）。但是在多信源中，单个信源的信号数N不再是需要关注的点，因为我们关注的是多信源这么一个整体。所以这个信息量测度，记为I，其和N无关，并且也和事件具体的内容或意义无关，只和其发生的概率有关，这是第一点。然后根据我们之前的分析，I应该是和事件发生的概率负相关的，即I应该是关于事件概率P的减函数，这是第二点。因为我们的目的是最小化信源所需要的信息量，数学直观提示我们应该假设I是关于P的连续函数，而多信源语境也给我们这种假设保证了现实的合理性，这是第三点。最后，很显然的一点是，两个独立信源，要同时传输两者发出的信号，那么所需要的容量是两者之和，所以这就要求I具有可加性，即I(PxPy)=I(Px)+I(Py)，Px和Py分别是两个独立信源发出信号X、Y的概率。所以综合以上四点，可以从数学上严格的推导出信息量测度I具有唯一的表达形式：
I(p)=-K㏒(p)
其中K是系数，可以令K=1，令对数底数为2，则I(p)的单位就是bit。
有了信息量测度，我们就可以对一个信源求信息量的期望，而信息量的期望就是我们所说的信息熵，记为H，其表达式为：
H(X)=-∑p(x)㏒2p(x)
由此可知信息量是在多信源语境下，为使总体通道容量最小推出应该给一个随机事件或信号分配多大的容量值，而信息熵则是一个在测度为1的信源或者概率分布的最小期望信息量，其描述的对象是一个概率分布。
不确定性也是一个抽象的概念，首先其并不能等价为概率，概率越大或越小都表示不确定性越小，说明概率并不是一个衡量不确定性的指标，而且，这里的不确定性描述的对象，应该是一个概率分布，而不是某一个可能性。信息熵衡量的是平均上无损传输一个信源所需要的期望信息量，由于信息和不确定性是对立的，即如果我们把信息定义为消除不确定性，那么由于我们已经有了信息量和信息熵的概念，那么我们便可以通过它们来定义不确定性，所以根据信息和不确定性的对立性，我们把信息熵理解成不确定性的大小，因为信息熵就是衡量平均意义上无损传输一个信源所需要的信息量，即可以认为消除一个信源不确定性所需要的信息量，从而，信息熵越大，不确定性也越大，因此我们可以将信息熵也理解为不确定性。实际上，香农的《通信的数学理论》这篇论文中，就是直接先推导信息熵，并且直接将信息熵等价于不确定性的。

如果X是一个离散型随机变量，取值空间为R，其分布律为p(x)=P(X=x),x∈R，那么X的熵H(X)定义为式
H(X)=-∑p(x)㏒2p(x)
由于在公示中对数以2为底，该公式定义的熵的单位为二进制(比特)，通常将㏒2p(x)简写成㏒p(x)
一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==>信息量的度量就等于不确定性的多少
熵又称为自信息量(self-information)，可以视为描述一个随机变量的不确定性的数量，它表示信息源X每发一个符号(不论发什么符号)所提供的平均信息量，一个随机变量的熵越大，它的不确定性越大，那么正确估计其值的可能性就越小，越不确定的随机变量越需要大的信息量用来确定其值。

例1：猜世界杯冠军，总共32支队伍，
假设每支队的夺冠机率都是一样：1/32，可以通过使用二叉搜索法获取信息来确定冠军（每次确认结果为是或者否，相当于一个比特(bit)中的0或者1）：
首先将32支队伍依次编号，第一次确认信息为：冠军是在1~16还是17~32中？在得到答案后可将范围缩小一半，以此类推要判断一支夺冠几率为1/32的队伍最终是否能夺冠共需5次确认，即：-㏒1/32，它表示当事件发生的几率是1/32的时候需要获取5个bit(即5次确认)的信息才能最终确定结果，同理当一个事件发生的几率是1的时候表示它无需任何额外信息就能确定
由H(X)=-∑p(x)㏒2p(x)可知，当支队的夺冠机率都是一样时，预测冠军总共需要的信息量是：-32*(1/32)*㏒(1/32)=5
一般来说每个队夺冠的几率不是相等的，由H(X)=-∑p(x)㏒2p(x)可知，预测冠军需要的信息量会少于5

例2：a,b,c,d,e,f6个字符在某一简单的语言中随机出现，每个字符出现的概率分别为：1/8,1/4,1/8,1/4,1/8,1/8，那么，每个字符的熵为：
H(X)=-∑p(x)㏒2p(x)
    =-[4×1/8㏒1/8﹢2×1/4㏒1/4] = 2.5(比特)
这个结果表明，我们可以设计一种编码，传输一个字符平均只需要2.5个比特：
字符: a   b   c   d   e   f
编码: 100 00  101 01  110 111
现实含义：比如我们要用二进制来表示出上述6个字符构成的一段比特流，通常的做法是所有的字符一律按3个比特来编码，但是可以根据出现概率高的字符用比较短的编码，出现概率低的字符用长一点的编码为原则来压缩整个比特流。熵的意思为：要表达这6个字符平均每个字符最低需要用2.5个比特位，即压缩的极限。
假设一个系统的状态的概率都很平均，那他的熵就越大，就说明这个系统很混乱不确定很大
假设一个系统的状态虽然多，但是某一个状态出现的概率在99%以上，那他的熵也会很小，那系统的不确定性也很小
==========================================================信息量和信息熵==========================================================

==========================================================标准差和方差==========================================================
标准差（对象总体标准差）
标准差是数值分散的测量。
标准差的符号是σ（希腊语字母 西格马，英语 sigma）
公式很简单：方差的平方根。那么…… "方差是什么？"

方差（对象总体标准差）
方差的定义是：
离平均的平方距离的平均。
按照以下的步骤来计算方差：
求数值的平均
从每一个数值减去平均，然后求差的平方。
求结果的平均。（为什么要求平方？)

可是……如果数据是样本数据
以上方法针对的数据是对象总体的数据
但如果数据是个样本（只是对象总体的一部分），计算便会有点改变！
如果你有 "N"个数值，而这些数值是：
对象总体：在求方差时除以 N（如上）
样本：在求方差时除以 N-1
其他的计算步骤不变，包括计算平均在内。


例子：如果有五条狗，身高（到肩膀）分别是：600mm、470mm、170mm、430mm 和 300mm。
求平均、方差和标准差。
第一步是求平均：
平均  = 600 + 470 + 170 + 430 + 300 / 5 = 1970  / 5 = 394
第二步是求方差（每条狗与平均身高的差的平方，然后求平均）：
方差 = (600-394)的平方 + (470-394)的平方 + (170-394)的平方 + (430-394)的平方 + (300-394)的平方 / 5 = 108,520 / 5 = 21,704
第三步是求标准差（标准差是方差的平方根）
σ = √21,704 = 147.32……
注意：但是如果我们的5条狗只是更多狗里的一个样本集，我们便要除以4，而不是除以5：
样本方差 = 108,520 / 4 = 27,130
样本标准差 = √27,130 = 164.71……

*脚注：为什么要求差的平方？
如果我们只把和平均的差加起来……负值和正值便会互相抵消：
例如：4,4,-4,-4
那如果用绝对值：
|4| + |4| + |−4| + |−4| / 4 = 4
|7| + |1| + |−6| + |−2| / 4 = 4
上述两个例子中第二个比第一个数据要分散，但结果相同
但如果用每个差的平方（最后才取平方根）：
√64/4 = 4
√90/4 = 4.74...
当数据比较分散时，标准差也比较大……
==========================================================标准差和方差==========================================================

==========================================================排列组合==========================================================
① 从n个不同元素中，任取m个元素按照一定的顺序排成一列，叫做从n个不同元素中取出m个元素的一个排列。
② 从n个不同元素中，取出m个元素的所有排列的个数，叫做从n个不同元素中取出m个元素的排列数。
排列用符号A(n,m)表示，m≦n。
计算公式是：A(n,m)＝n(n-1)(n-2)……(n-m+1)＝n!/(n-m)!
此外规定0!=1，n!表示n(n-1)(n-2)…1
例如：A(6,4)=6!/(6-4)!=(6x5x4x3x2x1)/2=360

① 从n个不同元素中，任取m个元素并成一组，叫做从n个不同元素中取出m个元素的一个组合。
② 从n个不同元素中，取出m个元素的所有组合的个数，叫做从n个不同元素中取出m个元素的组合数。
组合用符号C(n,m)表示，m≦n。
计算公式是：C(n,m)=A(n,m)/m!　或　C(n,m)=C(n,n-m)。
例如：C(5,2)=A(5,2)/2!=(1x2x3x4x5)/[2x(1x2x3)]=10。
==========================================================排列组合==========================================================

========================================================sup（上确界）和inf（下确界）========================================================
inf:infimum或infima，中文是下确界或最大下界。比如inf(E)inf(E)，EE表示一个集合，inf(E)inf(E)是指集合EE的下确界，即小于或等于E的所有其他元素的最大元素，这个数不一定在集合E中。
例子：
1. inf{1,2,3}=1;
2. inf{x∈R,0<x<1}=0;
3. inf{(−1)^n+1/n:n=1,2,3,...}=−1;
由上面的例子可以看出，如果一个集合有最小元素，则下确界等于这个最小元素。反之，则下确界不属于这个集合，这一点从例子2，3，中可以得出。

与之对偶的一个概念是，最小上界，也即上确界，表示为sup:supremum。比如sup{E}，是指集合E的上确界，即大于或等于E的所有其他元素的最小元素，这个数不一定在集合E中。
例子：
1. sup{1,2,3}=3;
2. sup{x∈R,0<x<1}=sup{x∈R,0≤x≤1}=1;
3. sup{(−1)n−1/n:n=1,2,3,...}=1;
4. sup{a+b:a∈A and b∈B}=sup(A)+sup(B);
由上面的例子可以看出，如果一个集合有最大元素，则上确界等于这个最大元素。反之，则上确界不属于这个集合，这一点从例子2，3中可以得出。
========================================================sup（上确界）和inf（下确界）========================================================