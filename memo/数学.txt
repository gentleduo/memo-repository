==========================================================概率论==========================================================
1.联合概率
联合概率指的是包含多个条件且所有条件同时成立的概率，记作P(X=a,Y=b)或P(a,b)，有的书上也习惯记作P(ab)，但是这种记法个人不太习惯，所以下文采用以逗号分隔的记法。
2.边缘概率
边缘概率是与联合概率对应的，P(X=a)或P(Y=b)，这类仅与单个随机变量有关的概率称为边缘概率
3.条件概率
条件概率表示在条件Y=b成立的情况下，X=a的概率，记作P(X=a|Y=b)或P(a|b)

例：
红J 红Q 红K 红J
红Q 红K 红1 红2
黑K 黑1 黑2 红3
黑3 黑4 黑5 黑6
将上述表格中的牌分成两个类别（数字牌和人头牌(X)、红色和黑色(Y)）来统计它们所占的比例，其中的每一种分类都包含了整个样本整体，得到的统计表如下：
        Y=数字牌  Y=人头牌
X=红色  3/16      6/16
X=黑色  6/16      1/16
这其中我们可以看到：若按颜色划分类别P(X=红色)(9/16)+P(X=黑色)(7/16)=1，若按牌的类别划分P(Y=数字牌)(9/16)+P(Y=人头牌)(7/16)=1，而这两个分类其实是你中有我，我中有你，因为本身都在16张牌中，怎么分两个类别都会有交叉。
表格中的概率值很清晰地表达了X和Y的联合概率，所谓联合概率，就是既满足X条件，又满足Y条件的概率，两个条件的满足是站在同一起跑线的，同时它们所对应的包围圈（对于这样的一个包围圈，其中各类概率的总和为1，而这个1所容纳的范围只是相对而言的）也是总体的16张扑克牌。
而对于条件概率，和联合概率就不一样了，首先它所对应的包围圈更为狭小，比如说P(Y=数字牌|X=红色)这个条件概率所对应的包围圈只限定在X=红色这个圈中，而红色的牌只有9张，9张牌中我再按照牌的类别进行划分，便得到P(Y=数字牌|X=红色)=3/9，P(Y=人头牌|X=红色)=6/9。当你看到条件概率的公式：P(Y=b|X=a)=P(X=a,Y=b)/P(X=a)，公式中的包围圈是16张牌这个整体。这只是分了两类，分成多类也是类似的道理。
边缘概率是相对于联合概率而言的，只是抹去了其中分类的数量，比如上面的例子，单一就从颜色的类别上说，P(X=红色)=9/16，P(X=黑色)=7/16，这就是边缘分布。多个类别也可以以此类推。

独立 (概率论)
两个事件A和B是独立的当且仅当Pr(A ∩ B) = Pr(A)Pr(B)。这里，A ∩ B是A和B的交集，即为A和B两个事件都会发生的事件。
更一般地，任意个事件都是互相独立的当且仅当对其任一有限子集A1, ..., An，会有 Pr(A1 ∩ ... ∩ An) = Pr(A1)...Pr(An)
若两个事件A和B是独立的，则其B给之A的条件概率和A的“无条件概率”一样，即 Pr(A | B) = Pr(A)
Pr(A | B) = Pr(A)的推导过程如下：
条件概率Pr(A | B)的定义为：Pr(A | B) = Pr(A ∩ B) / Pr(B)
由于A和B是独立的，则有Pr(A ∩ B) = Pr(A)Pr(B)，再将Pr(A ∩ B) = Pr(A)Pr(B)代入Pr(A | B) = Pr(A ∩ B) / Pr(B)中得到：Pr(A | B) = Pr(A)

条件独立
在概率论和统计学中，两事件R和B在给定的另一事件Y发生时条件独立，类似于统计独立性，就是指当事件Y发生时，R发生与否和B发生与否就条件概率分布而言是独立的。换句话讲，R和B在给定Y发生时条件独立，当且仅当已知Y发生时，知道R发生与否无助于知道B发生与否，同样知道B发生与否也无助于知道R发生与否。
R和B在给定Y发生时条件独立，用概率论的标准记号表示为：Pr(R ∩ B | Y) = Pr(R | Y)Pr(B | Y)、也可等价的表示为：Pr(R | B ∩ Y) = Pr(R | Y)
两个随机变量X和Y在给定第三个随机变量Z的情况下条件独立当且仅当它们在给定Z时的条件概率分布互相独立，也就是说，给定Z的任一值，X的概率分布和Y的值无关，Y的概率分布也和X的值无关。

==========================================================概率论==========================================================

==========================================================信息论==========================================================

如果X是一个离散型随机变量，取值空间为R，其概率分布为p(x)=P(X=x),x∈R，那么X的熵H(X)定义为式

H(X)=-∑p(x)㏒2p(x)

由于在公示中对数以2为底，该公式定义的熵的单位为二进制(比特)，通常将㏒2p(x)简写成㏒p(x)

熵又称为自信息量(self-information)，可以视为描述一个随机变量的不确定性的数量，它表示信息源X每发一个符号(不论发什么符号)所提供的平均信息量，一个随机变量的熵越大，它的不确定性越大，那么正确估计其值的可能性就越小，越不确定的随机变量越需要大的信息量用来确定其值。

假设a,b,c,d,e,f6个字符在某一简单的语言中随机出现，每个字符出现的概率分别为：1/8,1/4,1/8,1/4,1/8,1/8，那么，每个字符的熵为：

H(X)=-∑p(x)㏒2p(x)

    =-[4×1/8㏒1/8﹢2×1/4㏒1/4] = 2.5(比特)

这个结果表明，我们可以设计一种编码，传输一个字符平均只需要2.5个比特：

字符: a   b   c   d   e   f

编码: 100 00  101 01  110 111

现实含义：比如我们要用二进制来表示出上述6个字符构成的一段比特流，通常的做法是所有的字符一律按3个比特来编码，但是可以根据出现概率高的字符用比较短的编码，出现概率低的字符用长一点的编码为原则来压缩整个比特流。熵的意思为：要表达这6个字符平均每个字符最低需要用2.5个比特位，即压缩的极限。

假设一个系统的状态的概率都很平均，那他的熵就越大，就说明这个系统很混乱不确定很大

假设一个系统的状态虽然多，但是某一个状态出现的概率在99%以上，那他的熵也会很小，那系统的不确定性也很小

==========================================================信息论==========================================================